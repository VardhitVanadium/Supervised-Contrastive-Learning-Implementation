{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzTnLcGXGoTc",
        "outputId": "691ad5fe-1b3e-4404-a16b-d6e3af4a58ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboard_logger\n",
            "  Downloading tensorboard_logger-0.1.0-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.11.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (9.4.0)\n",
            "Installing collected packages: tensorboard_logger\n",
            "Successfully installed tensorboard_logger-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard_logger"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuib9L03Hvh9",
        "outputId": "2fe6f2e3-b163-41f1-cbcc-66bf1dbd1d81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apex\n",
            "  Downloading apex-0.9.10dev.tar.gz (36 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cryptacular (from apex)\n",
            "  Downloading cryptacular-1.6.2.tar.gz (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zope.sqlalchemy (from apex)\n",
            "  Downloading zope.sqlalchemy-3.1-py3-none-any.whl (23 kB)\n",
            "Collecting velruse>=1.0.3 (from apex)\n",
            "  Downloading velruse-1.1.1.tar.gz (709 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.8/709.8 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyramid>1.1.2 (from apex)\n",
            "  Downloading pyramid-2.0.2-py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyramid_mailer (from apex)\n",
            "  Downloading pyramid_mailer-0.15.1-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from apex) (2.31.0)\n",
            "Collecting wtforms (from apex)\n",
            "  Downloading wtforms-3.1.2-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wtforms-recaptcha (from apex)\n",
            "  Downloading wtforms_recaptcha-0.3.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting hupper>=1.5 (from pyramid>1.1.2->apex)\n",
            "  Downloading hupper-1.12.1-py3-none-any.whl (22 kB)\n",
            "Collecting plaster (from pyramid>1.1.2->apex)\n",
            "  Downloading plaster-1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting plaster-pastedeploy (from pyramid>1.1.2->apex)\n",
            "  Downloading plaster_pastedeploy-1.0.1-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyramid>1.1.2->apex) (67.7.2)\n",
            "Collecting translationstring>=0.4 (from pyramid>1.1.2->apex)\n",
            "  Downloading translationstring-1.4-py2.py3-none-any.whl (15 kB)\n",
            "Collecting venusian>=1.0 (from pyramid>1.1.2->apex)\n",
            "  Downloading venusian-3.1.0-py3-none-any.whl (13 kB)\n",
            "Collecting webob>=1.8.3 (from pyramid>1.1.2->apex)\n",
            "  Downloading WebOb-1.8.7-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.deprecation>=3.5.0 (from pyramid>1.1.2->apex)\n",
            "  Downloading zope.deprecation-5.0-py3-none-any.whl (10 kB)\n",
            "Collecting zope.interface>=3.8.0 (from pyramid>1.1.2->apex)\n",
            "  Downloading zope.interface-6.4.post2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/247.8 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from velruse>=1.0.3->apex) (1.3.1)\n",
            "Collecting anykeystore (from velruse>=1.0.3->apex)\n",
            "  Downloading anykeystore-0.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python3-openid (from velruse>=1.0.3->apex)\n",
            "  Downloading python3_openid-3.2.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pbkdf2 (from cryptacular->apex)\n",
            "  Downloading pbkdf2-1.3.tar.gz (6.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting repoze.sendmail>=4.1 (from pyramid_mailer->apex)\n",
            "  Downloading repoze.sendmail-4.4.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transaction (from pyramid_mailer->apex)\n",
            "  Downloading transaction-4.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (2024.6.2)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from wtforms->apex) (2.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from zope.sqlalchemy->apex) (24.1)\n",
            "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from zope.sqlalchemy->apex) (2.0.30)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (3.0.3)\n",
            "Collecting PasteDeploy>=2.0 (from plaster-pastedeploy->pyramid>1.1.2->apex)\n",
            "  Downloading PasteDeploy-3.1.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from python3-openid->velruse>=1.0.3->apex) (0.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->velruse>=1.0.3->apex) (3.2.2)\n",
            "Building wheels for collected packages: apex, velruse, cryptacular, anykeystore, pbkdf2\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.9.10.dev0-py3-none-any.whl size=46442 sha256=6ca14dacf45d23651617667847b528b307d35983e0fbde30d3203b5cf9795ed4\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/62/59/9b100fce7ebd989603b3b7a4ca259150da72c9e107fcaa2a30\n",
            "  Building wheel for velruse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for velruse: filename=velruse-1.1.1-py3-none-any.whl size=50909 sha256=25a8e3361c7a6bbbec0af72e658d696bccd21167d70f3e07736f197004e67522\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/f9/a4/fc4ea7b935ee9c58b9bc772cabd94f6a8560f35444097d948d\n",
            "  Building wheel for cryptacular (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cryptacular: filename=cryptacular-1.6.2-cp310-cp310-linux_x86_64.whl size=55079 sha256=af5732aee8af1b2e5d35461d826be9f7f367981b1d49c901377b091fa162c25a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/6e/09/a7fba517f95b2a6a36bd01b6d4f4679fa7259615a493b64b8f\n",
            "  Building wheel for anykeystore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for anykeystore: filename=anykeystore-0.2-py3-none-any.whl size=16813 sha256=c7994b65c725d93bacddd26049e67b68a813aad0d5545f2d083fb9936225c4f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/9e/24/35542b7d376b53a6f8426524cc5a3f7998f975037b32d19906\n",
            "  Building wheel for pbkdf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pbkdf2: filename=pbkdf2-1.3-py3-none-any.whl size=5083 sha256=599417852e50e57a23a9a7d440a7a126f633d435ef0d50bb9b8993bd78f850d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/7d/8b/4269ff90fda80497ec59f6ff7d1e1596cb697c1dc8e9bbe320\n",
            "Successfully built apex velruse cryptacular anykeystore pbkdf2\n",
            "Installing collected packages: translationstring, pbkdf2, anykeystore, zope.interface, zope.deprecation, wtforms, webob, venusian, python3-openid, plaster, PasteDeploy, hupper, cryptacular, wtforms-recaptcha, transaction, plaster-pastedeploy, zope.sqlalchemy, repoze.sendmail, pyramid, velruse, pyramid_mailer, apex\n",
            "Successfully installed PasteDeploy-3.1.0 anykeystore-0.2 apex-0.9.10.dev0 cryptacular-1.6.2 hupper-1.12.1 pbkdf2-1.3 plaster-1.1.2 plaster-pastedeploy-1.0.1 pyramid-2.0.2 pyramid_mailer-0.15.1 python3-openid-3.2.0 repoze.sendmail-4.4.1 transaction-4.0 translationstring-1.4 velruse-1.1.1 venusian-3.1.0 webob-1.8.7 wtforms-3.1.2 wtforms-recaptcha-0.3.2 zope.deprecation-5.0 zope.interface-6.4.post2 zope.sqlalchemy-3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SupConLoss(nn.Module):\"\n",
        "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
        "                 base_temperature=0.07):\n",
        "        super(SupConLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.contrast_mode = contrast_mode\n",
        "        self.base_temperature = base_temperature\n",
        "\n",
        "    def forward(self, features, labels=None, mask=None):\n",
        "        device = (torch.device('cuda')\n",
        "                  if features.is_cuda\n",
        "                  else torch.device('cpu'))\n",
        "\n",
        "        if len(features.shape) < 3:\n",
        "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
        "                             'at least 3 dimensions are required')\n",
        "        if len(features.shape) > 3:\n",
        "            features = features.view(features.shape[0], features.shape[1], -1)\n",
        "\n",
        "        batch_size = features.shape[0]\n",
        "        if labels is not None and mask is not None:\n",
        "            raise ValueError('Cannot define both `labels` and `mask`')\n",
        "        elif labels is None and mask is None:\n",
        "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
        "        elif labels is not None:\n",
        "            labels = labels.contiguous().view(-1, 1)\n",
        "            if labels.shape[0] != batch_size:\n",
        "                raise ValueError('Num of labels does not match num of features')\n",
        "            mask = torch.eq(labels, labels.T).float().to(device)\n",
        "        else:\n",
        "            mask = mask.float().to(device)\n",
        "\n",
        "        contrast_count = features.shape[1]\n",
        "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
        "        if self.contrast_mode == 'one':\n",
        "            anchor_feature = features[:, 0]\n",
        "            anchor_count = 1\n",
        "        elif self.contrast_mode == 'all':\n",
        "            anchor_feature = contrast_feature\n",
        "            anchor_count = contrast_count\n",
        "        else:\n",
        "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
        "\n",
        "        # compute logits\n",
        "        anchor_dot_contrast = torch.div(\n",
        "            torch.matmul(anchor_feature, contrast_feature.T),\n",
        "            self.temperature)\n",
        "        # for numerical stability\n",
        "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
        "        logits = anchor_dot_contrast - logits_max.detach()\n",
        "\n",
        "        # tile mask\n",
        "        mask = mask.repeat(anchor_count, contrast_count)\n",
        "        # mask-out self-contrast cases\n",
        "        logits_mask = torch.scatter(\n",
        "            torch.ones_like(mask),\n",
        "            1,\n",
        "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
        "            0\n",
        "        )\n",
        "        mask = mask * logits_mask\n",
        "\n",
        "        # compute log_prob\n",
        "        exp_logits = torch.exp(logits) * logits_mask\n",
        "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
        "\n",
        "        # compute mean of log-likelihood over positive\n",
        "        # modified to handle edge cases when there is no positive pair\n",
        "        # for an anchor point.\n",
        "        # Edge case e.g.:-\n",
        "        # features of shape: [4,1,...]\n",
        "        # labels:            [0,1,1,2]\n",
        "        # loss before mean:  [nan, ..., ..., nan]\n",
        "        mask_pos_pairs = mask.sum(1)\n",
        "        mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs\n",
        "\n",
        "        # loss\n",
        "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
        "        loss = loss.view(anchor_count, batch_size).mean()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "LB5QveWRH0c-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class TwoCropTransform:\n",
        "    \"\"\"Create two crops of the same image\"\"\"\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [self.transform(x), self.transform(x)]\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def adjust_learning_rate(args, optimizer, epoch):\n",
        "    lr = args.learning_rate\n",
        "    if args.cosine:\n",
        "        eta_min = lr * (args.lr_decay_rate ** 3)\n",
        "        lr = eta_min + (lr - eta_min) * (\n",
        "                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n",
        "    else:\n",
        "        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n",
        "        if steps > 0:\n",
        "            lr = lr * (args.lr_decay_rate ** steps)\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n",
        "    if args.warm and epoch <= args.warm_epochs:\n",
        "        p = (batch_id + (epoch - 1) * total_batches) / \\\n",
        "            (args.warm_epochs * total_batches)\n",
        "        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def set_optimizer(opt, model):\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=opt.learning_rate,\n",
        "                          momentum=opt.momentum,\n",
        "                          weight_decay=opt.weight_decay)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def save_model(model, optimizer, opt, epoch, save_file):\n",
        "    print('==> Saving...')\n",
        "    state = {\n",
        "        'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    torch.save(state, save_file)\n",
        "    del state"
      ],
      "metadata": {
        "id": "gvLaDtxlH_Je"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"ResNet in PyTorch.\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.is_last = is_last\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        preact = out\n",
        "        out = F.relu(out)\n",
        "        if self.is_last:\n",
        "            return out, preact\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.is_last = is_last\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        preact = out\n",
        "        out = F.relu(out)\n",
        "        if self.is_last:\n",
        "            return out, preact\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, in_channel=3, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves\n",
        "        # like an identity. This improves the model by 0.2~0.3% according to:\n",
        "        # https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for i in range(num_blocks):\n",
        "            stride = strides[i]\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, layer=100):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "\n",
        "\n",
        "def resnet34(**kwargs):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet50(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet101(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "\n",
        "\n",
        "model_dict = {\n",
        "    'resnet18': [resnet18, 512],\n",
        "    'resnet34': [resnet34, 512],\n",
        "    'resnet50': [resnet50, 2048],\n",
        "    'resnet101': [resnet101, 2048],\n",
        "}\n",
        "\n",
        "\n",
        "class LinearBatchNorm(nn.Module):\n",
        "    \"\"\"Implements BatchNorm1d by BatchNorm2d, for SyncBN purpose\"\"\"\n",
        "    def __init__(self, dim, affine=True):\n",
        "        super(LinearBatchNorm, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.bn = nn.BatchNorm2d(dim, affine=affine)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.dim, 1, 1)\n",
        "        x = self.bn(x)\n",
        "        x = x.view(-1, self.dim)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SupConResNet(nn.Module):\n",
        "    \"\"\"backbone + projection head\"\"\"\n",
        "    def __init__(self, name='resnet50', head='mlp', feat_dim=128):\n",
        "        super(SupConResNet, self).__init__()\n",
        "        model_fun, dim_in = model_dict[name]\n",
        "        self.encoder = model_fun()\n",
        "        if head == 'linear':\n",
        "            self.head = nn.Linear(dim_in, feat_dim)\n",
        "        elif head == 'mlp':\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(dim_in, dim_in),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(dim_in, feat_dim)\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                'head not supported: {}'.format(head))\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.encoder(x)\n",
        "        feat = F.normalize(self.head(feat), dim=1)\n",
        "        return feat\n",
        "\n",
        "\n",
        "class SupCEResNet(nn.Module):\n",
        "    \"\"\"encoder + classifier\"\"\"\n",
        "    def __init__(self, name='resnet50', num_classes=10):\n",
        "        super(SupCEResNet, self).__init__()\n",
        "        model_fun, dim_in = model_dict[name]\n",
        "        self.encoder = model_fun()\n",
        "        self.fc = nn.Linear(dim_in, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.encoder(x))\n",
        "\n",
        "\n",
        "class LinearClassifier(nn.Module):\n",
        "    \"\"\"Linear classifier\"\"\"\n",
        "    def __init__(self, name='resnet50', num_classes=10):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        _, feat_dim = model_dict[name]\n",
        "        self.fc = nn.Linear(feat_dim, num_classes)\n",
        "\n",
        "    def forward(self, features):\n",
        "        return self.fc(features)"
      ],
      "metadata": {
        "id": "C2dRMNk8IGWR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Entropy Loss Function on CIFAR100"
      ],
      "metadata": {
        "id": "EuGF3R4lLx6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import tensorboard_logger as tb_logger\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "try:\n",
        "    import apex\n",
        "    from apex import amp, optimizers\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# Hardcoded configurations\n",
        "class Config:\n",
        "    print_freq = 10\n",
        "    save_freq = 50\n",
        "    batch_size = 256\n",
        "    num_workers = 16\n",
        "    epochs = 25\n",
        "    learning_rate = 0.2\n",
        "    lr_decay_epochs = [350, 400, 450]\n",
        "    lr_decay_rate = 0.1\n",
        "    weight_decay = 1e-4\n",
        "    momentum = 0.9\n",
        "    model = 'resnet50'\n",
        "    dataset = 'cifar100'\n",
        "    cosine = False\n",
        "    syncBN = False\n",
        "    warm = True\n",
        "    trial = '0'\n",
        "    data_folder = './datasets/'\n",
        "    model_path = './save/SupCon/cifar100_models'\n",
        "    tb_path = './save/SupCon/cifar100_tensorboard'\n",
        "\n",
        "opt = Config()\n",
        "\n",
        "opt.model_name = 'SupCE_{}_{}_lr_{}_decay_{}_bsz_{}_trial_{}'.format(\n",
        "    opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,\n",
        "    opt.batch_size, opt.trial)\n",
        "\n",
        "if opt.cosine:\n",
        "    opt.model_name = '{}_cosine'.format(opt.model_name)\n",
        "\n",
        "if opt.warm:\n",
        "    opt.model_name = '{}_warm'.format(opt.model_name)\n",
        "    opt.warmup_from = 0.01\n",
        "    opt.warm_epochs = 10\n",
        "    if opt.cosine:\n",
        "        eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
        "        opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
        "            1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
        "    else:\n",
        "        opt.warmup_to = opt.learning_rate\n",
        "\n",
        "opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n",
        "if not os.path.isdir(opt.tb_folder):\n",
        "    os.makedirs(opt.tb_folder)\n",
        "\n",
        "opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
        "if not os.path.isdir(opt.save_folder):\n",
        "    os.makedirs(opt.save_folder)\n",
        "\n",
        "opt.n_cls = 100\n",
        "\n",
        "def set_loader(opt):\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "    normalize = transforms.Normalize(mean=mean, std=std)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR100(root=opt.data_folder,\n",
        "                                      transform=train_transform,\n",
        "                                      download=True)\n",
        "    val_dataset = datasets.CIFAR100(root=opt.data_folder,\n",
        "                                    train=False,\n",
        "                                    transform=val_transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=opt.batch_size, shuffle=True,\n",
        "        num_workers=opt.num_workers, pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=256, shuffle=False,\n",
        "        num_workers=8, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def set_model(opt):\n",
        "    model = SupCEResNet(name=opt.model, num_classes=opt.n_cls)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    if opt.syncBN:\n",
        "        model = apex.parallel.convert_syncbn_model(model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    return model, criterion\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, opt):\n",
        "    model.train()\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    end = time.time()\n",
        "\n",
        "    for idx, (images, labels) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "        images = images.cuda(non_blocking=True)\n",
        "        labels = labels.cuda(non_blocking=True)\n",
        "        bsz = labels.shape[0]\n",
        "        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        losses.update(loss.item(), bsz)\n",
        "        acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n",
        "        top1.update(acc1[0], bsz)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if (idx + 1) % opt.print_freq == 0:\n",
        "            print('Train: [{0}][{1}/{2}]\\t'\n",
        "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n",
        "                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses, top1=top1))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "def validate(val_loader, model, criterion, opt):\n",
        "    model.eval()\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for idx, (images, labels) in enumerate(val_loader):\n",
        "            images = images.float().cuda()\n",
        "            labels = labels.cuda()\n",
        "            bsz = labels.shape[0]\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            losses.update(loss.item(), bsz)\n",
        "            acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n",
        "            top1.update(acc1[0], bsz)\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if idx % opt.print_freq == 0:\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                       idx, len(val_loader), batch_time=batch_time,\n",
        "                       loss=losses, top1=top1))\n",
        "\n",
        "    print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "def main():\n",
        "    best_acc = 0\n",
        "    # build data loader\n",
        "    train_loader, val_loader = set_loader(opt)\n",
        "    # build model and criterion\n",
        "    model, criterion = set_model(opt)\n",
        "    # build optimizer\n",
        "    optimizer = set_optimizer(opt, model)\n",
        "    # tensorboard\n",
        "    logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n",
        "    # training routine\n",
        "    for epoch in range(1, opt.epochs + 1):\n",
        "        adjust_learning_rate(opt, optimizer, epoch)\n",
        "        time1 = time.time()\n",
        "        loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, opt)\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "        logger.log_value('train_loss', loss, epoch)\n",
        "        logger.log_value('train_acc', train_acc, epoch)\n",
        "        logger.log_value('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
        "        loss, val_acc = validate(val_loader, model, criterion, opt)\n",
        "        logger.log_value('val_loss', loss, epoch)\n",
        "        logger.log_value('val_acc', val_acc, epoch)\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "        if epoch % opt.save_freq == 0:\n",
        "            save_file = os.path.join(\n",
        "                opt.save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            save_model(model, optimizer, opt, epoch, save_file)\n",
        "    save_file = os.path.join(\n",
        "        opt.save_folder, 'last.pth')\n",
        "    save_model(model, optimizer, opt, opt.epochs, save_file)\n",
        "    print('best accuracy: {:.2f}'.format(best_acc))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0uAEKivILGX",
        "outputId": "4f67ba5f-5817-4150-b849-31ae7b732ae5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./datasets/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:13<00:00, 12928925.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/cifar-100-python.tar.gz to ./datasets/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: [1][10/196]\tBT 0.377 (0.960)\tDT 0.000 (0.069)\tloss 4.733 (4.750)\tAcc@1 1.562 (1.250)\n",
            "Train: [1][20/196]\tBT 0.378 (0.669)\tDT 0.000 (0.047)\tloss 5.061 (4.785)\tAcc@1 0.391 (1.562)\n",
            "Train: [1][30/196]\tBT 0.379 (0.572)\tDT 0.000 (0.039)\tloss 4.865 (4.894)\tAcc@1 2.344 (1.523)\n",
            "Train: [1][40/196]\tBT 0.378 (0.524)\tDT 0.000 (0.035)\tloss 5.005 (4.905)\tAcc@1 3.125 (1.699)\n",
            "Train: [1][50/196]\tBT 0.381 (0.495)\tDT 0.000 (0.033)\tloss 4.710 (4.876)\tAcc@1 2.734 (1.852)\n",
            "Train: [1][60/196]\tBT 0.379 (0.476)\tDT 0.000 (0.032)\tloss 4.715 (4.830)\tAcc@1 1.953 (2.070)\n",
            "Train: [1][70/196]\tBT 0.383 (0.462)\tDT 0.000 (0.031)\tloss 4.534 (4.786)\tAcc@1 5.078 (2.338)\n",
            "Train: [1][80/196]\tBT 0.380 (0.452)\tDT 0.000 (0.030)\tloss 4.432 (4.742)\tAcc@1 4.688 (2.617)\n",
            "Train: [1][90/196]\tBT 0.381 (0.444)\tDT 0.000 (0.029)\tloss 4.295 (4.697)\tAcc@1 5.078 (2.873)\n",
            "Train: [1][100/196]\tBT 0.380 (0.438)\tDT 0.000 (0.029)\tloss 4.352 (4.660)\tAcc@1 6.250 (3.195)\n",
            "Train: [1][110/196]\tBT 0.382 (0.433)\tDT 0.000 (0.028)\tloss 4.276 (4.628)\tAcc@1 10.938 (3.466)\n",
            "Train: [1][120/196]\tBT 0.382 (0.428)\tDT 0.000 (0.028)\tloss 4.282 (4.597)\tAcc@1 7.812 (3.682)\n",
            "Train: [1][130/196]\tBT 0.382 (0.425)\tDT 0.000 (0.028)\tloss 4.155 (4.564)\tAcc@1 7.031 (3.918)\n",
            "Train: [1][140/196]\tBT 0.383 (0.422)\tDT 0.000 (0.028)\tloss 4.117 (4.530)\tAcc@1 6.641 (4.205)\n",
            "Train: [1][150/196]\tBT 0.383 (0.419)\tDT 0.000 (0.027)\tloss 4.028 (4.498)\tAcc@1 6.250 (4.440)\n",
            "Train: [1][160/196]\tBT 0.384 (0.417)\tDT 0.000 (0.027)\tloss 3.958 (4.469)\tAcc@1 8.594 (4.639)\n",
            "Train: [1][170/196]\tBT 0.386 (0.415)\tDT 0.000 (0.027)\tloss 4.096 (4.445)\tAcc@1 10.156 (4.862)\n",
            "Train: [1][180/196]\tBT 0.388 (0.413)\tDT 0.000 (0.027)\tloss 4.025 (4.421)\tAcc@1 11.328 (5.052)\n",
            "Train: [1][190/196]\tBT 0.382 (0.412)\tDT 0.000 (0.027)\tloss 4.068 (4.401)\tAcc@1 10.547 (5.275)\n",
            "epoch 1, total time 81.98\n",
            "Test: [0/40]\tTime 0.439 (0.439)\tLoss 3.9407 (3.9407)\tAcc@1 11.719 (11.719)\n",
            "Test: [10/40]\tTime 0.122 (0.151)\tLoss 4.2623 (4.2136)\tAcc@1 11.719 (9.766)\n",
            "Test: [20/40]\tTime 0.121 (0.137)\tLoss 4.3739 (4.1569)\tAcc@1 8.203 (9.859)\n",
            "Test: [30/40]\tTime 0.121 (0.132)\tLoss 4.2853 (4.1813)\tAcc@1 8.594 (9.929)\n",
            " * Acc@1 9.930\n",
            "Train: [2][10/196]\tBT 0.386 (0.444)\tDT 0.000 (0.082)\tloss 3.975 (3.896)\tAcc@1 6.250 (9.180)\n",
            "Train: [2][20/196]\tBT 0.386 (0.415)\tDT 0.000 (0.053)\tloss 3.948 (3.898)\tAcc@1 7.422 (9.648)\n",
            "Train: [2][30/196]\tBT 0.385 (0.405)\tDT 0.000 (0.044)\tloss 3.881 (3.906)\tAcc@1 10.938 (9.831)\n",
            "Train: [2][40/196]\tBT 0.385 (0.401)\tDT 0.000 (0.039)\tloss 3.958 (3.910)\tAcc@1 8.594 (9.902)\n",
            "Train: [2][50/196]\tBT 0.387 (0.398)\tDT 0.000 (0.036)\tloss 3.998 (3.910)\tAcc@1 8.984 (9.875)\n",
            "Train: [2][60/196]\tBT 0.391 (0.396)\tDT 0.000 (0.034)\tloss 3.871 (3.913)\tAcc@1 11.328 (9.967)\n",
            "Train: [2][70/196]\tBT 0.389 (0.395)\tDT 0.000 (0.033)\tloss 3.930 (3.909)\tAcc@1 11.719 (10.179)\n",
            "Train: [2][80/196]\tBT 0.391 (0.395)\tDT 0.000 (0.032)\tloss 3.907 (3.910)\tAcc@1 10.547 (10.273)\n",
            "Train: [2][90/196]\tBT 0.390 (0.394)\tDT 0.000 (0.031)\tloss 3.886 (3.906)\tAcc@1 10.938 (10.326)\n",
            "Train: [2][100/196]\tBT 0.389 (0.394)\tDT 0.000 (0.031)\tloss 3.793 (3.900)\tAcc@1 11.719 (10.355)\n",
            "Train: [2][110/196]\tBT 0.386 (0.393)\tDT 0.000 (0.030)\tloss 3.934 (3.895)\tAcc@1 8.984 (10.394)\n",
            "Train: [2][120/196]\tBT 0.387 (0.393)\tDT 0.000 (0.030)\tloss 3.710 (3.886)\tAcc@1 12.891 (10.511)\n",
            "Train: [2][130/196]\tBT 0.389 (0.393)\tDT 0.000 (0.029)\tloss 3.661 (3.877)\tAcc@1 12.500 (10.592)\n",
            "Train: [2][140/196]\tBT 0.386 (0.392)\tDT 0.000 (0.029)\tloss 3.767 (3.868)\tAcc@1 12.891 (10.698)\n",
            "Train: [2][150/196]\tBT 0.389 (0.392)\tDT 0.000 (0.029)\tloss 3.739 (3.860)\tAcc@1 10.547 (10.745)\n",
            "Train: [2][160/196]\tBT 0.388 (0.392)\tDT 0.000 (0.029)\tloss 3.611 (3.853)\tAcc@1 11.719 (10.869)\n",
            "Train: [2][170/196]\tBT 0.386 (0.391)\tDT 0.000 (0.028)\tloss 3.869 (3.849)\tAcc@1 12.109 (10.960)\n",
            "Train: [2][180/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 3.881 (3.845)\tAcc@1 8.984 (10.992)\n",
            "Train: [2][190/196]\tBT 0.386 (0.391)\tDT 0.000 (0.028)\tloss 3.783 (3.838)\tAcc@1 10.156 (11.020)\n",
            "epoch 2, total time 76.57\n",
            "Test: [0/40]\tTime 0.454 (0.454)\tLoss 3.5910 (3.5910)\tAcc@1 14.453 (14.453)\n",
            "Test: [10/40]\tTime 0.121 (0.153)\tLoss 3.5963 (3.5695)\tAcc@1 12.891 (14.205)\n",
            "Test: [20/40]\tTime 0.122 (0.138)\tLoss 3.5218 (3.5780)\tAcc@1 12.891 (13.951)\n",
            "Test: [30/40]\tTime 0.122 (0.133)\tLoss 3.7798 (3.5919)\tAcc@1 13.672 (14.100)\n",
            " * Acc@1 14.180\n",
            "Train: [3][10/196]\tBT 0.386 (0.437)\tDT 0.000 (0.075)\tloss 3.611 (3.686)\tAcc@1 14.844 (13.281)\n",
            "Train: [3][20/196]\tBT 0.386 (0.411)\tDT 0.000 (0.050)\tloss 3.642 (3.676)\tAcc@1 14.453 (13.652)\n",
            "Train: [3][30/196]\tBT 0.387 (0.403)\tDT 0.000 (0.042)\tloss 3.585 (3.653)\tAcc@1 11.719 (13.763)\n",
            "Train: [3][40/196]\tBT 0.387 (0.399)\tDT 0.000 (0.037)\tloss 3.631 (3.653)\tAcc@1 15.625 (13.828)\n",
            "Train: [3][50/196]\tBT 0.389 (0.397)\tDT 0.000 (0.035)\tloss 3.750 (3.645)\tAcc@1 13.672 (13.992)\n",
            "Train: [3][60/196]\tBT 0.387 (0.395)\tDT 0.000 (0.033)\tloss 3.696 (3.643)\tAcc@1 14.062 (14.017)\n",
            "Train: [3][70/196]\tBT 0.387 (0.394)\tDT 0.000 (0.032)\tloss 3.632 (3.637)\tAcc@1 14.453 (14.118)\n",
            "Train: [3][80/196]\tBT 0.387 (0.393)\tDT 0.000 (0.031)\tloss 3.571 (3.626)\tAcc@1 16.016 (14.326)\n",
            "Train: [3][90/196]\tBT 0.388 (0.393)\tDT 0.000 (0.030)\tloss 3.473 (3.618)\tAcc@1 17.578 (14.666)\n",
            "Train: [3][100/196]\tBT 0.387 (0.392)\tDT 0.000 (0.030)\tloss 3.504 (3.620)\tAcc@1 14.844 (14.602)\n",
            "Train: [3][110/196]\tBT 0.384 (0.392)\tDT 0.000 (0.029)\tloss 3.476 (3.621)\tAcc@1 16.797 (14.567)\n",
            "Train: [3][120/196]\tBT 0.388 (0.391)\tDT 0.000 (0.029)\tloss 3.609 (3.616)\tAcc@1 14.844 (14.688)\n",
            "Train: [3][130/196]\tBT 0.387 (0.391)\tDT 0.000 (0.029)\tloss 3.520 (3.607)\tAcc@1 14.453 (14.727)\n",
            "Train: [3][140/196]\tBT 0.390 (0.391)\tDT 0.000 (0.028)\tloss 3.345 (3.600)\tAcc@1 19.531 (14.849)\n",
            "Train: [3][150/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 3.532 (3.596)\tAcc@1 17.578 (14.846)\n",
            "Train: [3][160/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 3.597 (3.591)\tAcc@1 12.109 (14.871)\n",
            "Train: [3][170/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 3.375 (3.587)\tAcc@1 16.016 (14.943)\n",
            "Train: [3][180/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 3.665 (3.583)\tAcc@1 11.719 (15.074)\n",
            "Train: [3][190/196]\tBT 0.385 (0.390)\tDT 0.000 (0.028)\tloss 3.470 (3.578)\tAcc@1 14.844 (15.130)\n",
            "epoch 3, total time 76.44\n",
            "Test: [0/40]\tTime 0.395 (0.395)\tLoss 6.6417 (6.6417)\tAcc@1 11.719 (11.719)\n",
            "Test: [10/40]\tTime 0.122 (0.148)\tLoss 6.5846 (6.5993)\tAcc@1 9.766 (10.440)\n",
            "Test: [20/40]\tTime 0.122 (0.136)\tLoss 6.1053 (6.5983)\tAcc@1 14.062 (10.826)\n",
            "Test: [30/40]\tTime 0.122 (0.131)\tLoss 6.8953 (6.5463)\tAcc@1 8.594 (10.723)\n",
            " * Acc@1 10.610\n",
            "Train: [4][10/196]\tBT 0.385 (0.442)\tDT 0.000 (0.079)\tloss 3.557 (3.603)\tAcc@1 16.016 (14.102)\n",
            "Train: [4][20/196]\tBT 0.386 (0.414)\tDT 0.000 (0.052)\tloss 3.569 (3.591)\tAcc@1 16.406 (14.473)\n",
            "Train: [4][30/196]\tBT 0.386 (0.405)\tDT 0.000 (0.043)\tloss 3.427 (3.569)\tAcc@1 16.797 (14.753)\n",
            "Train: [4][40/196]\tBT 0.390 (0.401)\tDT 0.000 (0.038)\tloss 3.427 (3.554)\tAcc@1 16.797 (15.137)\n",
            "Train: [4][50/196]\tBT 0.384 (0.398)\tDT 0.000 (0.036)\tloss 3.401 (3.530)\tAcc@1 18.750 (15.719)\n",
            "Train: [4][60/196]\tBT 0.387 (0.396)\tDT 0.000 (0.034)\tloss 3.525 (3.516)\tAcc@1 12.500 (15.990)\n",
            "Train: [4][70/196]\tBT 0.387 (0.395)\tDT 0.000 (0.033)\tloss 3.320 (3.502)\tAcc@1 19.141 (16.083)\n",
            "Train: [4][80/196]\tBT 0.388 (0.394)\tDT 0.000 (0.032)\tloss 3.398 (3.490)\tAcc@1 17.578 (16.279)\n",
            "Train: [4][90/196]\tBT 0.391 (0.393)\tDT 0.000 (0.031)\tloss 3.366 (3.474)\tAcc@1 19.531 (16.606)\n",
            "Train: [4][100/196]\tBT 0.387 (0.393)\tDT 0.000 (0.030)\tloss 3.365 (3.455)\tAcc@1 19.922 (17.031)\n",
            "Train: [4][110/196]\tBT 0.387 (0.392)\tDT 0.000 (0.030)\tloss 3.348 (3.448)\tAcc@1 19.922 (17.180)\n",
            "Train: [4][120/196]\tBT 0.388 (0.392)\tDT 0.000 (0.029)\tloss 3.241 (3.435)\tAcc@1 21.484 (17.454)\n",
            "Train: [4][130/196]\tBT 0.387 (0.391)\tDT 0.000 (0.029)\tloss 3.414 (3.427)\tAcc@1 17.188 (17.623)\n",
            "Train: [4][140/196]\tBT 0.386 (0.391)\tDT 0.000 (0.029)\tloss 3.413 (3.422)\tAcc@1 17.188 (17.715)\n",
            "Train: [4][150/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 3.238 (3.413)\tAcc@1 21.875 (17.948)\n",
            "Train: [4][160/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 3.301 (3.405)\tAcc@1 17.188 (18.015)\n",
            "Train: [4][170/196]\tBT 0.388 (0.390)\tDT 0.000 (0.028)\tloss 3.206 (3.398)\tAcc@1 20.703 (18.185)\n",
            "Train: [4][180/196]\tBT 0.388 (0.390)\tDT 0.000 (0.028)\tloss 3.263 (3.388)\tAcc@1 19.531 (18.340)\n",
            "Train: [4][190/196]\tBT 0.385 (0.390)\tDT 0.000 (0.028)\tloss 3.390 (3.382)\tAcc@1 21.484 (18.507)\n",
            "epoch 4, total time 76.47\n",
            "Test: [0/40]\tTime 0.432 (0.432)\tLoss 3.1890 (3.1890)\tAcc@1 23.438 (23.438)\n",
            "Test: [10/40]\tTime 0.122 (0.151)\tLoss 3.2903 (3.2791)\tAcc@1 19.922 (20.135)\n",
            "Test: [20/40]\tTime 0.122 (0.137)\tLoss 3.2290 (3.2900)\tAcc@1 18.359 (20.312)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 3.4904 (3.2982)\tAcc@1 18.359 (20.086)\n",
            " * Acc@1 20.160\n",
            "Train: [5][10/196]\tBT 0.387 (0.429)\tDT 0.000 (0.066)\tloss 3.245 (3.279)\tAcc@1 18.750 (20.312)\n",
            "Train: [5][20/196]\tBT 0.387 (0.408)\tDT 0.000 (0.046)\tloss 3.353 (3.323)\tAcc@1 17.969 (19.883)\n",
            "Train: [5][30/196]\tBT 0.384 (0.401)\tDT 0.000 (0.039)\tloss 3.399 (3.330)\tAcc@1 17.578 (19.805)\n",
            "Train: [5][40/196]\tBT 0.391 (0.398)\tDT 0.000 (0.035)\tloss 3.116 (3.301)\tAcc@1 20.312 (20.127)\n",
            "Train: [5][50/196]\tBT 0.389 (0.396)\tDT 0.000 (0.033)\tloss 3.183 (3.289)\tAcc@1 16.797 (20.266)\n",
            "Train: [5][60/196]\tBT 0.388 (0.394)\tDT 0.000 (0.032)\tloss 3.220 (3.271)\tAcc@1 21.875 (20.677)\n",
            "Train: [5][70/196]\tBT 0.388 (0.393)\tDT 0.000 (0.031)\tloss 3.352 (3.266)\tAcc@1 21.484 (20.893)\n",
            "Train: [5][80/196]\tBT 0.388 (0.393)\tDT 0.000 (0.030)\tloss 3.247 (3.255)\tAcc@1 19.922 (21.016)\n",
            "Train: [5][90/196]\tBT 0.389 (0.392)\tDT 0.000 (0.029)\tloss 3.091 (3.244)\tAcc@1 22.656 (21.289)\n",
            "Train: [5][100/196]\tBT 0.386 (0.392)\tDT 0.000 (0.029)\tloss 3.370 (3.238)\tAcc@1 17.969 (21.320)\n",
            "Train: [5][110/196]\tBT 0.388 (0.391)\tDT 0.000 (0.029)\tloss 3.395 (3.252)\tAcc@1 20.703 (21.001)\n",
            "Train: [5][120/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 3.473 (3.259)\tAcc@1 14.844 (20.837)\n",
            "Train: [5][130/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 3.212 (3.262)\tAcc@1 21.484 (20.730)\n",
            "Train: [5][140/196]\tBT 0.386 (0.390)\tDT 0.000 (0.028)\tloss 3.240 (3.260)\tAcc@1 21.484 (20.751)\n",
            "Train: [5][150/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 3.204 (3.262)\tAcc@1 19.141 (20.672)\n",
            "Train: [5][160/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 3.152 (3.256)\tAcc@1 21.875 (20.740)\n",
            "Train: [5][170/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 3.366 (3.253)\tAcc@1 23.047 (20.761)\n",
            "Train: [5][180/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 3.160 (3.248)\tAcc@1 24.609 (20.877)\n",
            "Train: [5][190/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 3.032 (3.241)\tAcc@1 21.094 (21.014)\n",
            "epoch 5, total time 76.37\n",
            "Test: [0/40]\tTime 0.401 (0.401)\tLoss 3.0143 (3.0143)\tAcc@1 26.172 (26.172)\n",
            "Test: [10/40]\tTime 0.122 (0.148)\tLoss 3.0609 (3.0478)\tAcc@1 20.703 (23.757)\n",
            "Test: [20/40]\tTime 0.122 (0.136)\tLoss 2.9514 (3.0573)\tAcc@1 26.562 (23.958)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 3.3008 (3.0670)\tAcc@1 17.969 (23.866)\n",
            " * Acc@1 23.940\n",
            "Train: [6][10/196]\tBT 0.386 (0.443)\tDT 0.000 (0.081)\tloss 3.155 (3.115)\tAcc@1 25.391 (21.953)\n",
            "Train: [6][20/196]\tBT 0.387 (0.415)\tDT 0.000 (0.053)\tloss 3.167 (3.087)\tAcc@1 29.688 (23.516)\n",
            "Train: [6][30/196]\tBT 0.386 (0.406)\tDT 0.000 (0.044)\tloss 3.051 (3.077)\tAcc@1 21.094 (23.776)\n",
            "Train: [6][40/196]\tBT 0.388 (0.402)\tDT 0.000 (0.039)\tloss 3.078 (3.078)\tAcc@1 23.438 (23.809)\n",
            "Train: [6][50/196]\tBT 0.387 (0.399)\tDT 0.000 (0.036)\tloss 2.842 (3.057)\tAcc@1 27.344 (23.945)\n",
            "Train: [6][60/196]\tBT 0.386 (0.397)\tDT 0.000 (0.034)\tloss 2.968 (3.049)\tAcc@1 26.172 (24.219)\n",
            "Train: [6][70/196]\tBT 0.387 (0.396)\tDT 0.000 (0.033)\tloss 2.924 (3.038)\tAcc@1 26.953 (24.425)\n",
            "Train: [6][80/196]\tBT 0.388 (0.395)\tDT 0.000 (0.032)\tloss 2.952 (3.030)\tAcc@1 26.562 (24.575)\n",
            "Train: [6][90/196]\tBT 0.386 (0.394)\tDT 0.000 (0.031)\tloss 2.957 (3.031)\tAcc@1 26.953 (24.640)\n",
            "Train: [6][100/196]\tBT 0.388 (0.393)\tDT 0.000 (0.031)\tloss 2.898 (3.026)\tAcc@1 26.953 (24.867)\n",
            "Train: [6][110/196]\tBT 0.389 (0.393)\tDT 0.000 (0.030)\tloss 3.036 (3.020)\tAcc@1 23.438 (25.014)\n",
            "Train: [6][120/196]\tBT 0.384 (0.393)\tDT 0.000 (0.030)\tloss 3.130 (3.014)\tAcc@1 23.438 (25.179)\n",
            "Train: [6][130/196]\tBT 0.385 (0.392)\tDT 0.000 (0.029)\tloss 3.003 (3.009)\tAcc@1 26.172 (25.225)\n",
            "Train: [6][140/196]\tBT 0.386 (0.392)\tDT 0.000 (0.029)\tloss 2.982 (3.002)\tAcc@1 27.734 (25.312)\n",
            "Train: [6][150/196]\tBT 0.387 (0.392)\tDT 0.000 (0.029)\tloss 2.968 (2.998)\tAcc@1 27.344 (25.385)\n",
            "Train: [6][160/196]\tBT 0.387 (0.391)\tDT 0.000 (0.029)\tloss 2.747 (2.992)\tAcc@1 31.250 (25.449)\n",
            "Train: [6][170/196]\tBT 0.384 (0.391)\tDT 0.000 (0.028)\tloss 2.904 (2.986)\tAcc@1 26.172 (25.515)\n",
            "Train: [6][180/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 2.802 (2.978)\tAcc@1 30.078 (25.638)\n",
            "Train: [6][190/196]\tBT 0.391 (0.391)\tDT 0.000 (0.028)\tloss 2.952 (2.977)\tAcc@1 26.953 (25.668)\n",
            "epoch 6, total time 76.58\n",
            "Test: [0/40]\tTime 0.428 (0.428)\tLoss 2.9845 (2.9845)\tAcc@1 25.781 (25.781)\n",
            "Test: [10/40]\tTime 0.122 (0.150)\tLoss 3.1430 (3.1511)\tAcc@1 26.172 (24.751)\n",
            "Test: [20/40]\tTime 0.121 (0.137)\tLoss 2.8453 (3.1121)\tAcc@1 26.953 (25.539)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 3.2668 (3.1153)\tAcc@1 19.922 (25.113)\n",
            " * Acc@1 25.130\n",
            "Train: [7][10/196]\tBT 0.386 (0.425)\tDT 0.000 (0.067)\tloss 2.774 (2.891)\tAcc@1 28.516 (27.656)\n",
            "Train: [7][20/196]\tBT 0.383 (0.406)\tDT 0.000 (0.046)\tloss 2.843 (2.875)\tAcc@1 32.031 (28.203)\n",
            "Train: [7][30/196]\tBT 0.388 (0.400)\tDT 0.000 (0.039)\tloss 2.792 (2.830)\tAcc@1 33.203 (28.672)\n",
            "Train: [7][40/196]\tBT 0.386 (0.397)\tDT 0.000 (0.035)\tloss 2.744 (2.821)\tAcc@1 29.688 (28.770)\n",
            "Train: [7][50/196]\tBT 0.392 (0.395)\tDT 0.000 (0.033)\tloss 2.644 (2.803)\tAcc@1 33.203 (29.219)\n",
            "Train: [7][60/196]\tBT 0.387 (0.394)\tDT 0.000 (0.032)\tloss 2.724 (2.804)\tAcc@1 27.734 (29.128)\n",
            "Train: [7][70/196]\tBT 0.388 (0.393)\tDT 0.000 (0.031)\tloss 2.776 (2.799)\tAcc@1 27.344 (29.096)\n",
            "Train: [7][80/196]\tBT 0.387 (0.392)\tDT 0.000 (0.030)\tloss 2.794 (2.797)\tAcc@1 27.734 (29.043)\n",
            "Train: [7][90/196]\tBT 0.388 (0.392)\tDT 0.000 (0.030)\tloss 2.629 (2.784)\tAcc@1 32.812 (29.410)\n",
            "Train: [7][100/196]\tBT 0.387 (0.391)\tDT 0.000 (0.029)\tloss 2.876 (2.777)\tAcc@1 25.391 (29.559)\n",
            "Train: [7][110/196]\tBT 0.387 (0.391)\tDT 0.000 (0.029)\tloss 2.680 (2.773)\tAcc@1 30.859 (29.659)\n",
            "Train: [7][120/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 2.699 (2.767)\tAcc@1 28.125 (29.811)\n",
            "Train: [7][130/196]\tBT 0.389 (0.391)\tDT 0.000 (0.028)\tloss 2.503 (2.760)\tAcc@1 34.375 (29.886)\n",
            "Train: [7][140/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 2.656 (2.757)\tAcc@1 33.203 (29.947)\n",
            "Train: [7][150/196]\tBT 0.389 (0.390)\tDT 0.000 (0.028)\tloss 2.844 (2.755)\tAcc@1 29.297 (29.992)\n",
            "Train: [7][160/196]\tBT 0.385 (0.390)\tDT 0.000 (0.028)\tloss 2.982 (2.748)\tAcc@1 23.828 (30.161)\n",
            "Train: [7][170/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 2.602 (2.743)\tAcc@1 35.547 (30.326)\n",
            "Train: [7][180/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 2.646 (2.737)\tAcc@1 32.812 (30.462)\n",
            "Train: [7][190/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 2.397 (2.734)\tAcc@1 37.500 (30.522)\n",
            "epoch 7, total time 76.38\n",
            "Test: [0/40]\tTime 0.399 (0.399)\tLoss 2.5804 (2.5804)\tAcc@1 33.203 (33.203)\n",
            "Test: [10/40]\tTime 0.122 (0.148)\tLoss 2.6950 (2.6125)\tAcc@1 26.172 (31.463)\n",
            "Test: [20/40]\tTime 0.122 (0.136)\tLoss 2.4533 (2.6215)\tAcc@1 32.812 (31.566)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 2.8033 (2.6176)\tAcc@1 29.297 (31.666)\n",
            " * Acc@1 32.140\n",
            "Train: [8][10/196]\tBT 0.386 (0.435)\tDT 0.000 (0.073)\tloss 2.771 (2.649)\tAcc@1 30.469 (31.758)\n",
            "Train: [8][20/196]\tBT 0.386 (0.411)\tDT 0.000 (0.049)\tloss 2.589 (2.608)\tAcc@1 31.641 (32.695)\n",
            "Train: [8][30/196]\tBT 0.387 (0.403)\tDT 0.000 (0.041)\tloss 2.567 (2.601)\tAcc@1 33.203 (32.630)\n",
            "Train: [8][40/196]\tBT 0.387 (0.399)\tDT 0.000 (0.037)\tloss 2.530 (2.594)\tAcc@1 33.984 (33.242)\n",
            "Train: [8][50/196]\tBT 0.388 (0.397)\tDT 0.000 (0.035)\tloss 2.779 (2.594)\tAcc@1 34.375 (33.359)\n",
            "Train: [8][60/196]\tBT 0.386 (0.395)\tDT 0.000 (0.033)\tloss 2.575 (2.587)\tAcc@1 36.328 (33.529)\n",
            "Train: [8][70/196]\tBT 0.387 (0.394)\tDT 0.000 (0.032)\tloss 2.541 (2.585)\tAcc@1 37.500 (33.622)\n",
            "Train: [8][80/196]\tBT 0.388 (0.393)\tDT 0.000 (0.031)\tloss 2.684 (2.585)\tAcc@1 33.203 (33.638)\n",
            "Train: [8][90/196]\tBT 0.389 (0.393)\tDT 0.000 (0.030)\tloss 2.509 (2.586)\tAcc@1 35.156 (33.524)\n",
            "Train: [8][100/196]\tBT 0.382 (0.392)\tDT 0.000 (0.030)\tloss 2.568 (2.582)\tAcc@1 35.938 (33.660)\n",
            "Train: [8][110/196]\tBT 0.387 (0.392)\tDT 0.000 (0.029)\tloss 2.668 (2.579)\tAcc@1 31.641 (33.665)\n",
            "Train: [8][120/196]\tBT 0.385 (0.392)\tDT 0.000 (0.029)\tloss 2.442 (2.578)\tAcc@1 39.453 (33.669)\n",
            "Train: [8][130/196]\tBT 0.389 (0.391)\tDT 0.000 (0.029)\tloss 2.544 (2.573)\tAcc@1 38.672 (33.822)\n",
            "Train: [8][140/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 2.298 (2.563)\tAcc@1 39.453 (34.065)\n",
            "Train: [8][150/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 2.579 (2.561)\tAcc@1 32.031 (34.060)\n",
            "Train: [8][160/196]\tBT 0.389 (0.391)\tDT 0.000 (0.028)\tloss 2.449 (2.558)\tAcc@1 35.547 (34.146)\n",
            "Train: [8][170/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 2.603 (2.551)\tAcc@1 36.328 (34.322)\n",
            "Train: [8][180/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 2.498 (2.550)\tAcc@1 35.156 (34.327)\n",
            "Train: [8][190/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 2.535 (2.548)\tAcc@1 33.594 (34.381)\n",
            "epoch 8, total time 76.49\n",
            "Test: [0/40]\tTime 0.401 (0.401)\tLoss 2.4680 (2.4680)\tAcc@1 36.719 (36.719)\n",
            "Test: [10/40]\tTime 0.122 (0.148)\tLoss 2.6317 (2.5246)\tAcc@1 32.422 (34.482)\n",
            "Test: [20/40]\tTime 0.122 (0.136)\tLoss 2.3655 (2.5308)\tAcc@1 37.891 (34.970)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 2.7933 (2.5384)\tAcc@1 31.250 (34.955)\n",
            " * Acc@1 34.960\n",
            "Train: [9][10/196]\tBT 0.384 (0.434)\tDT 0.000 (0.071)\tloss 2.543 (2.457)\tAcc@1 31.250 (36.172)\n",
            "Train: [9][20/196]\tBT 0.387 (0.411)\tDT 0.000 (0.048)\tloss 2.469 (2.440)\tAcc@1 37.109 (36.445)\n",
            "Train: [9][30/196]\tBT 0.388 (0.403)\tDT 0.000 (0.040)\tloss 2.273 (2.442)\tAcc@1 39.062 (36.159)\n",
            "Train: [9][40/196]\tBT 0.387 (0.399)\tDT 0.000 (0.036)\tloss 2.630 (2.436)\tAcc@1 29.688 (36.328)\n",
            "Train: [9][50/196]\tBT 0.389 (0.397)\tDT 0.000 (0.034)\tloss 2.311 (2.436)\tAcc@1 39.062 (36.094)\n",
            "Train: [9][60/196]\tBT 0.386 (0.395)\tDT 0.000 (0.033)\tloss 2.383 (2.430)\tAcc@1 39.062 (36.230)\n",
            "Train: [9][70/196]\tBT 0.389 (0.394)\tDT 0.000 (0.032)\tloss 2.410 (2.436)\tAcc@1 33.984 (36.044)\n",
            "Train: [9][80/196]\tBT 0.388 (0.394)\tDT 0.000 (0.031)\tloss 2.511 (2.432)\tAcc@1 35.156 (36.172)\n",
            "Train: [9][90/196]\tBT 0.386 (0.393)\tDT 0.000 (0.030)\tloss 2.327 (2.432)\tAcc@1 40.234 (36.037)\n",
            "Train: [9][100/196]\tBT 0.388 (0.392)\tDT 0.000 (0.030)\tloss 2.341 (2.425)\tAcc@1 39.453 (36.250)\n",
            "Train: [9][110/196]\tBT 0.385 (0.392)\tDT 0.000 (0.029)\tloss 2.166 (2.418)\tAcc@1 43.750 (36.495)\n",
            "Train: [9][120/196]\tBT 0.388 (0.392)\tDT 0.000 (0.029)\tloss 2.340 (2.412)\tAcc@1 39.844 (36.725)\n",
            "Train: [9][130/196]\tBT 0.386 (0.391)\tDT 0.000 (0.029)\tloss 2.428 (2.412)\tAcc@1 37.500 (36.668)\n",
            "Train: [9][140/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 2.505 (2.409)\tAcc@1 34.375 (36.680)\n",
            "Train: [9][150/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 2.391 (2.410)\tAcc@1 40.234 (36.753)\n",
            "Train: [9][160/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 2.412 (2.408)\tAcc@1 35.156 (36.880)\n",
            "Train: [9][170/196]\tBT 0.389 (0.391)\tDT 0.000 (0.028)\tloss 2.331 (2.403)\tAcc@1 40.234 (37.031)\n",
            "Train: [9][180/196]\tBT 0.389 (0.390)\tDT 0.000 (0.028)\tloss 2.377 (2.401)\tAcc@1 39.453 (37.120)\n",
            "Train: [9][190/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 2.418 (2.400)\tAcc@1 37.891 (37.122)\n",
            "epoch 9, total time 76.47\n",
            "Test: [0/40]\tTime 0.428 (0.428)\tLoss 2.3320 (2.3320)\tAcc@1 40.234 (40.234)\n",
            "Test: [10/40]\tTime 0.122 (0.150)\tLoss 2.3627 (2.2616)\tAcc@1 39.062 (39.666)\n",
            "Test: [20/40]\tTime 0.122 (0.137)\tLoss 2.1127 (2.2646)\tAcc@1 43.359 (39.844)\n",
            "Test: [30/40]\tTime 0.123 (0.132)\tLoss 2.2845 (2.2593)\tAcc@1 39.453 (40.033)\n",
            " * Acc@1 40.150\n",
            "Train: [10][10/196]\tBT 0.384 (0.438)\tDT 0.000 (0.076)\tloss 2.262 (2.330)\tAcc@1 41.797 (39.141)\n",
            "Train: [10][20/196]\tBT 0.389 (0.413)\tDT 0.000 (0.050)\tloss 2.272 (2.318)\tAcc@1 41.406 (39.512)\n",
            "Train: [10][30/196]\tBT 0.383 (0.404)\tDT 0.000 (0.042)\tloss 2.381 (2.325)\tAcc@1 35.938 (39.206)\n",
            "Train: [10][40/196]\tBT 0.393 (0.400)\tDT 0.000 (0.038)\tloss 2.290 (2.317)\tAcc@1 37.109 (39.170)\n",
            "Train: [10][50/196]\tBT 0.385 (0.398)\tDT 0.000 (0.035)\tloss 2.163 (2.312)\tAcc@1 41.406 (39.102)\n",
            "Train: [10][60/196]\tBT 0.384 (0.396)\tDT 0.000 (0.033)\tloss 2.209 (2.294)\tAcc@1 41.016 (39.473)\n",
            "Train: [10][70/196]\tBT 0.387 (0.395)\tDT 0.000 (0.032)\tloss 2.399 (2.291)\tAcc@1 39.453 (39.671)\n",
            "Train: [10][80/196]\tBT 0.387 (0.394)\tDT 0.000 (0.031)\tloss 2.522 (2.291)\tAcc@1 37.891 (39.707)\n",
            "Train: [10][90/196]\tBT 0.384 (0.393)\tDT 0.000 (0.030)\tloss 2.186 (2.293)\tAcc@1 41.797 (39.627)\n",
            "Train: [10][100/196]\tBT 0.388 (0.393)\tDT 0.000 (0.030)\tloss 2.284 (2.286)\tAcc@1 39.062 (39.758)\n",
            "Train: [10][110/196]\tBT 0.387 (0.392)\tDT 0.000 (0.029)\tloss 2.442 (2.283)\tAcc@1 32.422 (39.883)\n",
            "Train: [10][120/196]\tBT 0.390 (0.392)\tDT 0.000 (0.029)\tloss 2.357 (2.285)\tAcc@1 37.891 (39.850)\n",
            "Train: [10][130/196]\tBT 0.388 (0.392)\tDT 0.000 (0.029)\tloss 2.241 (2.286)\tAcc@1 41.016 (39.691)\n",
            "Train: [10][140/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 2.135 (2.283)\tAcc@1 42.188 (39.741)\n",
            "Train: [10][150/196]\tBT 0.389 (0.391)\tDT 0.000 (0.028)\tloss 2.110 (2.278)\tAcc@1 42.969 (39.919)\n",
            "Train: [10][160/196]\tBT 0.389 (0.391)\tDT 0.000 (0.028)\tloss 2.302 (2.275)\tAcc@1 41.016 (40.034)\n",
            "Train: [10][170/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 2.195 (2.275)\tAcc@1 41.016 (40.142)\n",
            "Train: [10][180/196]\tBT 0.386 (0.391)\tDT 0.000 (0.028)\tloss 2.080 (2.274)\tAcc@1 46.094 (40.106)\n",
            "Train: [10][190/196]\tBT 0.388 (0.390)\tDT 0.000 (0.028)\tloss 2.152 (2.272)\tAcc@1 41.016 (40.214)\n",
            "epoch 10, total time 76.52\n",
            "Test: [0/40]\tTime 0.429 (0.429)\tLoss 2.3718 (2.3718)\tAcc@1 40.625 (40.625)\n",
            "Test: [10/40]\tTime 0.122 (0.150)\tLoss 2.5575 (2.4047)\tAcc@1 35.156 (39.169)\n",
            "Test: [20/40]\tTime 0.122 (0.137)\tLoss 2.1887 (2.4213)\tAcc@1 39.062 (38.914)\n",
            "Test: [30/40]\tTime 0.123 (0.133)\tLoss 2.6152 (2.4420)\tAcc@1 39.062 (38.647)\n",
            " * Acc@1 38.690\n",
            "Train: [11][10/196]\tBT 0.391 (0.426)\tDT 0.000 (0.063)\tloss 2.233 (2.215)\tAcc@1 38.672 (40.117)\n",
            "Train: [11][20/196]\tBT 0.384 (0.407)\tDT 0.000 (0.044)\tloss 2.370 (2.204)\tAcc@1 39.453 (41.387)\n",
            "Train: [11][30/196]\tBT 0.388 (0.401)\tDT 0.000 (0.037)\tloss 2.193 (2.200)\tAcc@1 38.672 (41.159)\n",
            "Train: [11][40/196]\tBT 0.390 (0.397)\tDT 0.000 (0.034)\tloss 2.090 (2.195)\tAcc@1 44.922 (41.543)\n",
            "Train: [11][50/196]\tBT 0.388 (0.396)\tDT 0.000 (0.032)\tloss 2.312 (2.196)\tAcc@1 41.016 (41.484)\n",
            "Train: [11][60/196]\tBT 0.388 (0.394)\tDT 0.000 (0.031)\tloss 2.105 (2.191)\tAcc@1 41.797 (41.725)\n",
            "Train: [11][70/196]\tBT 0.386 (0.393)\tDT 0.000 (0.030)\tloss 2.113 (2.185)\tAcc@1 42.578 (41.908)\n",
            "Train: [11][80/196]\tBT 0.387 (0.393)\tDT 0.000 (0.029)\tloss 2.207 (2.187)\tAcc@1 39.453 (41.855)\n",
            "Train: [11][90/196]\tBT 0.386 (0.392)\tDT 0.000 (0.029)\tloss 2.146 (2.183)\tAcc@1 41.797 (42.014)\n",
            "Train: [11][100/196]\tBT 0.386 (0.392)\tDT 0.000 (0.029)\tloss 2.155 (2.183)\tAcc@1 41.406 (42.020)\n",
            "Train: [11][110/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 2.064 (2.174)\tAcc@1 44.922 (42.205)\n",
            "Train: [11][120/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 2.157 (2.175)\tAcc@1 40.234 (42.295)\n",
            "Train: [11][130/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 2.191 (2.171)\tAcc@1 42.188 (42.287)\n",
            "Train: [11][140/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 2.065 (2.169)\tAcc@1 42.578 (42.369)\n",
            "Train: [11][150/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 2.299 (2.168)\tAcc@1 41.797 (42.357)\n",
            "Train: [11][160/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 2.088 (2.167)\tAcc@1 44.531 (42.405)\n",
            "Train: [11][170/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 2.117 (2.164)\tAcc@1 44.141 (42.431)\n",
            "Train: [11][180/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 2.233 (2.163)\tAcc@1 41.797 (42.378)\n",
            "Train: [11][190/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 2.072 (2.159)\tAcc@1 41.406 (42.424)\n",
            "epoch 11, total time 76.38\n",
            "Test: [0/40]\tTime 0.443 (0.443)\tLoss 2.0994 (2.0994)\tAcc@1 42.969 (42.969)\n",
            "Test: [10/40]\tTime 0.122 (0.152)\tLoss 2.1765 (2.1043)\tAcc@1 43.359 (44.531)\n",
            "Test: [20/40]\tTime 0.122 (0.138)\tLoss 2.0815 (2.0985)\tAcc@1 42.969 (44.327)\n",
            "Test: [30/40]\tTime 0.122 (0.133)\tLoss 2.1843 (2.0971)\tAcc@1 40.625 (44.443)\n",
            " * Acc@1 44.510\n",
            "Train: [12][10/196]\tBT 0.385 (0.434)\tDT 0.000 (0.072)\tloss 2.064 (2.106)\tAcc@1 42.969 (43.555)\n",
            "Train: [12][20/196]\tBT 0.387 (0.411)\tDT 0.000 (0.049)\tloss 1.946 (2.072)\tAcc@1 47.656 (43.906)\n",
            "Train: [12][30/196]\tBT 0.387 (0.403)\tDT 0.000 (0.041)\tloss 2.211 (2.077)\tAcc@1 42.578 (43.971)\n",
            "Train: [12][40/196]\tBT 0.386 (0.399)\tDT 0.000 (0.037)\tloss 2.019 (2.071)\tAcc@1 47.656 (44.512)\n",
            "Train: [12][50/196]\tBT 0.387 (0.397)\tDT 0.000 (0.034)\tloss 2.061 (2.046)\tAcc@1 44.922 (45.000)\n",
            "Train: [12][60/196]\tBT 0.388 (0.395)\tDT 0.000 (0.033)\tloss 2.033 (2.032)\tAcc@1 44.922 (45.280)\n",
            "Train: [12][70/196]\tBT 0.389 (0.394)\tDT 0.000 (0.032)\tloss 2.152 (2.029)\tAcc@1 43.750 (45.391)\n",
            "Train: [12][80/196]\tBT 0.386 (0.393)\tDT 0.000 (0.031)\tloss 1.947 (2.024)\tAcc@1 48.047 (45.552)\n",
            "Train: [12][90/196]\tBT 0.386 (0.393)\tDT 0.000 (0.030)\tloss 2.036 (2.027)\tAcc@1 42.969 (45.399)\n",
            "Train: [12][100/196]\tBT 0.387 (0.392)\tDT 0.000 (0.030)\tloss 1.850 (2.025)\tAcc@1 48.828 (45.520)\n",
            "Train: [12][110/196]\tBT 0.424 (0.392)\tDT 0.000 (0.029)\tloss 1.908 (2.024)\tAcc@1 44.531 (45.437)\n",
            "Train: [12][120/196]\tBT 0.387 (0.392)\tDT 0.000 (0.029)\tloss 2.057 (2.021)\tAcc@1 48.828 (45.622)\n",
            "Train: [12][130/196]\tBT 0.388 (0.391)\tDT 0.000 (0.029)\tloss 2.022 (2.023)\tAcc@1 46.484 (45.619)\n",
            "Train: [12][140/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 1.881 (2.022)\tAcc@1 49.219 (45.628)\n",
            "Train: [12][150/196]\tBT 0.389 (0.391)\tDT 0.000 (0.028)\tloss 2.006 (2.020)\tAcc@1 44.531 (45.633)\n",
            "Train: [12][160/196]\tBT 0.386 (0.391)\tDT 0.000 (0.028)\tloss 1.961 (2.018)\tAcc@1 50.000 (45.667)\n",
            "Train: [12][170/196]\tBT 0.390 (0.391)\tDT 0.000 (0.028)\tloss 2.067 (2.015)\tAcc@1 43.359 (45.671)\n",
            "Train: [12][180/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 1.998 (2.016)\tAcc@1 46.484 (45.692)\n",
            "Train: [12][190/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.980 (2.014)\tAcc@1 49.609 (45.755)\n",
            "epoch 12, total time 76.48\n",
            "Test: [0/40]\tTime 0.422 (0.422)\tLoss 1.9290 (1.9290)\tAcc@1 49.219 (49.219)\n",
            "Test: [10/40]\tTime 0.122 (0.150)\tLoss 2.1048 (1.9810)\tAcc@1 42.188 (45.987)\n",
            "Test: [20/40]\tTime 0.123 (0.137)\tLoss 1.8452 (1.9708)\tAcc@1 48.047 (46.577)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 2.1398 (1.9712)\tAcc@1 50.391 (46.825)\n",
            " * Acc@1 47.160\n",
            "Train: [13][10/196]\tBT 0.386 (0.430)\tDT 0.000 (0.068)\tloss 1.929 (1.932)\tAcc@1 49.219 (47.891)\n",
            "Train: [13][20/196]\tBT 0.388 (0.409)\tDT 0.000 (0.046)\tloss 1.937 (1.949)\tAcc@1 47.266 (47.363)\n",
            "Train: [13][30/196]\tBT 0.388 (0.402)\tDT 0.000 (0.039)\tloss 1.869 (1.946)\tAcc@1 50.781 (47.630)\n",
            "Train: [13][40/196]\tBT 0.388 (0.398)\tDT 0.000 (0.036)\tloss 1.733 (1.925)\tAcc@1 53.516 (47.920)\n",
            "Train: [13][50/196]\tBT 0.390 (0.396)\tDT 0.000 (0.033)\tloss 1.967 (1.916)\tAcc@1 48.828 (48.266)\n",
            "Train: [13][60/196]\tBT 0.385 (0.395)\tDT 0.000 (0.032)\tloss 2.028 (1.914)\tAcc@1 43.359 (48.359)\n",
            "Train: [13][70/196]\tBT 0.385 (0.394)\tDT 0.000 (0.031)\tloss 1.979 (1.914)\tAcc@1 49.609 (48.320)\n",
            "Train: [13][80/196]\tBT 0.387 (0.393)\tDT 0.000 (0.030)\tloss 1.929 (1.917)\tAcc@1 51.953 (48.291)\n",
            "Train: [13][90/196]\tBT 0.389 (0.393)\tDT 0.000 (0.030)\tloss 2.081 (1.917)\tAcc@1 42.578 (48.303)\n",
            "Train: [13][100/196]\tBT 0.388 (0.392)\tDT 0.000 (0.029)\tloss 1.891 (1.917)\tAcc@1 52.344 (48.355)\n",
            "Train: [13][110/196]\tBT 0.387 (0.392)\tDT 0.000 (0.029)\tloss 2.060 (1.914)\tAcc@1 44.141 (48.366)\n",
            "Train: [13][120/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 1.850 (1.914)\tAcc@1 51.953 (48.343)\n",
            "Train: [13][130/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 2.009 (1.920)\tAcc@1 50.000 (48.281)\n",
            "Train: [13][140/196]\tBT 0.390 (0.391)\tDT 0.000 (0.028)\tloss 1.890 (1.920)\tAcc@1 48.047 (48.306)\n",
            "Train: [13][150/196]\tBT 0.385 (0.391)\tDT 0.000 (0.028)\tloss 1.844 (1.920)\tAcc@1 48.828 (48.229)\n",
            "Train: [13][160/196]\tBT 0.386 (0.390)\tDT 0.000 (0.028)\tloss 1.826 (1.923)\tAcc@1 48.828 (48.164)\n",
            "Train: [13][170/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 1.695 (1.924)\tAcc@1 50.781 (48.208)\n",
            "Train: [13][180/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 1.810 (1.921)\tAcc@1 48.828 (48.257)\n",
            "Train: [13][190/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.937 (1.921)\tAcc@1 50.000 (48.320)\n",
            "epoch 13, total time 76.44\n",
            "Test: [0/40]\tTime 0.397 (0.397)\tLoss 1.8039 (1.8039)\tAcc@1 51.953 (51.953)\n",
            "Test: [10/40]\tTime 0.122 (0.148)\tLoss 1.8626 (1.8325)\tAcc@1 50.000 (50.213)\n",
            "Test: [20/40]\tTime 0.122 (0.136)\tLoss 1.7061 (1.8355)\tAcc@1 52.734 (49.851)\n",
            "Test: [30/40]\tTime 0.123 (0.131)\tLoss 1.9889 (1.8281)\tAcc@1 47.656 (49.635)\n",
            " * Acc@1 49.850\n",
            "Train: [14][10/196]\tBT 0.384 (0.427)\tDT 0.000 (0.064)\tloss 1.753 (1.768)\tAcc@1 52.344 (52.344)\n",
            "Train: [14][20/196]\tBT 0.387 (0.407)\tDT 0.000 (0.044)\tloss 1.879 (1.797)\tAcc@1 51.562 (50.645)\n",
            "Train: [14][30/196]\tBT 0.389 (0.401)\tDT 0.000 (0.038)\tloss 1.819 (1.800)\tAcc@1 51.953 (50.313)\n",
            "Train: [14][40/196]\tBT 0.390 (0.397)\tDT 0.000 (0.035)\tloss 1.720 (1.816)\tAcc@1 52.734 (50.156)\n",
            "Train: [14][50/196]\tBT 0.388 (0.395)\tDT 0.000 (0.033)\tloss 1.872 (1.814)\tAcc@1 48.438 (49.992)\n",
            "Train: [14][60/196]\tBT 0.389 (0.394)\tDT 0.000 (0.031)\tloss 1.969 (1.830)\tAcc@1 42.969 (49.577)\n",
            "Train: [14][70/196]\tBT 0.389 (0.393)\tDT 0.000 (0.030)\tloss 1.845 (1.829)\tAcc@1 50.000 (49.598)\n",
            "Train: [14][80/196]\tBT 0.386 (0.393)\tDT 0.000 (0.030)\tloss 1.941 (1.834)\tAcc@1 47.656 (49.653)\n",
            "Train: [14][90/196]\tBT 0.387 (0.392)\tDT 0.000 (0.029)\tloss 1.833 (1.830)\tAcc@1 50.391 (49.805)\n",
            "Train: [14][100/196]\tBT 0.388 (0.392)\tDT 0.000 (0.029)\tloss 1.869 (1.831)\tAcc@1 50.391 (49.855)\n",
            "Train: [14][110/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 1.813 (1.825)\tAcc@1 52.344 (50.043)\n",
            "Train: [14][120/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 1.795 (1.821)\tAcc@1 51.172 (50.163)\n",
            "Train: [14][130/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 2.049 (1.820)\tAcc@1 44.141 (50.186)\n",
            "Train: [14][140/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 1.907 (1.817)\tAcc@1 48.047 (50.251)\n",
            "Train: [14][150/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 1.879 (1.819)\tAcc@1 47.656 (50.224)\n",
            "Train: [14][160/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.880 (1.818)\tAcc@1 44.922 (50.251)\n",
            "Train: [14][170/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 1.665 (1.818)\tAcc@1 57.031 (50.296)\n",
            "Train: [14][180/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.800 (1.816)\tAcc@1 50.000 (50.375)\n",
            "Train: [14][190/196]\tBT 0.383 (0.390)\tDT 0.000 (0.027)\tloss 1.779 (1.815)\tAcc@1 51.172 (50.387)\n",
            "epoch 14, total time 76.38\n",
            "Test: [0/40]\tTime 0.429 (0.429)\tLoss 1.8214 (1.8214)\tAcc@1 49.219 (49.219)\n",
            "Test: [10/40]\tTime 0.122 (0.151)\tLoss 1.8967 (1.8247)\tAcc@1 51.172 (49.538)\n",
            "Test: [20/40]\tTime 0.122 (0.137)\tLoss 1.9312 (1.8719)\tAcc@1 47.656 (49.126)\n",
            "Test: [30/40]\tTime 0.123 (0.132)\tLoss 2.1193 (1.8750)\tAcc@1 42.969 (48.841)\n",
            " * Acc@1 49.340\n",
            "Train: [15][10/196]\tBT 0.385 (0.433)\tDT 0.000 (0.070)\tloss 1.892 (1.789)\tAcc@1 46.094 (51.172)\n",
            "Train: [15][20/196]\tBT 0.386 (0.411)\tDT 0.000 (0.048)\tloss 1.705 (1.788)\tAcc@1 56.641 (51.816)\n",
            "Train: [15][30/196]\tBT 0.386 (0.403)\tDT 0.000 (0.040)\tloss 1.830 (1.755)\tAcc@1 51.562 (52.383)\n",
            "Train: [15][40/196]\tBT 0.387 (0.399)\tDT 0.000 (0.036)\tloss 1.854 (1.755)\tAcc@1 46.875 (52.500)\n",
            "Train: [15][50/196]\tBT 0.387 (0.397)\tDT 0.000 (0.034)\tloss 1.766 (1.758)\tAcc@1 46.875 (52.078)\n",
            "Train: [15][60/196]\tBT 0.387 (0.395)\tDT 0.000 (0.032)\tloss 1.769 (1.748)\tAcc@1 50.391 (52.272)\n",
            "Train: [15][70/196]\tBT 0.387 (0.394)\tDT 0.000 (0.031)\tloss 1.887 (1.743)\tAcc@1 51.172 (52.394)\n",
            "Train: [15][80/196]\tBT 0.390 (0.393)\tDT 0.000 (0.031)\tloss 1.811 (1.744)\tAcc@1 48.828 (52.314)\n",
            "Train: [15][90/196]\tBT 0.387 (0.393)\tDT 0.000 (0.030)\tloss 1.625 (1.744)\tAcc@1 53.516 (52.357)\n",
            "Train: [15][100/196]\tBT 0.388 (0.392)\tDT 0.000 (0.029)\tloss 1.619 (1.738)\tAcc@1 54.688 (52.566)\n",
            "Train: [15][110/196]\tBT 0.386 (0.392)\tDT 0.000 (0.029)\tloss 1.869 (1.740)\tAcc@1 47.656 (52.418)\n",
            "Train: [15][120/196]\tBT 0.386 (0.391)\tDT 0.000 (0.029)\tloss 1.699 (1.738)\tAcc@1 51.562 (52.428)\n",
            "Train: [15][130/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 1.847 (1.740)\tAcc@1 45.703 (52.311)\n",
            "Train: [15][140/196]\tBT 0.391 (0.391)\tDT 0.000 (0.028)\tloss 1.932 (1.745)\tAcc@1 47.266 (52.143)\n",
            "Train: [15][150/196]\tBT 0.386 (0.391)\tDT 0.000 (0.028)\tloss 1.569 (1.749)\tAcc@1 55.859 (52.104)\n",
            "Train: [15][160/196]\tBT 0.386 (0.391)\tDT 0.000 (0.028)\tloss 1.895 (1.748)\tAcc@1 49.219 (52.126)\n",
            "Train: [15][170/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 2.033 (1.750)\tAcc@1 45.703 (52.043)\n",
            "Train: [15][180/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.693 (1.750)\tAcc@1 56.641 (52.051)\n",
            "Train: [15][190/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 1.819 (1.751)\tAcc@1 51.562 (52.046)\n",
            "epoch 15, total time 76.47\n",
            "Test: [0/40]\tTime 0.412 (0.412)\tLoss 1.9404 (1.9404)\tAcc@1 51.562 (51.562)\n",
            "Test: [10/40]\tTime 0.122 (0.149)\tLoss 2.0313 (1.9572)\tAcc@1 49.609 (50.568)\n",
            "Test: [20/40]\tTime 0.122 (0.136)\tLoss 1.9487 (1.9909)\tAcc@1 49.609 (49.349)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 2.1072 (1.9898)\tAcc@1 49.609 (49.383)\n",
            " * Acc@1 49.350\n",
            "Train: [16][10/196]\tBT 0.385 (0.427)\tDT 0.000 (0.062)\tloss 1.521 (1.691)\tAcc@1 58.203 (53.750)\n",
            "Train: [16][20/196]\tBT 0.387 (0.407)\tDT 0.000 (0.044)\tloss 1.595 (1.687)\tAcc@1 54.297 (53.906)\n",
            "Train: [16][30/196]\tBT 0.383 (0.401)\tDT 0.000 (0.037)\tloss 1.884 (1.674)\tAcc@1 47.656 (54.115)\n",
            "Train: [16][40/196]\tBT 0.388 (0.397)\tDT 0.000 (0.034)\tloss 1.478 (1.678)\tAcc@1 58.203 (53.945)\n",
            "Train: [16][50/196]\tBT 0.388 (0.395)\tDT 0.000 (0.032)\tloss 1.480 (1.671)\tAcc@1 58.594 (53.875)\n",
            "Train: [16][60/196]\tBT 0.388 (0.394)\tDT 0.000 (0.031)\tloss 1.792 (1.664)\tAcc@1 52.734 (54.089)\n",
            "Train: [16][70/196]\tBT 0.386 (0.393)\tDT 0.000 (0.030)\tloss 1.598 (1.672)\tAcc@1 51.562 (53.834)\n",
            "Train: [16][80/196]\tBT 0.387 (0.392)\tDT 0.000 (0.030)\tloss 1.592 (1.678)\tAcc@1 57.812 (53.950)\n",
            "Train: [16][90/196]\tBT 0.388 (0.392)\tDT 0.000 (0.029)\tloss 1.942 (1.684)\tAcc@1 46.875 (53.924)\n",
            "Train: [16][100/196]\tBT 0.386 (0.391)\tDT 0.000 (0.029)\tloss 1.636 (1.677)\tAcc@1 53.516 (53.984)\n",
            "Train: [16][110/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 1.934 (1.679)\tAcc@1 49.609 (54.002)\n",
            "Train: [16][120/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 1.455 (1.678)\tAcc@1 58.203 (53.988)\n",
            "Train: [16][130/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 1.674 (1.680)\tAcc@1 53.125 (53.933)\n",
            "Train: [16][140/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 1.784 (1.677)\tAcc@1 49.609 (53.954)\n",
            "Train: [16][150/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 1.807 (1.680)\tAcc@1 50.000 (53.885)\n",
            "Train: [16][160/196]\tBT 0.393 (0.390)\tDT 0.000 (0.027)\tloss 1.654 (1.682)\tAcc@1 56.250 (53.835)\n",
            "Train: [16][170/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 1.684 (1.680)\tAcc@1 53.125 (53.867)\n",
            "Train: [16][180/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 1.659 (1.676)\tAcc@1 54.297 (53.939)\n",
            "Train: [16][190/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 1.625 (1.676)\tAcc@1 58.594 (53.956)\n",
            "epoch 16, total time 76.38\n",
            "Test: [0/40]\tTime 0.384 (0.384)\tLoss 1.7177 (1.7177)\tAcc@1 55.469 (55.469)\n",
            "Test: [10/40]\tTime 0.122 (0.146)\tLoss 1.6939 (1.6840)\tAcc@1 52.344 (53.906)\n",
            "Test: [20/40]\tTime 0.122 (0.135)\tLoss 1.5957 (1.6705)\tAcc@1 53.125 (53.683)\n",
            "Test: [30/40]\tTime 0.122 (0.131)\tLoss 1.8911 (1.6915)\tAcc@1 52.344 (53.453)\n",
            " * Acc@1 53.780\n",
            "Train: [17][10/196]\tBT 0.386 (0.428)\tDT 0.000 (0.066)\tloss 1.616 (1.643)\tAcc@1 57.031 (54.609)\n",
            "Train: [17][20/196]\tBT 0.386 (0.408)\tDT 0.000 (0.046)\tloss 1.490 (1.632)\tAcc@1 59.375 (55.469)\n",
            "Train: [17][30/196]\tBT 0.389 (0.401)\tDT 0.000 (0.039)\tloss 1.604 (1.624)\tAcc@1 55.859 (55.169)\n",
            "Train: [17][40/196]\tBT 0.388 (0.398)\tDT 0.000 (0.035)\tloss 1.462 (1.619)\tAcc@1 57.422 (54.971)\n",
            "Train: [17][50/196]\tBT 0.388 (0.396)\tDT 0.000 (0.033)\tloss 1.543 (1.618)\tAcc@1 57.031 (54.891)\n",
            "Train: [17][60/196]\tBT 0.386 (0.394)\tDT 0.000 (0.032)\tloss 1.659 (1.629)\tAcc@1 53.516 (54.668)\n",
            "Train: [17][70/196]\tBT 0.387 (0.393)\tDT 0.000 (0.031)\tloss 1.646 (1.626)\tAcc@1 53.906 (54.844)\n",
            "Train: [17][80/196]\tBT 0.387 (0.393)\tDT 0.000 (0.030)\tloss 1.717 (1.619)\tAcc@1 52.344 (55.034)\n",
            "Train: [17][90/196]\tBT 0.388 (0.392)\tDT 0.000 (0.029)\tloss 1.723 (1.621)\tAcc@1 55.859 (55.109)\n",
            "Train: [17][100/196]\tBT 0.389 (0.392)\tDT 0.000 (0.029)\tloss 1.598 (1.622)\tAcc@1 56.250 (55.094)\n",
            "Train: [17][110/196]\tBT 0.389 (0.391)\tDT 0.000 (0.029)\tloss 1.859 (1.621)\tAcc@1 53.125 (55.121)\n",
            "Train: [17][120/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 1.623 (1.624)\tAcc@1 53.125 (55.013)\n",
            "Train: [17][130/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 1.590 (1.621)\tAcc@1 55.078 (55.042)\n",
            "Train: [17][140/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 1.520 (1.620)\tAcc@1 54.297 (55.003)\n",
            "Train: [17][150/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 1.708 (1.623)\tAcc@1 55.859 (55.023)\n",
            "Train: [17][160/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.626 (1.619)\tAcc@1 57.812 (55.120)\n",
            "Train: [17][170/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 1.722 (1.615)\tAcc@1 49.609 (55.202)\n",
            "Train: [17][180/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.662 (1.616)\tAcc@1 56.641 (55.174)\n",
            "Train: [17][190/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.586 (1.617)\tAcc@1 57.812 (55.125)\n",
            "epoch 17, total time 76.39\n",
            "Test: [0/40]\tTime 0.423 (0.423)\tLoss 1.5986 (1.5986)\tAcc@1 56.641 (56.641)\n",
            "Test: [10/40]\tTime 0.122 (0.150)\tLoss 1.6603 (1.6349)\tAcc@1 55.469 (55.504)\n",
            "Test: [20/40]\tTime 0.121 (0.137)\tLoss 1.5806 (1.6474)\tAcc@1 56.641 (55.115)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 1.7029 (1.6398)\tAcc@1 55.859 (55.066)\n",
            " * Acc@1 55.340\n",
            "Train: [18][10/196]\tBT 0.391 (0.427)\tDT 0.000 (0.064)\tloss 1.566 (1.574)\tAcc@1 58.594 (56.133)\n",
            "Train: [18][20/196]\tBT 0.388 (0.407)\tDT 0.000 (0.044)\tloss 1.433 (1.546)\tAcc@1 61.328 (56.582)\n",
            "Train: [18][30/196]\tBT 0.387 (0.401)\tDT 0.000 (0.038)\tloss 1.449 (1.515)\tAcc@1 60.938 (57.539)\n",
            "Train: [18][40/196]\tBT 0.387 (0.397)\tDT 0.000 (0.035)\tloss 1.572 (1.535)\tAcc@1 53.906 (57.139)\n",
            "Train: [18][50/196]\tBT 0.386 (0.395)\tDT 0.000 (0.033)\tloss 1.626 (1.545)\tAcc@1 55.859 (56.898)\n",
            "Train: [18][60/196]\tBT 0.387 (0.394)\tDT 0.000 (0.031)\tloss 1.508 (1.550)\tAcc@1 56.641 (56.764)\n",
            "Train: [18][70/196]\tBT 0.389 (0.393)\tDT 0.000 (0.030)\tloss 1.583 (1.554)\tAcc@1 55.469 (56.546)\n",
            "Train: [18][80/196]\tBT 0.382 (0.393)\tDT 0.000 (0.030)\tloss 1.620 (1.559)\tAcc@1 55.078 (56.562)\n",
            "Train: [18][90/196]\tBT 0.377 (0.392)\tDT 0.000 (0.029)\tloss 1.575 (1.561)\tAcc@1 57.031 (56.628)\n",
            "Train: [18][100/196]\tBT 0.390 (0.392)\tDT 0.000 (0.029)\tloss 1.415 (1.560)\tAcc@1 60.156 (56.629)\n",
            "Train: [18][110/196]\tBT 0.386 (0.391)\tDT 0.000 (0.028)\tloss 1.565 (1.562)\tAcc@1 58.203 (56.516)\n",
            "Train: [18][120/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 1.621 (1.561)\tAcc@1 52.734 (56.523)\n",
            "Train: [18][130/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 1.566 (1.562)\tAcc@1 53.125 (56.526)\n",
            "Train: [18][140/196]\tBT 0.389 (0.390)\tDT 0.000 (0.028)\tloss 1.468 (1.561)\tAcc@1 58.984 (56.590)\n",
            "Train: [18][150/196]\tBT 0.385 (0.390)\tDT 0.000 (0.027)\tloss 1.532 (1.562)\tAcc@1 54.688 (56.581)\n",
            "Train: [18][160/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.397 (1.559)\tAcc@1 60.156 (56.680)\n",
            "Train: [18][170/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 1.718 (1.561)\tAcc@1 51.172 (56.638)\n",
            "Train: [18][180/196]\tBT 0.389 (0.390)\tDT 0.000 (0.027)\tloss 1.724 (1.562)\tAcc@1 56.641 (56.660)\n",
            "Train: [18][190/196]\tBT 0.391 (0.390)\tDT 0.000 (0.027)\tloss 1.450 (1.561)\tAcc@1 60.547 (56.637)\n",
            "epoch 18, total time 76.39\n",
            "Test: [0/40]\tTime 0.431 (0.431)\tLoss 1.7972 (1.7972)\tAcc@1 53.906 (53.906)\n",
            "Test: [10/40]\tTime 0.122 (0.151)\tLoss 1.7676 (1.7528)\tAcc@1 52.734 (53.409)\n",
            "Test: [20/40]\tTime 0.122 (0.137)\tLoss 1.5773 (1.7578)\tAcc@1 54.688 (52.734)\n",
            "Test: [30/40]\tTime 0.122 (0.133)\tLoss 1.8935 (1.7645)\tAcc@1 50.000 (52.697)\n",
            " * Acc@1 52.970\n",
            "Train: [19][10/196]\tBT 0.389 (0.431)\tDT 0.000 (0.068)\tloss 1.569 (1.514)\tAcc@1 54.297 (59.258)\n",
            "Train: [19][20/196]\tBT 0.388 (0.409)\tDT 0.000 (0.046)\tloss 1.437 (1.530)\tAcc@1 59.375 (57.754)\n",
            "Train: [19][30/196]\tBT 0.387 (0.402)\tDT 0.000 (0.039)\tloss 1.384 (1.507)\tAcc@1 58.203 (58.138)\n",
            "Train: [19][40/196]\tBT 0.387 (0.398)\tDT 0.000 (0.036)\tloss 1.488 (1.513)\tAcc@1 61.328 (57.930)\n",
            "Train: [19][50/196]\tBT 0.388 (0.396)\tDT 0.000 (0.034)\tloss 1.618 (1.516)\tAcc@1 53.516 (57.867)\n",
            "Train: [19][60/196]\tBT 0.386 (0.395)\tDT 0.000 (0.032)\tloss 1.399 (1.510)\tAcc@1 60.938 (58.040)\n",
            "Train: [19][70/196]\tBT 0.388 (0.394)\tDT 0.000 (0.031)\tloss 1.516 (1.506)\tAcc@1 55.078 (57.969)\n",
            "Train: [19][80/196]\tBT 0.386 (0.393)\tDT 0.000 (0.030)\tloss 1.466 (1.507)\tAcc@1 58.984 (57.910)\n",
            "Train: [19][90/196]\tBT 0.386 (0.392)\tDT 0.000 (0.030)\tloss 1.352 (1.502)\tAcc@1 62.891 (58.134)\n",
            "Train: [19][100/196]\tBT 0.386 (0.392)\tDT 0.000 (0.029)\tloss 1.749 (1.510)\tAcc@1 51.172 (57.879)\n",
            "Train: [19][110/196]\tBT 0.386 (0.391)\tDT 0.000 (0.029)\tloss 1.449 (1.511)\tAcc@1 57.812 (57.834)\n",
            "Train: [19][120/196]\tBT 0.388 (0.391)\tDT 0.000 (0.029)\tloss 1.250 (1.505)\tAcc@1 64.062 (57.819)\n",
            "Train: [19][130/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 1.476 (1.500)\tAcc@1 60.156 (57.987)\n",
            "Train: [19][140/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 1.579 (1.503)\tAcc@1 56.641 (57.919)\n",
            "Train: [19][150/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 1.543 (1.501)\tAcc@1 58.203 (57.982)\n",
            "Train: [19][160/196]\tBT 0.386 (0.390)\tDT 0.000 (0.028)\tloss 1.538 (1.501)\tAcc@1 58.594 (58.057)\n",
            "Train: [19][170/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 1.314 (1.503)\tAcc@1 61.328 (58.109)\n",
            "Train: [19][180/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.423 (1.502)\tAcc@1 58.594 (58.147)\n",
            "Train: [19][190/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 1.608 (1.503)\tAcc@1 55.859 (58.191)\n",
            "epoch 19, total time 76.39\n",
            "Test: [0/40]\tTime 0.420 (0.420)\tLoss 1.6051 (1.6051)\tAcc@1 58.203 (58.203)\n",
            "Test: [10/40]\tTime 0.122 (0.150)\tLoss 1.7055 (1.6455)\tAcc@1 55.078 (55.753)\n",
            "Test: [20/40]\tTime 0.122 (0.137)\tLoss 1.6594 (1.6732)\tAcc@1 55.469 (54.390)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 1.8893 (1.6815)\tAcc@1 51.172 (54.385)\n",
            " * Acc@1 54.500\n",
            "Train: [20][10/196]\tBT 0.385 (0.428)\tDT 0.000 (0.064)\tloss 1.503 (1.415)\tAcc@1 58.984 (59.258)\n",
            "Train: [20][20/196]\tBT 0.387 (0.408)\tDT 0.000 (0.045)\tloss 1.225 (1.412)\tAcc@1 65.625 (59.629)\n",
            "Train: [20][30/196]\tBT 0.388 (0.401)\tDT 0.000 (0.038)\tloss 1.483 (1.436)\tAcc@1 57.812 (59.635)\n",
            "Train: [20][40/196]\tBT 0.388 (0.397)\tDT 0.000 (0.035)\tloss 1.571 (1.442)\tAcc@1 53.906 (59.482)\n",
            "Train: [20][50/196]\tBT 0.386 (0.395)\tDT 0.000 (0.033)\tloss 1.526 (1.442)\tAcc@1 61.328 (59.695)\n",
            "Train: [20][60/196]\tBT 0.386 (0.394)\tDT 0.000 (0.031)\tloss 1.438 (1.439)\tAcc@1 57.812 (59.766)\n",
            "Train: [20][70/196]\tBT 0.387 (0.393)\tDT 0.000 (0.031)\tloss 1.527 (1.450)\tAcc@1 56.641 (59.442)\n",
            "Train: [20][80/196]\tBT 0.391 (0.392)\tDT 0.000 (0.030)\tloss 1.315 (1.446)\tAcc@1 60.547 (59.448)\n",
            "Train: [20][90/196]\tBT 0.387 (0.392)\tDT 0.000 (0.029)\tloss 1.486 (1.450)\tAcc@1 57.031 (59.431)\n",
            "Train: [20][100/196]\tBT 0.387 (0.391)\tDT 0.000 (0.029)\tloss 1.387 (1.451)\tAcc@1 64.062 (59.422)\n",
            "Train: [20][110/196]\tBT 0.386 (0.391)\tDT 0.000 (0.028)\tloss 1.338 (1.451)\tAcc@1 63.672 (59.435)\n",
            "Train: [20][120/196]\tBT 0.385 (0.391)\tDT 0.000 (0.028)\tloss 1.579 (1.454)\tAcc@1 55.469 (59.359)\n",
            "Train: [20][130/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 1.380 (1.456)\tAcc@1 59.766 (59.234)\n",
            "Train: [20][140/196]\tBT 0.388 (0.390)\tDT 0.000 (0.028)\tloss 1.558 (1.457)\tAcc@1 53.516 (59.202)\n",
            "Train: [20][150/196]\tBT 0.388 (0.390)\tDT 0.000 (0.028)\tloss 1.484 (1.457)\tAcc@1 57.031 (59.266)\n",
            "Train: [20][160/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 1.508 (1.459)\tAcc@1 58.203 (59.219)\n",
            "Train: [20][170/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.494 (1.461)\tAcc@1 58.984 (59.196)\n",
            "Train: [20][180/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 1.452 (1.464)\tAcc@1 60.938 (59.128)\n",
            "Train: [20][190/196]\tBT 0.387 (0.389)\tDT 0.000 (0.027)\tloss 1.515 (1.463)\tAcc@1 59.766 (59.155)\n",
            "epoch 20, total time 76.33\n",
            "Test: [0/40]\tTime 0.446 (0.446)\tLoss 1.5836 (1.5836)\tAcc@1 56.250 (56.250)\n",
            "Test: [10/40]\tTime 0.122 (0.152)\tLoss 1.7140 (1.6853)\tAcc@1 54.688 (55.362)\n",
            "Test: [20/40]\tTime 0.122 (0.138)\tLoss 1.5862 (1.6797)\tAcc@1 55.078 (54.892)\n",
            "Test: [30/40]\tTime 0.122 (0.133)\tLoss 1.9226 (1.7095)\tAcc@1 52.734 (54.864)\n",
            " * Acc@1 55.170\n",
            "Train: [21][10/196]\tBT 0.385 (0.424)\tDT 0.000 (0.062)\tloss 1.618 (1.476)\tAcc@1 57.812 (58.594)\n",
            "Train: [21][20/196]\tBT 0.387 (0.406)\tDT 0.000 (0.044)\tloss 1.398 (1.449)\tAcc@1 61.719 (59.512)\n",
            "Train: [21][30/196]\tBT 0.385 (0.400)\tDT 0.000 (0.037)\tloss 1.487 (1.440)\tAcc@1 61.719 (59.831)\n",
            "Train: [21][40/196]\tBT 0.387 (0.396)\tDT 0.000 (0.034)\tloss 1.279 (1.441)\tAcc@1 61.328 (59.990)\n",
            "Train: [21][50/196]\tBT 0.388 (0.395)\tDT 0.000 (0.032)\tloss 1.516 (1.443)\tAcc@1 59.375 (59.945)\n",
            "Train: [21][60/196]\tBT 0.389 (0.393)\tDT 0.000 (0.031)\tloss 1.344 (1.432)\tAcc@1 60.156 (60.000)\n",
            "Train: [21][70/196]\tBT 0.383 (0.393)\tDT 0.000 (0.030)\tloss 1.361 (1.435)\tAcc@1 61.328 (59.760)\n",
            "Train: [21][80/196]\tBT 0.386 (0.392)\tDT 0.000 (0.030)\tloss 1.412 (1.437)\tAcc@1 57.422 (59.766)\n",
            "Train: [21][90/196]\tBT 0.389 (0.391)\tDT 0.000 (0.029)\tloss 1.271 (1.431)\tAcc@1 62.109 (59.891)\n",
            "Train: [21][100/196]\tBT 0.389 (0.391)\tDT 0.000 (0.029)\tloss 1.446 (1.429)\tAcc@1 63.672 (59.977)\n",
            "Train: [21][110/196]\tBT 0.387 (0.391)\tDT 0.000 (0.028)\tloss 1.467 (1.430)\tAcc@1 57.422 (59.893)\n",
            "Train: [21][120/196]\tBT 0.388 (0.390)\tDT 0.000 (0.028)\tloss 1.364 (1.428)\tAcc@1 60.938 (59.909)\n",
            "Train: [21][130/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 1.237 (1.424)\tAcc@1 65.234 (60.006)\n",
            "Train: [21][140/196]\tBT 0.388 (0.390)\tDT 0.000 (0.028)\tloss 1.496 (1.424)\tAcc@1 55.469 (60.039)\n",
            "Train: [21][150/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 1.453 (1.425)\tAcc@1 60.547 (60.013)\n",
            "Train: [21][160/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.402 (1.424)\tAcc@1 62.891 (60.076)\n",
            "Train: [21][170/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.478 (1.424)\tAcc@1 56.641 (60.099)\n",
            "Train: [21][180/196]\tBT 0.388 (0.389)\tDT 0.000 (0.027)\tloss 1.384 (1.422)\tAcc@1 60.547 (60.139)\n",
            "Train: [21][190/196]\tBT 0.387 (0.389)\tDT 0.000 (0.027)\tloss 1.482 (1.423)\tAcc@1 57.422 (60.072)\n",
            "epoch 21, total time 76.30\n",
            "Test: [0/40]\tTime 0.402 (0.402)\tLoss 1.4338 (1.4338)\tAcc@1 63.281 (63.281)\n",
            "Test: [10/40]\tTime 0.122 (0.148)\tLoss 1.4426 (1.5314)\tAcc@1 63.281 (58.629)\n",
            "Test: [20/40]\tTime 0.122 (0.136)\tLoss 1.3826 (1.5379)\tAcc@1 59.375 (58.036)\n",
            "Test: [30/40]\tTime 0.121 (0.131)\tLoss 1.7231 (1.5523)\tAcc@1 56.250 (58.077)\n",
            " * Acc@1 58.140\n",
            "Train: [22][10/196]\tBT 0.386 (0.434)\tDT 0.000 (0.070)\tloss 1.450 (1.371)\tAcc@1 57.422 (62.109)\n",
            "Train: [22][20/196]\tBT 0.388 (0.411)\tDT 0.000 (0.048)\tloss 1.274 (1.346)\tAcc@1 59.766 (61.953)\n",
            "Train: [22][30/196]\tBT 0.385 (0.403)\tDT 0.000 (0.040)\tloss 1.465 (1.368)\tAcc@1 59.766 (61.432)\n",
            "Train: [22][40/196]\tBT 0.386 (0.399)\tDT 0.000 (0.036)\tloss 1.253 (1.372)\tAcc@1 64.062 (61.475)\n",
            "Train: [22][50/196]\tBT 0.388 (0.397)\tDT 0.000 (0.034)\tloss 1.076 (1.370)\tAcc@1 68.750 (61.211)\n",
            "Train: [22][60/196]\tBT 0.387 (0.395)\tDT 0.000 (0.032)\tloss 1.257 (1.364)\tAcc@1 62.109 (61.445)\n",
            "Train: [22][70/196]\tBT 0.387 (0.394)\tDT 0.000 (0.031)\tloss 1.020 (1.365)\tAcc@1 67.969 (61.362)\n",
            "Train: [22][80/196]\tBT 0.392 (0.393)\tDT 0.000 (0.031)\tloss 1.409 (1.363)\tAcc@1 58.203 (61.372)\n",
            "Train: [22][90/196]\tBT 0.387 (0.393)\tDT 0.000 (0.030)\tloss 1.471 (1.366)\tAcc@1 56.250 (61.337)\n",
            "Train: [22][100/196]\tBT 0.386 (0.392)\tDT 0.000 (0.029)\tloss 1.359 (1.365)\tAcc@1 59.766 (61.285)\n",
            "Train: [22][110/196]\tBT 0.386 (0.392)\tDT 0.000 (0.029)\tloss 1.578 (1.367)\tAcc@1 55.469 (61.232)\n",
            "Train: [22][120/196]\tBT 0.387 (0.391)\tDT 0.000 (0.029)\tloss 1.362 (1.367)\tAcc@1 58.203 (61.250)\n",
            "Train: [22][130/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 1.504 (1.369)\tAcc@1 58.594 (61.229)\n",
            "Train: [22][140/196]\tBT 0.386 (0.391)\tDT 0.000 (0.028)\tloss 1.344 (1.369)\tAcc@1 66.406 (61.270)\n",
            "Train: [22][150/196]\tBT 0.385 (0.390)\tDT 0.000 (0.028)\tloss 1.520 (1.373)\tAcc@1 55.859 (61.237)\n",
            "Train: [22][160/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 1.287 (1.371)\tAcc@1 64.062 (61.265)\n",
            "Train: [22][170/196]\tBT 0.383 (0.390)\tDT 0.000 (0.028)\tloss 1.261 (1.368)\tAcc@1 65.234 (61.287)\n",
            "Train: [22][180/196]\tBT 0.385 (0.390)\tDT 0.000 (0.027)\tloss 1.354 (1.372)\tAcc@1 58.203 (61.202)\n",
            "Train: [22][190/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 1.337 (1.373)\tAcc@1 63.281 (61.131)\n",
            "epoch 22, total time 76.39\n",
            "Test: [0/40]\tTime 0.387 (0.387)\tLoss 1.5499 (1.5499)\tAcc@1 60.156 (60.156)\n",
            "Test: [10/40]\tTime 0.122 (0.147)\tLoss 1.4909 (1.5110)\tAcc@1 57.422 (58.913)\n",
            "Test: [20/40]\tTime 0.122 (0.135)\tLoss 1.5043 (1.5272)\tAcc@1 59.375 (58.259)\n",
            "Test: [30/40]\tTime 0.122 (0.131)\tLoss 1.6723 (1.5281)\tAcc@1 55.469 (58.077)\n",
            " * Acc@1 58.410\n",
            "Train: [23][10/196]\tBT 0.383 (0.426)\tDT 0.000 (0.063)\tloss 1.334 (1.307)\tAcc@1 61.719 (62.812)\n",
            "Train: [23][20/196]\tBT 0.388 (0.407)\tDT 0.000 (0.044)\tloss 1.255 (1.308)\tAcc@1 64.844 (62.715)\n",
            "Train: [23][30/196]\tBT 0.386 (0.400)\tDT 0.000 (0.038)\tloss 1.496 (1.322)\tAcc@1 59.375 (62.448)\n",
            "Train: [23][40/196]\tBT 0.386 (0.397)\tDT 0.000 (0.034)\tloss 1.280 (1.343)\tAcc@1 62.109 (61.895)\n",
            "Train: [23][50/196]\tBT 0.386 (0.395)\tDT 0.000 (0.033)\tloss 1.378 (1.350)\tAcc@1 60.547 (61.742)\n",
            "Train: [23][60/196]\tBT 0.388 (0.394)\tDT 0.000 (0.031)\tloss 1.466 (1.352)\tAcc@1 60.156 (61.693)\n",
            "Train: [23][70/196]\tBT 0.385 (0.393)\tDT 0.000 (0.030)\tloss 1.403 (1.356)\tAcc@1 60.547 (61.663)\n",
            "Train: [23][80/196]\tBT 0.386 (0.392)\tDT 0.000 (0.030)\tloss 1.329 (1.362)\tAcc@1 60.938 (61.533)\n",
            "Train: [23][90/196]\tBT 0.384 (0.391)\tDT 0.000 (0.029)\tloss 1.403 (1.364)\tAcc@1 59.766 (61.385)\n",
            "Train: [23][100/196]\tBT 0.386 (0.391)\tDT 0.000 (0.029)\tloss 1.419 (1.369)\tAcc@1 59.375 (61.336)\n",
            "Train: [23][110/196]\tBT 0.386 (0.391)\tDT 0.000 (0.028)\tloss 1.399 (1.369)\tAcc@1 62.109 (61.335)\n",
            "Train: [23][120/196]\tBT 0.386 (0.390)\tDT 0.000 (0.028)\tloss 1.343 (1.361)\tAcc@1 64.844 (61.582)\n",
            "Train: [23][130/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 1.426 (1.365)\tAcc@1 59.766 (61.475)\n",
            "Train: [23][140/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 1.273 (1.365)\tAcc@1 62.891 (61.423)\n",
            "Train: [23][150/196]\tBT 0.388 (0.390)\tDT 0.000 (0.027)\tloss 1.516 (1.369)\tAcc@1 56.641 (61.357)\n",
            "Train: [23][160/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.495 (1.369)\tAcc@1 57.031 (61.370)\n",
            "Train: [23][170/196]\tBT 0.386 (0.389)\tDT 0.000 (0.027)\tloss 1.214 (1.365)\tAcc@1 66.406 (61.452)\n",
            "Train: [23][180/196]\tBT 0.388 (0.389)\tDT 0.000 (0.027)\tloss 1.323 (1.367)\tAcc@1 66.016 (61.413)\n",
            "Train: [23][190/196]\tBT 0.386 (0.389)\tDT 0.000 (0.027)\tloss 1.125 (1.363)\tAcc@1 66.797 (61.517)\n",
            "epoch 23, total time 76.29\n",
            "Test: [0/40]\tTime 0.427 (0.427)\tLoss 1.6256 (1.6256)\tAcc@1 58.594 (58.594)\n",
            "Test: [10/40]\tTime 0.122 (0.150)\tLoss 1.5979 (1.6758)\tAcc@1 56.641 (56.250)\n",
            "Test: [20/40]\tTime 0.122 (0.137)\tLoss 1.5600 (1.6991)\tAcc@1 57.422 (55.432)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 1.7834 (1.6857)\tAcc@1 49.609 (55.582)\n",
            " * Acc@1 55.560\n",
            "Train: [24][10/196]\tBT 0.386 (0.436)\tDT 0.000 (0.072)\tloss 1.113 (1.270)\tAcc@1 67.188 (63.477)\n",
            "Train: [24][20/196]\tBT 0.387 (0.411)\tDT 0.000 (0.048)\tloss 1.139 (1.254)\tAcc@1 66.406 (64.062)\n",
            "Train: [24][30/196]\tBT 0.387 (0.403)\tDT 0.000 (0.040)\tloss 1.296 (1.262)\tAcc@1 60.547 (63.854)\n",
            "Train: [24][40/196]\tBT 0.385 (0.399)\tDT 0.000 (0.036)\tloss 1.357 (1.277)\tAcc@1 62.500 (63.418)\n",
            "Train: [24][50/196]\tBT 0.387 (0.397)\tDT 0.000 (0.034)\tloss 1.264 (1.274)\tAcc@1 66.016 (63.594)\n",
            "Train: [24][60/196]\tBT 0.382 (0.395)\tDT 0.000 (0.033)\tloss 1.237 (1.279)\tAcc@1 64.453 (63.438)\n",
            "Train: [24][70/196]\tBT 0.388 (0.394)\tDT 0.000 (0.032)\tloss 1.283 (1.287)\tAcc@1 64.844 (63.225)\n",
            "Train: [24][80/196]\tBT 0.389 (0.393)\tDT 0.000 (0.031)\tloss 1.452 (1.294)\tAcc@1 57.812 (63.096)\n",
            "Train: [24][90/196]\tBT 0.386 (0.393)\tDT 0.000 (0.030)\tloss 1.230 (1.296)\tAcc@1 62.500 (63.056)\n",
            "Train: [24][100/196]\tBT 0.386 (0.392)\tDT 0.000 (0.030)\tloss 1.295 (1.294)\tAcc@1 64.062 (63.137)\n",
            "Train: [24][110/196]\tBT 0.387 (0.392)\tDT 0.000 (0.029)\tloss 1.334 (1.294)\tAcc@1 61.328 (63.161)\n",
            "Train: [24][120/196]\tBT 0.388 (0.391)\tDT 0.000 (0.029)\tloss 1.359 (1.296)\tAcc@1 64.453 (63.145)\n",
            "Train: [24][130/196]\tBT 0.386 (0.391)\tDT 0.000 (0.028)\tloss 1.264 (1.295)\tAcc@1 64.453 (63.161)\n",
            "Train: [24][140/196]\tBT 0.392 (0.391)\tDT 0.000 (0.028)\tloss 1.316 (1.296)\tAcc@1 61.328 (63.220)\n",
            "Train: [24][150/196]\tBT 0.388 (0.391)\tDT 0.000 (0.028)\tloss 1.397 (1.300)\tAcc@1 57.422 (63.169)\n",
            "Train: [24][160/196]\tBT 0.383 (0.390)\tDT 0.000 (0.028)\tloss 1.461 (1.305)\tAcc@1 56.250 (63.040)\n",
            "Train: [24][170/196]\tBT 0.386 (0.390)\tDT 0.000 (0.028)\tloss 1.522 (1.309)\tAcc@1 55.859 (62.946)\n",
            "Train: [24][180/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.261 (1.309)\tAcc@1 66.016 (62.919)\n",
            "Train: [24][190/196]\tBT 0.387 (0.390)\tDT 0.000 (0.027)\tloss 1.313 (1.309)\tAcc@1 62.500 (62.932)\n",
            "epoch 24, total time 76.41\n",
            "Test: [0/40]\tTime 0.429 (0.429)\tLoss 1.5429 (1.5429)\tAcc@1 58.984 (58.984)\n",
            "Test: [10/40]\tTime 0.122 (0.150)\tLoss 1.6418 (1.6206)\tAcc@1 57.031 (57.067)\n",
            "Test: [20/40]\tTime 0.121 (0.137)\tLoss 1.4579 (1.6295)\tAcc@1 60.547 (56.603)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 1.8673 (1.6345)\tAcc@1 54.297 (56.817)\n",
            " * Acc@1 56.810\n",
            "Train: [25][10/196]\tBT 0.384 (0.425)\tDT 0.000 (0.063)\tloss 1.168 (1.232)\tAcc@1 64.844 (64.727)\n",
            "Train: [25][20/196]\tBT 0.386 (0.406)\tDT 0.000 (0.044)\tloss 1.371 (1.242)\tAcc@1 60.938 (64.648)\n",
            "Train: [25][30/196]\tBT 0.386 (0.400)\tDT 0.000 (0.038)\tloss 1.302 (1.247)\tAcc@1 61.719 (64.388)\n",
            "Train: [25][40/196]\tBT 0.385 (0.397)\tDT 0.000 (0.034)\tloss 1.397 (1.242)\tAcc@1 62.500 (64.473)\n",
            "Train: [25][50/196]\tBT 0.386 (0.395)\tDT 0.001 (0.033)\tloss 1.390 (1.253)\tAcc@1 57.812 (64.203)\n",
            "Train: [25][60/196]\tBT 0.387 (0.393)\tDT 0.000 (0.031)\tloss 1.212 (1.251)\tAcc@1 61.328 (64.147)\n",
            "Train: [25][70/196]\tBT 0.386 (0.393)\tDT 0.000 (0.030)\tloss 1.052 (1.242)\tAcc@1 70.312 (64.425)\n",
            "Train: [25][80/196]\tBT 0.385 (0.392)\tDT 0.000 (0.030)\tloss 1.244 (1.249)\tAcc@1 67.578 (64.375)\n",
            "Train: [25][90/196]\tBT 0.386 (0.391)\tDT 0.000 (0.029)\tloss 1.307 (1.253)\tAcc@1 63.281 (64.240)\n",
            "Train: [25][100/196]\tBT 0.386 (0.391)\tDT 0.000 (0.029)\tloss 1.386 (1.254)\tAcc@1 62.500 (64.160)\n",
            "Train: [25][110/196]\tBT 0.381 (0.391)\tDT 0.000 (0.028)\tloss 1.414 (1.257)\tAcc@1 58.594 (64.094)\n",
            "Train: [25][120/196]\tBT 0.388 (0.390)\tDT 0.000 (0.028)\tloss 1.223 (1.264)\tAcc@1 66.406 (63.939)\n",
            "Train: [25][130/196]\tBT 0.387 (0.390)\tDT 0.000 (0.028)\tloss 1.306 (1.267)\tAcc@1 61.719 (63.885)\n",
            "Train: [25][140/196]\tBT 0.388 (0.390)\tDT 0.000 (0.028)\tloss 1.357 (1.271)\tAcc@1 61.719 (63.786)\n",
            "Train: [25][150/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 1.419 (1.272)\tAcc@1 63.281 (63.805)\n",
            "Train: [25][160/196]\tBT 0.386 (0.390)\tDT 0.000 (0.027)\tloss 1.314 (1.274)\tAcc@1 62.109 (63.755)\n",
            "Train: [25][170/196]\tBT 0.386 (0.389)\tDT 0.000 (0.027)\tloss 1.506 (1.279)\tAcc@1 54.688 (63.647)\n",
            "Train: [25][180/196]\tBT 0.385 (0.389)\tDT 0.000 (0.027)\tloss 1.362 (1.280)\tAcc@1 61.719 (63.592)\n",
            "Train: [25][190/196]\tBT 0.385 (0.389)\tDT 0.000 (0.027)\tloss 1.370 (1.282)\tAcc@1 63.672 (63.635)\n",
            "epoch 25, total time 76.27\n",
            "Test: [0/40]\tTime 0.387 (0.387)\tLoss 1.4949 (1.4949)\tAcc@1 61.719 (61.719)\n",
            "Test: [10/40]\tTime 0.122 (0.147)\tLoss 1.5705 (1.5977)\tAcc@1 57.031 (57.280)\n",
            "Test: [20/40]\tTime 0.122 (0.135)\tLoss 1.5546 (1.6160)\tAcc@1 57.031 (56.510)\n",
            "Test: [30/40]\tTime 0.122 (0.131)\tLoss 1.7646 (1.6085)\tAcc@1 53.125 (56.666)\n",
            " * Acc@1 57.000\n",
            "==> Saving...\n",
            "best accuracy: 58.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised Contrastive Learning on CIFAR-100"
      ],
      "metadata": {
        "id": "Xwfh3EXYLoHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tensorboard_logger as tb_logger\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "\n",
        "try:\n",
        "    import apex\n",
        "    from apex import amp, optimizers\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# Hardcoded configurations\n",
        "class Configuration:\n",
        "  print_freq = 10\n",
        "  save_freq = 50\n",
        "  batch_size = 256\n",
        "  num_workers = 16\n",
        "  epochs = 25\n",
        "  learning_rate = 0.05\n",
        "  lr_decay_epochs = [700,800,900]\n",
        "  lr_decay_rate = 0.1\n",
        "  weight_decay = 1e-4\n",
        "  momentum = 0.9\n",
        "  model_name = 'resnet50'\n",
        "  dataset = 'cifar100'  # options: ['cifar10', 'cifar100', 'path']\n",
        "  mean = None\n",
        "  std = None\n",
        "  data_folder = './datasets/'\n",
        "  size = 32\n",
        "  method = 'SupCon'  # options: ['SupCon', 'SimCLR']\n",
        "  temp = 0.07\n",
        "  cosine = False\n",
        "  syncBN = False\n",
        "  warm = False\n",
        "  trial = '0'\n",
        "  model_path = './save/SupCon/cifar100_models'\n",
        "  tb_path = './save/SupCon/cifar100_tensorboard'\n",
        "\n",
        "optimize = Configuration()"
      ],
      "metadata": {
        "id": "iH61ghhCaG9l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Derived configurations\n",
        "#if optimize.dataset == 'path':\n",
        "    #assert optimize.data_folder is not None and optimize.mean is not None and optimize.std is not None\n",
        "\n",
        "#if optimize.data_folder is None:\n",
        "    #data_folder = './datasets/'\n",
        "#optimize.model_path = './save/SupCon/{}_models'.format(optimize.dataset)\n",
        "#optimize.tb_path = './save/SupCon/{}_tensorboard'.format(optimize.dataset)\n",
        "\n",
        "#iterations = optimize.lr_decay_epochs.split(',')\n",
        "#lr_decay_epochs = list(map(int, iterations))\n",
        "\n",
        "model_name = '{}_{}_{}_lr_{}_decay_{}_bsz_{}_temp_{}_trial_{}'.format(\n",
        "    optimize.method, optimize.dataset, optimize.model_name, optimize.learning_rate,\n",
        "    optimize.weight_decay, optimize.batch_size, optimize.temp, optimize.trial)\n",
        "\n",
        "if optimize.cosine:\n",
        "    model_name += '_cosine'\n",
        "\n",
        "if optimize.batch_size > 256:\n",
        "    optimize.warm = True\n",
        "if optimize.warm:\n",
        "    model_name += '_warm'\n",
        "    warmup_from = 0.01\n",
        "    warm_epochs = 10\n",
        "    if optimize.cosine:\n",
        "        eta_min = optimize.learning_rate * (optimize.lr_decay_rate ** 3)\n",
        "        warmup_to = eta_min + (optimize.learning_rate - eta_min) * (\n",
        "                1 + math.cos(math.pi * warm_epochs / optimize.epochs)) / 2\n",
        "    else:\n",
        "        warmup_to = optimize.learning_rate\n",
        "\n",
        "tb_folder = os.path.join(optimize.tb_path, model_name)\n",
        "if not os.path.isdir(tb_folder):\n",
        "    os.makedirs(tb_folder)\n",
        "\n",
        "save_folder = os.path.join(optimize.model_path, model_name)\n",
        "if not os.path.isdir(save_folder):\n",
        "    os.makedirs(save_folder)\n",
        "\n",
        "def set_loader():\n",
        "    # construct data loader\n",
        "    if optimize.dataset == 'cifar10':\n",
        "        mean = (0.4914, 0.4822, 0.4465)\n",
        "        std = (0.2023, 0.1994, 0.2010)\n",
        "    elif optimize.dataset == 'cifar100':\n",
        "        mean = (0.5071, 0.4867, 0.4408)\n",
        "        std = (0.2675, 0.2565, 0.2761)\n",
        "    elif optimize.dataset == 'path':\n",
        "        mean = eval(mean)\n",
        "        std = eval(std)\n",
        "    else:\n",
        "        raise ValueError('dataset not supported: {}'.format(optimize.dataset))\n",
        "    normalize = transforms.Normalize(mean=mean, std=std)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=optimize.size, scale=(0.2, 1.)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
        "        ], p=0.8),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    if optimize.dataset == 'cifar10':\n",
        "        train_dataset = datasets.CIFAR10(root=optimize.data_folder,\n",
        "                                         transform=TwoCropTransform(train_transform),\n",
        "                                         download=True)\n",
        "    elif optimize.dataset == 'cifar100':\n",
        "        train_dataset = datasets.CIFAR100(root=optimize.data_folder,\n",
        "                                          transform=TwoCropTransform(train_transform),\n",
        "                                          download=True)\n",
        "    elif optimize.dataset == 'path':\n",
        "        train_dataset = datasets.ImageFolder(root=optimize.data_folder,\n",
        "                                             transform=TwoCropTransform(train_transform))\n",
        "    else:\n",
        "        raise ValueError(optimize.dataset)\n",
        "\n",
        "    train_sampler = None\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=optimize.batch_size, shuffle=(train_sampler is None),\n",
        "        num_workers=optimize.num_workers, pin_memory=True, sampler=train_sampler)\n",
        "\n",
        "    return train_loader\n",
        "\n",
        "def set_model():\n",
        "    model = SupConResNet(name='resnet50')\n",
        "    criterion = SupConLoss(temperature=optimize.temp)\n",
        "\n",
        "    # enable synchronized Batch Normalization\n",
        "    if optimize.syncBN:\n",
        "        model = apex.parallel.convert_syncbn_model(model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            model.encoder = torch.nn.DataParallel(model.encoder)\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    return model, criterion\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \"\"\"one epoch training\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    end = time.time()\n",
        "    for idx, (images, labels) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        images = torch.cat([images[0], images[1]], dim=0)\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda(non_blocking=True)\n",
        "            labels = labels.cuda(non_blocking=True)\n",
        "        bsz = labels.shape[0]\n",
        "\n",
        "        # warm-up learning rate\n",
        "        warmup_learning_rate(optimize, epoch, idx, len(train_loader), optimizer)\n",
        "\n",
        "        # compute loss\n",
        "        features = model(images)\n",
        "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
        "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
        "        if optimize.method == 'SupCon':\n",
        "            loss = criterion(features, labels)\n",
        "        elif optimize.method == 'SimCLR':\n",
        "            loss = criterion(features)\n",
        "        else:\n",
        "            raise ValueError('contrastive method not supported: {}'.\n",
        "                             format(optimize.method))\n",
        "\n",
        "        # update metric\n",
        "        losses.update(loss.item(), bsz)\n",
        "\n",
        "        # SGD\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # print info\n",
        "        if (idx + 1) % optimize.print_freq == 0:\n",
        "            print('Train: [{0}][{1}/{2}]\\t'\n",
        "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'loss {loss.val:.3f} ({loss.avg:.3f})'.format(\n",
        "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "def main():\n",
        "    # build data loader\n",
        "    train_loader = set_loader()\n",
        "\n",
        "    # build model and criterion\n",
        "    model, criterion = set_model()\n",
        "\n",
        "    # build optimizer\n",
        "    optimizer = set_optimizer(optimize, model)\n",
        "\n",
        "    # tensorboard\n",
        "    logger = tb_logger.Logger(logdir=tb_folder, flush_secs=2)\n",
        "\n",
        "    # training routine\n",
        "    for epoch in range(1, optimize.epochs + 1):\n",
        "        adjust_learning_rate(optimize, optimizer, epoch)\n",
        "\n",
        "        # train for one epoch\n",
        "        time1 = time.time()\n",
        "        loss = train(train_loader, model, criterion, optimizer, epoch)\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        # tensorboard logger\n",
        "        logger.log_value('loss', loss, epoch)\n",
        "        logger.log_value('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
        "\n",
        "        if epoch % optimize.save_freq == 0:\n",
        "            save_file = os.path.join(\n",
        "                save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            save_model(model, optimizer, optimize, epoch, save_file)\n",
        "\n",
        "    # save the last model\n",
        "    save_file = os.path.join(\n",
        "        save_folder, 'last.pth')\n",
        "    save_model(model, optimizer, optimize, optimize.epochs, save_file)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-psO2_kENOcL",
        "outputId": "31c2d252-3a2f-4c59-f4bd-157f81dfd5ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: [1][10/196]\tBT 0.801 (0.892)\tDT 0.000 (0.154)\tloss 6.233 (6.245)\n",
            "Train: [1][20/196]\tBT 0.801 (0.846)\tDT 0.000 (0.077)\tloss 6.235 (6.240)\n",
            "Train: [1][30/196]\tBT 0.800 (0.830)\tDT 0.000 (0.052)\tloss 6.230 (6.237)\n",
            "Train: [1][40/196]\tBT 0.803 (0.823)\tDT 0.000 (0.039)\tloss 6.228 (6.235)\n",
            "Train: [1][50/196]\tBT 0.787 (0.817)\tDT 0.000 (0.031)\tloss 6.217 (6.233)\n",
            "Train: [1][60/196]\tBT 0.789 (0.813)\tDT 0.000 (0.026)\tloss 6.220 (6.230)\n",
            "Train: [1][70/196]\tBT 0.788 (0.809)\tDT 0.000 (0.022)\tloss 6.189 (6.227)\n",
            "Train: [1][80/196]\tBT 0.791 (0.807)\tDT 0.000 (0.020)\tloss 6.207 (6.225)\n",
            "Train: [1][90/196]\tBT 0.790 (0.805)\tDT 0.000 (0.017)\tloss 6.177 (6.222)\n",
            "Train: [1][100/196]\tBT 0.791 (0.803)\tDT 0.000 (0.016)\tloss 6.213 (6.220)\n",
            "Train: [1][110/196]\tBT 0.790 (0.802)\tDT 0.000 (0.014)\tloss 6.218 (6.218)\n",
            "Train: [1][120/196]\tBT 0.789 (0.801)\tDT 0.000 (0.013)\tloss 6.209 (6.217)\n",
            "Train: [1][130/196]\tBT 0.789 (0.800)\tDT 0.000 (0.012)\tloss 6.199 (6.214)\n",
            "Train: [1][140/196]\tBT 0.797 (0.800)\tDT 0.000 (0.011)\tloss 6.129 (6.211)\n",
            "Train: [1][150/196]\tBT 0.795 (0.800)\tDT 0.000 (0.011)\tloss 6.196 (6.209)\n",
            "Train: [1][160/196]\tBT 0.798 (0.799)\tDT 0.000 (0.010)\tloss 6.156 (6.206)\n",
            "Train: [1][170/196]\tBT 0.787 (0.799)\tDT 0.000 (0.009)\tloss 6.161 (6.204)\n",
            "Train: [1][180/196]\tBT 0.793 (0.798)\tDT 0.000 (0.009)\tloss 6.157 (6.202)\n",
            "Train: [1][190/196]\tBT 0.791 (0.798)\tDT 0.000 (0.008)\tloss 6.180 (6.200)\n",
            "epoch 1, total time 156.28\n",
            "Train: [2][10/196]\tBT 0.791 (0.917)\tDT 0.000 (0.175)\tloss 6.191 (6.171)\n",
            "Train: [2][20/196]\tBT 0.793 (0.854)\tDT 0.000 (0.088)\tloss 6.165 (6.173)\n",
            "Train: [2][30/196]\tBT 0.788 (0.833)\tDT 0.000 (0.059)\tloss 6.156 (6.167)\n",
            "Train: [2][40/196]\tBT 0.796 (0.822)\tDT 0.000 (0.044)\tloss 6.184 (6.166)\n",
            "Train: [2][50/196]\tBT 0.796 (0.816)\tDT 0.000 (0.035)\tloss 6.090 (6.161)\n",
            "Train: [2][60/196]\tBT 0.796 (0.812)\tDT 0.000 (0.030)\tloss 6.160 (6.161)\n",
            "Train: [2][70/196]\tBT 0.795 (0.809)\tDT 0.000 (0.025)\tloss 6.191 (6.159)\n",
            "Train: [2][80/196]\tBT 0.791 (0.807)\tDT 0.000 (0.022)\tloss 6.173 (6.159)\n",
            "Train: [2][90/196]\tBT 0.793 (0.805)\tDT 0.000 (0.020)\tloss 6.167 (6.158)\n",
            "Train: [2][100/196]\tBT 0.794 (0.804)\tDT 0.000 (0.018)\tloss 6.168 (6.158)\n",
            "Train: [2][110/196]\tBT 0.795 (0.803)\tDT 0.000 (0.016)\tloss 6.143 (6.157)\n",
            "Train: [2][120/196]\tBT 0.789 (0.802)\tDT 0.000 (0.015)\tloss 6.109 (6.155)\n",
            "Train: [2][130/196]\tBT 0.794 (0.801)\tDT 0.000 (0.014)\tloss 6.130 (6.155)\n",
            "Train: [2][140/196]\tBT 0.797 (0.800)\tDT 0.000 (0.013)\tloss 6.137 (6.154)\n",
            "Train: [2][150/196]\tBT 0.797 (0.800)\tDT 0.000 (0.012)\tloss 6.124 (6.153)\n",
            "Train: [2][160/196]\tBT 0.791 (0.799)\tDT 0.000 (0.011)\tloss 6.172 (6.153)\n",
            "Train: [2][170/196]\tBT 0.787 (0.799)\tDT 0.000 (0.011)\tloss 6.139 (6.154)\n",
            "Train: [2][180/196]\tBT 0.794 (0.798)\tDT 0.000 (0.010)\tloss 6.180 (6.153)\n",
            "Train: [2][190/196]\tBT 0.793 (0.798)\tDT 0.000 (0.010)\tloss 6.135 (6.153)\n",
            "epoch 2, total time 156.24\n",
            "Train: [3][10/196]\tBT 0.788 (0.911)\tDT 0.000 (0.173)\tloss 6.156 (6.151)\n",
            "Train: [3][20/196]\tBT 0.792 (0.851)\tDT 0.000 (0.087)\tloss 6.164 (6.147)\n",
            "Train: [3][30/196]\tBT 0.790 (0.830)\tDT 0.000 (0.058)\tloss 6.091 (6.145)\n",
            "Train: [3][40/196]\tBT 0.790 (0.820)\tDT 0.000 (0.043)\tloss 6.144 (6.145)\n",
            "Train: [3][50/196]\tBT 0.788 (0.814)\tDT 0.000 (0.035)\tloss 6.147 (6.146)\n",
            "Train: [3][60/196]\tBT 0.787 (0.810)\tDT 0.000 (0.029)\tloss 6.107 (6.145)\n",
            "Train: [3][70/196]\tBT 0.788 (0.807)\tDT 0.000 (0.025)\tloss 6.136 (6.143)\n",
            "Train: [3][80/196]\tBT 0.793 (0.805)\tDT 0.000 (0.022)\tloss 6.164 (6.141)\n",
            "Train: [3][90/196]\tBT 0.790 (0.804)\tDT 0.000 (0.020)\tloss 6.129 (6.141)\n",
            "Train: [3][100/196]\tBT 0.791 (0.802)\tDT 0.000 (0.018)\tloss 6.208 (6.141)\n",
            "Train: [3][110/196]\tBT 0.787 (0.801)\tDT 0.000 (0.016)\tloss 6.175 (6.142)\n",
            "Train: [3][120/196]\tBT 0.796 (0.800)\tDT 0.000 (0.015)\tloss 6.118 (6.141)\n",
            "Train: [3][130/196]\tBT 0.793 (0.800)\tDT 0.000 (0.014)\tloss 6.130 (6.141)\n",
            "Train: [3][140/196]\tBT 0.790 (0.799)\tDT 0.000 (0.013)\tloss 6.102 (6.142)\n",
            "Train: [3][150/196]\tBT 0.788 (0.799)\tDT 0.000 (0.012)\tloss 6.093 (6.142)\n",
            "Train: [3][160/196]\tBT 0.790 (0.798)\tDT 0.000 (0.011)\tloss 6.150 (6.142)\n",
            "Train: [3][170/196]\tBT 0.793 (0.798)\tDT 0.000 (0.011)\tloss 6.121 (6.141)\n",
            "Train: [3][180/196]\tBT 0.794 (0.797)\tDT 0.000 (0.010)\tloss 6.108 (6.141)\n",
            "Train: [3][190/196]\tBT 0.793 (0.797)\tDT 0.000 (0.010)\tloss 6.156 (6.141)\n",
            "epoch 3, total time 156.12\n",
            "Train: [4][10/196]\tBT 0.787 (0.923)\tDT 0.000 (0.183)\tloss 6.165 (6.154)\n",
            "Train: [4][20/196]\tBT 0.793 (0.857)\tDT 0.000 (0.092)\tloss 6.130 (6.143)\n",
            "Train: [4][30/196]\tBT 0.790 (0.835)\tDT 0.000 (0.061)\tloss 6.081 (6.135)\n",
            "Train: [4][40/196]\tBT 0.789 (0.825)\tDT 0.000 (0.046)\tloss 6.122 (6.135)\n",
            "Train: [4][50/196]\tBT 0.793 (0.818)\tDT 0.000 (0.037)\tloss 6.155 (6.136)\n",
            "Train: [4][60/196]\tBT 0.795 (0.814)\tDT 0.000 (0.031)\tloss 6.175 (6.135)\n",
            "Train: [4][70/196]\tBT 0.795 (0.811)\tDT 0.000 (0.026)\tloss 6.137 (6.136)\n",
            "Train: [4][80/196]\tBT 0.796 (0.808)\tDT 0.000 (0.023)\tloss 6.104 (6.136)\n",
            "Train: [4][90/196]\tBT 0.795 (0.806)\tDT 0.000 (0.021)\tloss 6.123 (6.136)\n",
            "Train: [4][100/196]\tBT 0.792 (0.805)\tDT 0.000 (0.019)\tloss 6.133 (6.136)\n",
            "Train: [4][110/196]\tBT 0.787 (0.804)\tDT 0.000 (0.017)\tloss 6.116 (6.135)\n",
            "Train: [4][120/196]\tBT 0.789 (0.803)\tDT 0.000 (0.016)\tloss 6.118 (6.135)\n",
            "Train: [4][130/196]\tBT 0.792 (0.802)\tDT 0.000 (0.014)\tloss 6.103 (6.135)\n",
            "Train: [4][140/196]\tBT 0.793 (0.801)\tDT 0.000 (0.013)\tloss 6.109 (6.135)\n",
            "Train: [4][150/196]\tBT 0.791 (0.800)\tDT 0.000 (0.013)\tloss 6.103 (6.135)\n",
            "Train: [4][160/196]\tBT 0.790 (0.800)\tDT 0.000 (0.012)\tloss 6.119 (6.134)\n",
            "Train: [4][170/196]\tBT 0.794 (0.799)\tDT 0.000 (0.011)\tloss 6.126 (6.133)\n",
            "Train: [4][180/196]\tBT 0.795 (0.799)\tDT 0.000 (0.011)\tloss 6.165 (6.132)\n",
            "Train: [4][190/196]\tBT 0.793 (0.799)\tDT 0.000 (0.010)\tloss 6.126 (6.132)\n",
            "epoch 4, total time 156.38\n",
            "Train: [5][10/196]\tBT 0.792 (0.926)\tDT 0.000 (0.187)\tloss 6.147 (6.132)\n",
            "Train: [5][20/196]\tBT 0.791 (0.859)\tDT 0.000 (0.094)\tloss 6.146 (6.138)\n",
            "Train: [5][30/196]\tBT 0.786 (0.837)\tDT 0.000 (0.063)\tloss 6.071 (6.132)\n",
            "Train: [5][40/196]\tBT 0.788 (0.825)\tDT 0.000 (0.047)\tloss 6.158 (6.132)\n",
            "Train: [5][50/196]\tBT 0.789 (0.819)\tDT 0.000 (0.038)\tloss 6.112 (6.131)\n",
            "Train: [5][60/196]\tBT 0.788 (0.814)\tDT 0.000 (0.032)\tloss 6.145 (6.129)\n",
            "Train: [5][70/196]\tBT 0.790 (0.811)\tDT 0.000 (0.027)\tloss 6.160 (6.128)\n",
            "Train: [5][80/196]\tBT 0.791 (0.809)\tDT 0.000 (0.024)\tloss 6.102 (6.127)\n",
            "Train: [5][90/196]\tBT 0.793 (0.807)\tDT 0.000 (0.021)\tloss 6.148 (6.125)\n",
            "Train: [5][100/196]\tBT 0.787 (0.805)\tDT 0.000 (0.019)\tloss 6.096 (6.123)\n",
            "Train: [5][110/196]\tBT 0.790 (0.804)\tDT 0.000 (0.017)\tloss 6.090 (6.120)\n",
            "Train: [5][120/196]\tBT 0.790 (0.803)\tDT 0.000 (0.016)\tloss 6.080 (6.119)\n",
            "Train: [5][130/196]\tBT 0.792 (0.802)\tDT 0.000 (0.015)\tloss 6.101 (6.118)\n",
            "Train: [5][140/196]\tBT 0.795 (0.801)\tDT 0.000 (0.014)\tloss 6.092 (6.115)\n",
            "Train: [5][150/196]\tBT 0.795 (0.800)\tDT 0.000 (0.013)\tloss 6.092 (6.114)\n",
            "Train: [5][160/196]\tBT 0.787 (0.800)\tDT 0.000 (0.012)\tloss 6.136 (6.114)\n",
            "Train: [5][170/196]\tBT 0.788 (0.799)\tDT 0.000 (0.011)\tloss 6.106 (6.112)\n",
            "Train: [5][180/196]\tBT 0.791 (0.799)\tDT 0.001 (0.011)\tloss 6.105 (6.111)\n",
            "Train: [5][190/196]\tBT 0.787 (0.798)\tDT 0.000 (0.010)\tloss 6.139 (6.110)\n",
            "epoch 5, total time 156.36\n",
            "Train: [6][10/196]\tBT 0.788 (0.885)\tDT 0.000 (0.142)\tloss 6.082 (6.089)\n",
            "Train: [6][20/196]\tBT 0.792 (0.838)\tDT 0.000 (0.071)\tloss 6.099 (6.078)\n",
            "Train: [6][30/196]\tBT 0.792 (0.823)\tDT 0.000 (0.048)\tloss 6.098 (6.080)\n",
            "Train: [6][40/196]\tBT 0.796 (0.815)\tDT 0.000 (0.036)\tloss 6.072 (6.076)\n",
            "Train: [6][50/196]\tBT 0.797 (0.810)\tDT 0.000 (0.029)\tloss 6.028 (6.074)\n",
            "Train: [6][60/196]\tBT 0.793 (0.807)\tDT 0.000 (0.024)\tloss 6.045 (6.070)\n",
            "Train: [6][70/196]\tBT 0.795 (0.805)\tDT 0.000 (0.021)\tloss 6.036 (6.064)\n",
            "Train: [6][80/196]\tBT 0.791 (0.803)\tDT 0.000 (0.018)\tloss 6.056 (6.066)\n",
            "Train: [6][90/196]\tBT 0.784 (0.802)\tDT 0.000 (0.016)\tloss 6.057 (6.065)\n",
            "Train: [6][100/196]\tBT 0.794 (0.801)\tDT 0.000 (0.015)\tloss 6.100 (6.063)\n",
            "Train: [6][110/196]\tBT 0.791 (0.800)\tDT 0.000 (0.013)\tloss 6.022 (6.062)\n",
            "Train: [6][120/196]\tBT 0.795 (0.799)\tDT 0.000 (0.012)\tloss 6.036 (6.060)\n",
            "Train: [6][130/196]\tBT 0.781 (0.799)\tDT 0.000 (0.011)\tloss 6.043 (6.058)\n",
            "Train: [6][140/196]\tBT 0.796 (0.798)\tDT 0.000 (0.011)\tloss 5.960 (6.056)\n",
            "Train: [6][150/196]\tBT 0.788 (0.798)\tDT 0.000 (0.010)\tloss 6.106 (6.054)\n",
            "Train: [6][160/196]\tBT 0.792 (0.797)\tDT 0.000 (0.009)\tloss 6.064 (6.054)\n",
            "Train: [6][170/196]\tBT 0.789 (0.797)\tDT 0.000 (0.009)\tloss 6.108 (6.053)\n",
            "Train: [6][180/196]\tBT 0.795 (0.797)\tDT 0.000 (0.008)\tloss 5.995 (6.050)\n",
            "Train: [6][190/196]\tBT 0.793 (0.796)\tDT 0.000 (0.008)\tloss 6.022 (6.049)\n",
            "epoch 6, total time 155.99\n",
            "Train: [7][10/196]\tBT 0.789 (0.890)\tDT 0.000 (0.150)\tloss 5.960 (6.018)\n",
            "Train: [7][20/196]\tBT 0.790 (0.840)\tDT 0.000 (0.075)\tloss 5.974 (6.010)\n",
            "Train: [7][30/196]\tBT 0.788 (0.823)\tDT 0.000 (0.050)\tloss 6.003 (6.014)\n",
            "Train: [7][40/196]\tBT 0.788 (0.815)\tDT 0.000 (0.038)\tloss 6.001 (6.013)\n",
            "Train: [7][50/196]\tBT 0.790 (0.810)\tDT 0.000 (0.030)\tloss 5.987 (6.014)\n",
            "Train: [7][60/196]\tBT 0.791 (0.807)\tDT 0.000 (0.025)\tloss 5.995 (6.014)\n",
            "Train: [7][70/196]\tBT 0.789 (0.804)\tDT 0.000 (0.022)\tloss 5.967 (6.010)\n",
            "Train: [7][80/196]\tBT 0.791 (0.802)\tDT 0.000 (0.019)\tloss 6.008 (6.007)\n",
            "Train: [7][90/196]\tBT 0.787 (0.801)\tDT 0.000 (0.017)\tloss 6.060 (6.009)\n",
            "Train: [7][100/196]\tBT 0.789 (0.800)\tDT 0.000 (0.015)\tloss 6.011 (6.009)\n",
            "Train: [7][110/196]\tBT 0.790 (0.799)\tDT 0.000 (0.014)\tloss 5.978 (6.006)\n",
            "Train: [7][120/196]\tBT 0.788 (0.798)\tDT 0.000 (0.013)\tloss 5.999 (6.005)\n",
            "Train: [7][130/196]\tBT 0.790 (0.797)\tDT 0.000 (0.012)\tloss 5.942 (6.004)\n",
            "Train: [7][140/196]\tBT 0.788 (0.797)\tDT 0.000 (0.011)\tloss 5.986 (6.002)\n",
            "Train: [7][150/196]\tBT 0.789 (0.796)\tDT 0.000 (0.010)\tloss 5.972 (5.998)\n",
            "Train: [7][160/196]\tBT 0.789 (0.796)\tDT 0.000 (0.010)\tloss 5.957 (5.996)\n",
            "Train: [7][170/196]\tBT 0.790 (0.796)\tDT 0.000 (0.009)\tloss 5.939 (5.994)\n",
            "Train: [7][180/196]\tBT 0.790 (0.795)\tDT 0.000 (0.009)\tloss 6.058 (5.993)\n",
            "Train: [7][190/196]\tBT 0.790 (0.795)\tDT 0.000 (0.008)\tloss 6.020 (5.993)\n",
            "epoch 7, total time 155.70\n",
            "Train: [8][10/196]\tBT 0.792 (0.878)\tDT 0.000 (0.137)\tloss 6.010 (5.976)\n",
            "Train: [8][20/196]\tBT 0.791 (0.834)\tDT 0.000 (0.069)\tloss 5.947 (5.959)\n",
            "Train: [8][30/196]\tBT 0.790 (0.820)\tDT 0.000 (0.046)\tloss 5.905 (5.955)\n",
            "Train: [8][40/196]\tBT 0.797 (0.813)\tDT 0.000 (0.035)\tloss 5.937 (5.954)\n",
            "Train: [8][50/196]\tBT 0.792 (0.809)\tDT 0.000 (0.028)\tloss 6.071 (5.960)\n",
            "Train: [8][60/196]\tBT 0.795 (0.806)\tDT 0.000 (0.023)\tloss 6.007 (5.963)\n",
            "Train: [8][70/196]\tBT 0.792 (0.804)\tDT 0.000 (0.020)\tloss 6.009 (5.963)\n",
            "Train: [8][80/196]\tBT 0.782 (0.803)\tDT 0.000 (0.018)\tloss 5.995 (5.962)\n",
            "Train: [8][90/196]\tBT 0.788 (0.801)\tDT 0.000 (0.016)\tloss 6.016 (5.964)\n",
            "Train: [8][100/196]\tBT 0.790 (0.800)\tDT 0.000 (0.014)\tloss 5.982 (5.963)\n",
            "Train: [8][110/196]\tBT 0.790 (0.800)\tDT 0.000 (0.013)\tloss 5.898 (5.960)\n",
            "Train: [8][120/196]\tBT 0.789 (0.799)\tDT 0.000 (0.012)\tloss 5.999 (5.960)\n",
            "Train: [8][130/196]\tBT 0.789 (0.798)\tDT 0.000 (0.011)\tloss 5.953 (5.959)\n",
            "Train: [8][140/196]\tBT 0.790 (0.798)\tDT 0.000 (0.010)\tloss 5.898 (5.955)\n",
            "Train: [8][150/196]\tBT 0.789 (0.798)\tDT 0.000 (0.010)\tloss 5.932 (5.953)\n",
            "Train: [8][160/196]\tBT 0.790 (0.797)\tDT 0.000 (0.009)\tloss 5.991 (5.952)\n",
            "Train: [8][170/196]\tBT 0.792 (0.797)\tDT 0.000 (0.008)\tloss 5.908 (5.950)\n",
            "Train: [8][180/196]\tBT 0.791 (0.797)\tDT 0.000 (0.008)\tloss 6.025 (5.950)\n",
            "Train: [8][190/196]\tBT 0.796 (0.796)\tDT 0.000 (0.008)\tloss 5.830 (5.948)\n",
            "epoch 8, total time 155.94\n",
            "Train: [9][10/196]\tBT 0.792 (0.888)\tDT 0.000 (0.149)\tloss 5.956 (5.954)\n",
            "Train: [9][20/196]\tBT 0.790 (0.839)\tDT 0.000 (0.075)\tloss 5.927 (5.933)\n",
            "Train: [9][30/196]\tBT 0.791 (0.823)\tDT 0.000 (0.050)\tloss 5.904 (5.914)\n",
            "Train: [9][40/196]\tBT 0.800 (0.815)\tDT 0.000 (0.038)\tloss 5.882 (5.918)\n",
            "Train: [9][50/196]\tBT 0.792 (0.811)\tDT 0.000 (0.030)\tloss 5.818 (5.915)\n",
            "Train: [9][60/196]\tBT 0.794 (0.808)\tDT 0.000 (0.025)\tloss 5.851 (5.914)\n",
            "Train: [9][70/196]\tBT 0.788 (0.806)\tDT 0.000 (0.022)\tloss 5.906 (5.913)\n",
            "Train: [9][80/196]\tBT 0.793 (0.804)\tDT 0.000 (0.019)\tloss 5.977 (5.908)\n",
            "Train: [9][90/196]\tBT 0.795 (0.803)\tDT 0.000 (0.017)\tloss 5.895 (5.906)\n",
            "Train: [9][100/196]\tBT 0.788 (0.801)\tDT 0.000 (0.015)\tloss 5.865 (5.901)\n",
            "Train: [9][110/196]\tBT 0.792 (0.801)\tDT 0.000 (0.014)\tloss 5.830 (5.897)\n",
            "Train: [9][120/196]\tBT 0.792 (0.800)\tDT 0.000 (0.013)\tloss 5.812 (5.894)\n",
            "Train: [9][130/196]\tBT 0.796 (0.799)\tDT 0.000 (0.012)\tloss 5.821 (5.890)\n",
            "Train: [9][140/196]\tBT 0.791 (0.799)\tDT 0.000 (0.011)\tloss 5.810 (5.886)\n",
            "Train: [9][150/196]\tBT 0.790 (0.798)\tDT 0.000 (0.010)\tloss 5.794 (5.882)\n",
            "Train: [9][160/196]\tBT 0.791 (0.798)\tDT 0.000 (0.010)\tloss 5.761 (5.877)\n",
            "Train: [9][170/196]\tBT 0.789 (0.797)\tDT 0.000 (0.009)\tloss 5.837 (5.874)\n",
            "Train: [9][180/196]\tBT 0.791 (0.797)\tDT 0.000 (0.009)\tloss 5.819 (5.871)\n",
            "Train: [9][190/196]\tBT 0.787 (0.797)\tDT 0.000 (0.008)\tloss 5.832 (5.867)\n",
            "epoch 9, total time 156.08\n",
            "Train: [10][10/196]\tBT 0.791 (0.894)\tDT 0.000 (0.152)\tloss 5.813 (5.799)\n",
            "Train: [10][20/196]\tBT 0.792 (0.842)\tDT 0.000 (0.076)\tloss 5.705 (5.791)\n",
            "Train: [10][30/196]\tBT 0.796 (0.824)\tDT 0.000 (0.051)\tloss 5.786 (5.782)\n",
            "Train: [10][40/196]\tBT 0.794 (0.816)\tDT 0.000 (0.038)\tloss 5.862 (5.790)\n",
            "Train: [10][50/196]\tBT 0.792 (0.811)\tDT 0.000 (0.031)\tloss 5.730 (5.792)\n",
            "Train: [10][60/196]\tBT 0.791 (0.808)\tDT 0.000 (0.026)\tloss 5.839 (5.792)\n",
            "Train: [10][70/196]\tBT 0.792 (0.806)\tDT 0.000 (0.022)\tloss 5.657 (5.784)\n",
            "Train: [10][80/196]\tBT 0.789 (0.804)\tDT 0.000 (0.019)\tloss 5.822 (5.786)\n",
            "Train: [10][90/196]\tBT 0.792 (0.803)\tDT 0.000 (0.017)\tloss 5.713 (5.781)\n",
            "Train: [10][100/196]\tBT 0.794 (0.802)\tDT 0.000 (0.016)\tloss 5.612 (5.775)\n",
            "Train: [10][110/196]\tBT 0.793 (0.801)\tDT 0.000 (0.014)\tloss 5.744 (5.775)\n",
            "Train: [10][120/196]\tBT 0.794 (0.800)\tDT 0.000 (0.013)\tloss 5.649 (5.769)\n",
            "Train: [10][130/196]\tBT 0.793 (0.799)\tDT 0.000 (0.012)\tloss 5.770 (5.769)\n",
            "Train: [10][140/196]\tBT 0.790 (0.799)\tDT 0.000 (0.011)\tloss 5.689 (5.766)\n",
            "Train: [10][150/196]\tBT 0.791 (0.798)\tDT 0.000 (0.011)\tloss 5.659 (5.762)\n",
            "Train: [10][160/196]\tBT 0.791 (0.798)\tDT 0.000 (0.010)\tloss 5.818 (5.760)\n",
            "Train: [10][170/196]\tBT 0.796 (0.798)\tDT 0.000 (0.009)\tloss 5.685 (5.758)\n",
            "Train: [10][180/196]\tBT 0.792 (0.797)\tDT 0.000 (0.009)\tloss 5.799 (5.758)\n",
            "Train: [10][190/196]\tBT 0.794 (0.797)\tDT 0.000 (0.008)\tloss 5.697 (5.754)\n",
            "epoch 10, total time 156.10\n",
            "Train: [11][10/196]\tBT 0.790 (0.910)\tDT 0.000 (0.167)\tloss 5.720 (5.738)\n",
            "Train: [11][20/196]\tBT 0.793 (0.851)\tDT 0.000 (0.084)\tloss 5.692 (5.712)\n",
            "Train: [11][30/196]\tBT 0.796 (0.832)\tDT 0.000 (0.056)\tloss 5.759 (5.710)\n",
            "Train: [11][40/196]\tBT 0.794 (0.822)\tDT 0.000 (0.042)\tloss 5.723 (5.713)\n",
            "Train: [11][50/196]\tBT 0.784 (0.816)\tDT 0.000 (0.034)\tloss 5.678 (5.708)\n",
            "Train: [11][60/196]\tBT 0.797 (0.812)\tDT 0.000 (0.028)\tloss 5.700 (5.706)\n",
            "Train: [11][70/196]\tBT 0.796 (0.809)\tDT 0.000 (0.024)\tloss 5.620 (5.704)\n",
            "Train: [11][80/196]\tBT 0.793 (0.807)\tDT 0.000 (0.021)\tloss 5.708 (5.699)\n",
            "Train: [11][90/196]\tBT 0.791 (0.805)\tDT 0.000 (0.019)\tloss 5.642 (5.695)\n",
            "Train: [11][100/196]\tBT 0.784 (0.804)\tDT 0.000 (0.017)\tloss 5.688 (5.692)\n",
            "Train: [11][110/196]\tBT 0.789 (0.803)\tDT 0.000 (0.016)\tloss 5.531 (5.690)\n",
            "Train: [11][120/196]\tBT 0.788 (0.802)\tDT 0.000 (0.014)\tloss 5.692 (5.690)\n",
            "Train: [11][130/196]\tBT 0.788 (0.801)\tDT 0.000 (0.013)\tloss 5.745 (5.694)\n",
            "Train: [11][140/196]\tBT 0.789 (0.800)\tDT 0.000 (0.012)\tloss 5.696 (5.691)\n",
            "Train: [11][150/196]\tBT 0.790 (0.800)\tDT 0.000 (0.012)\tloss 5.718 (5.690)\n",
            "Train: [11][160/196]\tBT 0.788 (0.799)\tDT 0.000 (0.011)\tloss 5.681 (5.689)\n",
            "Train: [11][170/196]\tBT 0.791 (0.799)\tDT 0.000 (0.010)\tloss 5.740 (5.688)\n",
            "Train: [11][180/196]\tBT 0.796 (0.799)\tDT 0.000 (0.010)\tloss 5.647 (5.685)\n",
            "Train: [11][190/196]\tBT 0.787 (0.798)\tDT 0.000 (0.009)\tloss 5.754 (5.683)\n",
            "epoch 11, total time 156.31\n",
            "Train: [12][10/196]\tBT 0.799 (0.864)\tDT 0.000 (0.122)\tloss 5.540 (5.661)\n",
            "Train: [12][20/196]\tBT 0.788 (0.826)\tDT 0.000 (0.061)\tloss 5.651 (5.634)\n",
            "Train: [12][30/196]\tBT 0.790 (0.814)\tDT 0.000 (0.041)\tloss 5.698 (5.640)\n",
            "Train: [12][40/196]\tBT 0.791 (0.808)\tDT 0.000 (0.031)\tloss 5.702 (5.630)\n",
            "Train: [12][50/196]\tBT 0.791 (0.804)\tDT 0.000 (0.025)\tloss 5.600 (5.626)\n",
            "Train: [12][60/196]\tBT 0.788 (0.802)\tDT 0.000 (0.021)\tloss 5.586 (5.619)\n",
            "Train: [12][70/196]\tBT 0.791 (0.800)\tDT 0.000 (0.018)\tloss 5.628 (5.620)\n",
            "Train: [12][80/196]\tBT 0.786 (0.799)\tDT 0.000 (0.016)\tloss 5.662 (5.620)\n",
            "Train: [12][90/196]\tBT 0.792 (0.798)\tDT 0.000 (0.014)\tloss 5.602 (5.617)\n",
            "Train: [12][100/196]\tBT 0.793 (0.798)\tDT 0.000 (0.013)\tloss 5.570 (5.612)\n",
            "Train: [12][110/196]\tBT 0.793 (0.797)\tDT 0.000 (0.012)\tloss 5.726 (5.615)\n",
            "Train: [12][120/196]\tBT 0.789 (0.797)\tDT 0.000 (0.011)\tloss 5.507 (5.612)\n",
            "Train: [12][130/196]\tBT 0.790 (0.796)\tDT 0.000 (0.010)\tloss 5.762 (5.610)\n",
            "Train: [12][140/196]\tBT 0.792 (0.796)\tDT 0.000 (0.009)\tloss 5.472 (5.607)\n",
            "Train: [12][150/196]\tBT 0.793 (0.796)\tDT 0.000 (0.009)\tloss 5.672 (5.605)\n",
            "Train: [12][160/196]\tBT 0.801 (0.795)\tDT 0.000 (0.008)\tloss 5.560 (5.602)\n",
            "Train: [12][170/196]\tBT 0.799 (0.795)\tDT 0.000 (0.008)\tloss 5.419 (5.600)\n",
            "Train: [12][180/196]\tBT 0.792 (0.795)\tDT 0.000 (0.007)\tloss 5.638 (5.597)\n",
            "Train: [12][190/196]\tBT 0.785 (0.795)\tDT 0.000 (0.007)\tloss 5.474 (5.593)\n",
            "epoch 12, total time 155.67\n",
            "Train: [13][10/196]\tBT 0.798 (0.907)\tDT 0.000 (0.167)\tloss 5.457 (5.549)\n",
            "Train: [13][20/196]\tBT 0.793 (0.848)\tDT 0.000 (0.084)\tloss 5.529 (5.556)\n",
            "Train: [13][30/196]\tBT 0.791 (0.829)\tDT 0.000 (0.056)\tloss 5.607 (5.538)\n",
            "Train: [13][40/196]\tBT 0.791 (0.819)\tDT 0.000 (0.042)\tloss 5.529 (5.533)\n",
            "Train: [13][50/196]\tBT 0.793 (0.813)\tDT 0.000 (0.034)\tloss 5.478 (5.531)\n",
            "Train: [13][60/196]\tBT 0.793 (0.810)\tDT 0.000 (0.028)\tloss 5.514 (5.534)\n",
            "Train: [13][70/196]\tBT 0.787 (0.807)\tDT 0.000 (0.024)\tloss 5.586 (5.535)\n",
            "Train: [13][80/196]\tBT 0.792 (0.805)\tDT 0.000 (0.021)\tloss 5.572 (5.535)\n",
            "Train: [13][90/196]\tBT 0.788 (0.804)\tDT 0.000 (0.019)\tloss 5.517 (5.530)\n",
            "Train: [13][100/196]\tBT 0.789 (0.803)\tDT 0.000 (0.017)\tloss 5.442 (5.529)\n",
            "Train: [13][110/196]\tBT 0.793 (0.802)\tDT 0.000 (0.016)\tloss 5.450 (5.526)\n",
            "Train: [13][120/196]\tBT 0.794 (0.801)\tDT 0.000 (0.014)\tloss 5.484 (5.522)\n",
            "Train: [13][130/196]\tBT 0.795 (0.800)\tDT 0.000 (0.013)\tloss 5.438 (5.519)\n",
            "Train: [13][140/196]\tBT 0.788 (0.800)\tDT 0.000 (0.012)\tloss 5.427 (5.518)\n",
            "Train: [13][150/196]\tBT 0.794 (0.799)\tDT 0.000 (0.012)\tloss 5.457 (5.513)\n",
            "Train: [13][160/196]\tBT 0.792 (0.799)\tDT 0.000 (0.011)\tloss 5.540 (5.511)\n",
            "Train: [13][170/196]\tBT 0.792 (0.798)\tDT 0.000 (0.010)\tloss 5.457 (5.510)\n",
            "Train: [13][180/196]\tBT 0.789 (0.798)\tDT 0.000 (0.010)\tloss 5.523 (5.509)\n",
            "Train: [13][190/196]\tBT 0.789 (0.798)\tDT 0.000 (0.009)\tloss 5.429 (5.504)\n",
            "epoch 13, total time 156.21\n",
            "Train: [14][10/196]\tBT 0.789 (0.865)\tDT 0.000 (0.139)\tloss 5.437 (5.518)\n",
            "Train: [14][20/196]\tBT 0.789 (0.827)\tDT 0.000 (0.070)\tloss 5.575 (5.483)\n",
            "Train: [14][30/196]\tBT 0.790 (0.815)\tDT 0.000 (0.047)\tloss 5.505 (5.487)\n",
            "Train: [14][40/196]\tBT 0.791 (0.809)\tDT 0.000 (0.035)\tloss 5.301 (5.488)\n",
            "Train: [14][50/196]\tBT 0.789 (0.805)\tDT 0.000 (0.028)\tloss 5.507 (5.478)\n",
            "Train: [14][60/196]\tBT 0.795 (0.803)\tDT 0.000 (0.024)\tloss 5.432 (5.477)\n",
            "Train: [14][70/196]\tBT 0.788 (0.801)\tDT 0.000 (0.020)\tloss 5.387 (5.473)\n",
            "Train: [14][80/196]\tBT 0.788 (0.799)\tDT 0.000 (0.018)\tloss 5.484 (5.471)\n",
            "Train: [14][90/196]\tBT 0.798 (0.798)\tDT 0.000 (0.016)\tloss 5.270 (5.466)\n",
            "Train: [14][100/196]\tBT 0.791 (0.798)\tDT 0.000 (0.014)\tloss 5.291 (5.461)\n",
            "Train: [14][110/196]\tBT 0.786 (0.797)\tDT 0.000 (0.013)\tloss 5.435 (5.457)\n",
            "Train: [14][120/196]\tBT 0.793 (0.797)\tDT 0.000 (0.012)\tloss 5.385 (5.451)\n",
            "Train: [14][130/196]\tBT 0.798 (0.796)\tDT 0.000 (0.011)\tloss 5.411 (5.449)\n",
            "Train: [14][140/196]\tBT 0.794 (0.796)\tDT 0.000 (0.010)\tloss 5.419 (5.447)\n",
            "Train: [14][150/196]\tBT 0.795 (0.796)\tDT 0.000 (0.010)\tloss 5.408 (5.444)\n",
            "Train: [14][160/196]\tBT 0.792 (0.795)\tDT 0.000 (0.009)\tloss 5.437 (5.444)\n",
            "Train: [14][170/196]\tBT 0.792 (0.795)\tDT 0.000 (0.009)\tloss 5.239 (5.441)\n",
            "Train: [14][180/196]\tBT 0.789 (0.795)\tDT 0.000 (0.008)\tloss 5.227 (5.438)\n",
            "Train: [14][190/196]\tBT 0.796 (0.795)\tDT 0.000 (0.008)\tloss 5.287 (5.431)\n",
            "epoch 14, total time 155.69\n",
            "Train: [15][10/196]\tBT 0.784 (0.866)\tDT 0.000 (0.124)\tloss 5.381 (5.465)\n",
            "Train: [15][20/196]\tBT 0.786 (0.829)\tDT 0.000 (0.062)\tloss 5.413 (5.412)\n",
            "Train: [15][30/196]\tBT 0.788 (0.817)\tDT 0.000 (0.042)\tloss 5.449 (5.417)\n",
            "Train: [15][40/196]\tBT 0.795 (0.811)\tDT 0.000 (0.031)\tloss 5.474 (5.398)\n",
            "Train: [15][50/196]\tBT 0.793 (0.807)\tDT 0.000 (0.025)\tloss 5.405 (5.387)\n",
            "Train: [15][60/196]\tBT 0.796 (0.804)\tDT 0.000 (0.021)\tloss 5.329 (5.370)\n",
            "Train: [15][70/196]\tBT 0.792 (0.803)\tDT 0.000 (0.018)\tloss 5.303 (5.368)\n",
            "Train: [15][80/196]\tBT 0.793 (0.801)\tDT 0.000 (0.016)\tloss 5.334 (5.366)\n",
            "Train: [15][90/196]\tBT 0.792 (0.800)\tDT 0.000 (0.014)\tloss 5.295 (5.359)\n",
            "Train: [15][100/196]\tBT 0.793 (0.799)\tDT 0.000 (0.013)\tloss 5.384 (5.357)\n",
            "Train: [15][110/196]\tBT 0.794 (0.799)\tDT 0.000 (0.012)\tloss 5.367 (5.351)\n",
            "Train: [15][120/196]\tBT 0.787 (0.798)\tDT 0.000 (0.011)\tloss 5.202 (5.345)\n",
            "Train: [15][130/196]\tBT 0.788 (0.797)\tDT 0.000 (0.010)\tloss 5.564 (5.343)\n",
            "Train: [15][140/196]\tBT 0.789 (0.797)\tDT 0.000 (0.009)\tloss 5.246 (5.338)\n",
            "Train: [15][150/196]\tBT 0.786 (0.797)\tDT 0.000 (0.009)\tloss 5.239 (5.334)\n",
            "Train: [15][160/196]\tBT 0.791 (0.796)\tDT 0.000 (0.008)\tloss 5.192 (5.333)\n",
            "Train: [15][170/196]\tBT 0.788 (0.796)\tDT 0.000 (0.008)\tloss 5.224 (5.331)\n",
            "Train: [15][180/196]\tBT 0.789 (0.796)\tDT 0.000 (0.007)\tloss 5.282 (5.332)\n",
            "Train: [15][190/196]\tBT 0.789 (0.796)\tDT 0.000 (0.007)\tloss 5.220 (5.325)\n",
            "epoch 15, total time 155.84\n",
            "Train: [16][10/196]\tBT 0.786 (0.877)\tDT 0.000 (0.135)\tloss 5.274 (5.279)\n",
            "Train: [16][20/196]\tBT 0.791 (0.834)\tDT 0.000 (0.068)\tloss 5.343 (5.283)\n",
            "Train: [16][30/196]\tBT 0.786 (0.820)\tDT 0.000 (0.045)\tloss 5.242 (5.275)\n",
            "Train: [16][40/196]\tBT 0.790 (0.813)\tDT 0.000 (0.034)\tloss 5.275 (5.270)\n",
            "Train: [16][50/196]\tBT 0.794 (0.809)\tDT 0.000 (0.027)\tloss 5.248 (5.272)\n",
            "Train: [16][60/196]\tBT 0.797 (0.806)\tDT 0.000 (0.023)\tloss 5.133 (5.269)\n",
            "Train: [16][70/196]\tBT 0.794 (0.804)\tDT 0.000 (0.020)\tloss 5.222 (5.264)\n",
            "Train: [16][80/196]\tBT 0.789 (0.803)\tDT 0.000 (0.017)\tloss 5.282 (5.256)\n",
            "Train: [16][90/196]\tBT 0.792 (0.801)\tDT 0.000 (0.015)\tloss 5.306 (5.256)\n",
            "Train: [16][100/196]\tBT 0.794 (0.800)\tDT 0.000 (0.014)\tloss 5.198 (5.253)\n",
            "Train: [16][110/196]\tBT 0.790 (0.800)\tDT 0.000 (0.013)\tloss 5.150 (5.256)\n",
            "Train: [16][120/196]\tBT 0.798 (0.799)\tDT 0.000 (0.012)\tloss 5.236 (5.253)\n",
            "Train: [16][130/196]\tBT 0.787 (0.798)\tDT 0.000 (0.011)\tloss 5.428 (5.251)\n",
            "Train: [16][140/196]\tBT 0.793 (0.798)\tDT 0.000 (0.010)\tloss 5.192 (5.248)\n",
            "Train: [16][150/196]\tBT 0.793 (0.797)\tDT 0.000 (0.009)\tloss 5.324 (5.248)\n",
            "Train: [16][160/196]\tBT 0.792 (0.797)\tDT 0.000 (0.009)\tloss 5.373 (5.248)\n",
            "Train: [16][170/196]\tBT 0.789 (0.797)\tDT 0.000 (0.008)\tloss 5.393 (5.249)\n",
            "Train: [16][180/196]\tBT 0.790 (0.796)\tDT 0.000 (0.008)\tloss 5.182 (5.248)\n",
            "Train: [16][190/196]\tBT 0.789 (0.796)\tDT 0.000 (0.008)\tloss 5.202 (5.249)\n",
            "epoch 16, total time 155.93\n",
            "Train: [17][10/196]\tBT 0.788 (0.869)\tDT 0.000 (0.128)\tloss 5.232 (5.181)\n",
            "Train: [17][20/196]\tBT 0.788 (0.830)\tDT 0.000 (0.064)\tloss 5.125 (5.195)\n",
            "Train: [17][30/196]\tBT 0.788 (0.817)\tDT 0.000 (0.043)\tloss 5.169 (5.183)\n",
            "Train: [17][40/196]\tBT 0.797 (0.811)\tDT 0.000 (0.032)\tloss 5.091 (5.190)\n",
            "Train: [17][50/196]\tBT 0.786 (0.807)\tDT 0.000 (0.026)\tloss 5.067 (5.183)\n",
            "Train: [17][60/196]\tBT 0.789 (0.805)\tDT 0.000 (0.022)\tloss 5.296 (5.183)\n",
            "Train: [17][70/196]\tBT 0.793 (0.803)\tDT 0.000 (0.019)\tloss 5.061 (5.181)\n",
            "Train: [17][80/196]\tBT 0.792 (0.802)\tDT 0.000 (0.016)\tloss 5.217 (5.181)\n",
            "Train: [17][90/196]\tBT 0.788 (0.801)\tDT 0.000 (0.015)\tloss 5.209 (5.175)\n",
            "Train: [17][100/196]\tBT 0.789 (0.800)\tDT 0.000 (0.013)\tloss 5.188 (5.173)\n",
            "Train: [17][110/196]\tBT 0.796 (0.799)\tDT 0.000 (0.012)\tloss 5.048 (5.173)\n",
            "Train: [17][120/196]\tBT 0.796 (0.798)\tDT 0.000 (0.011)\tloss 5.041 (5.167)\n",
            "Train: [17][130/196]\tBT 0.793 (0.798)\tDT 0.000 (0.010)\tloss 5.293 (5.168)\n",
            "Train: [17][140/196]\tBT 0.788 (0.798)\tDT 0.000 (0.010)\tloss 5.195 (5.168)\n",
            "Train: [17][150/196]\tBT 0.795 (0.797)\tDT 0.000 (0.009)\tloss 5.199 (5.165)\n",
            "Train: [17][160/196]\tBT 0.793 (0.797)\tDT 0.000 (0.008)\tloss 5.004 (5.165)\n",
            "Train: [17][170/196]\tBT 0.794 (0.796)\tDT 0.000 (0.008)\tloss 5.215 (5.164)\n",
            "Train: [17][180/196]\tBT 0.795 (0.796)\tDT 0.000 (0.008)\tloss 5.153 (5.162)\n",
            "Train: [17][190/196]\tBT 0.791 (0.796)\tDT 0.000 (0.007)\tloss 5.133 (5.161)\n",
            "epoch 17, total time 155.89\n",
            "Train: [18][10/196]\tBT 0.787 (0.894)\tDT 0.000 (0.151)\tloss 5.139 (5.118)\n",
            "Train: [18][20/196]\tBT 0.796 (0.844)\tDT 0.000 (0.076)\tloss 5.224 (5.138)\n",
            "Train: [18][30/196]\tBT 0.794 (0.826)\tDT 0.000 (0.051)\tloss 5.027 (5.134)\n",
            "Train: [18][40/196]\tBT 0.787 (0.818)\tDT 0.000 (0.038)\tloss 5.003 (5.121)\n",
            "Train: [18][50/196]\tBT 0.790 (0.813)\tDT 0.000 (0.031)\tloss 5.048 (5.112)\n",
            "Train: [18][60/196]\tBT 0.795 (0.809)\tDT 0.000 (0.026)\tloss 5.104 (5.106)\n",
            "Train: [18][70/196]\tBT 0.790 (0.807)\tDT 0.000 (0.022)\tloss 5.051 (5.101)\n",
            "Train: [18][80/196]\tBT 0.789 (0.805)\tDT 0.000 (0.019)\tloss 5.113 (5.097)\n",
            "Train: [18][90/196]\tBT 0.789 (0.804)\tDT 0.000 (0.017)\tloss 5.055 (5.089)\n",
            "Train: [18][100/196]\tBT 0.789 (0.802)\tDT 0.000 (0.016)\tloss 5.192 (5.090)\n",
            "Train: [18][110/196]\tBT 0.799 (0.802)\tDT 0.000 (0.014)\tloss 5.094 (5.088)\n",
            "Train: [18][120/196]\tBT 0.794 (0.801)\tDT 0.000 (0.013)\tloss 5.058 (5.087)\n",
            "Train: [18][130/196]\tBT 0.789 (0.800)\tDT 0.000 (0.012)\tloss 5.276 (5.089)\n",
            "Train: [18][140/196]\tBT 0.794 (0.799)\tDT 0.000 (0.011)\tloss 4.931 (5.091)\n",
            "Train: [18][150/196]\tBT 0.787 (0.799)\tDT 0.000 (0.011)\tloss 4.946 (5.092)\n",
            "Train: [18][160/196]\tBT 0.787 (0.798)\tDT 0.000 (0.010)\tloss 5.115 (5.089)\n",
            "Train: [18][170/196]\tBT 0.795 (0.798)\tDT 0.000 (0.009)\tloss 5.110 (5.091)\n",
            "Train: [18][180/196]\tBT 0.796 (0.798)\tDT 0.000 (0.009)\tloss 5.094 (5.091)\n",
            "Train: [18][190/196]\tBT 0.790 (0.797)\tDT 0.000 (0.008)\tloss 4.895 (5.086)\n",
            "epoch 18, total time 156.15\n",
            "Train: [19][10/196]\tBT 0.797 (0.922)\tDT 0.000 (0.179)\tloss 5.011 (5.026)\n",
            "Train: [19][20/196]\tBT 0.797 (0.857)\tDT 0.000 (0.090)\tloss 5.009 (5.001)\n",
            "Train: [19][30/196]\tBT 0.797 (0.835)\tDT 0.000 (0.060)\tloss 5.071 (5.010)\n",
            "Train: [19][40/196]\tBT 0.792 (0.824)\tDT 0.000 (0.045)\tloss 5.109 (5.016)\n",
            "Train: [19][50/196]\tBT 0.789 (0.818)\tDT 0.000 (0.036)\tloss 5.069 (5.019)\n",
            "Train: [19][60/196]\tBT 0.794 (0.814)\tDT 0.000 (0.030)\tloss 5.209 (5.034)\n",
            "Train: [19][70/196]\tBT 0.787 (0.810)\tDT 0.000 (0.026)\tloss 4.897 (5.033)\n",
            "Train: [19][80/196]\tBT 0.789 (0.808)\tDT 0.000 (0.023)\tloss 4.838 (5.029)\n",
            "Train: [19][90/196]\tBT 0.789 (0.806)\tDT 0.000 (0.020)\tloss 5.057 (5.023)\n",
            "Train: [19][100/196]\tBT 0.789 (0.805)\tDT 0.000 (0.018)\tloss 5.048 (5.020)\n",
            "Train: [19][110/196]\tBT 0.795 (0.804)\tDT 0.000 (0.017)\tloss 5.111 (5.018)\n",
            "Train: [19][120/196]\tBT 0.797 (0.803)\tDT 0.000 (0.015)\tloss 4.904 (5.016)\n",
            "Train: [19][130/196]\tBT 0.792 (0.802)\tDT 0.000 (0.014)\tloss 5.063 (5.015)\n",
            "Train: [19][140/196]\tBT 0.794 (0.801)\tDT 0.000 (0.013)\tloss 4.903 (5.013)\n",
            "Train: [19][150/196]\tBT 0.792 (0.800)\tDT 0.000 (0.012)\tloss 4.975 (5.011)\n",
            "Train: [19][160/196]\tBT 0.789 (0.800)\tDT 0.000 (0.012)\tloss 4.957 (5.017)\n",
            "Train: [19][170/196]\tBT 0.790 (0.799)\tDT 0.000 (0.011)\tloss 5.045 (5.015)\n",
            "Train: [19][180/196]\tBT 0.793 (0.799)\tDT 0.000 (0.010)\tloss 4.994 (5.016)\n",
            "Train: [19][190/196]\tBT 0.792 (0.798)\tDT 0.000 (0.010)\tloss 5.082 (5.016)\n",
            "epoch 19, total time 156.37\n",
            "Train: [20][10/196]\tBT 0.788 (0.894)\tDT 0.000 (0.155)\tloss 4.920 (5.007)\n",
            "Train: [20][20/196]\tBT 0.793 (0.843)\tDT 0.000 (0.078)\tloss 4.988 (5.018)\n",
            "Train: [20][30/196]\tBT 0.792 (0.826)\tDT 0.000 (0.052)\tloss 5.041 (4.998)\n",
            "Train: [20][40/196]\tBT 0.793 (0.817)\tDT 0.000 (0.039)\tloss 4.917 (4.986)\n",
            "Train: [20][50/196]\tBT 0.797 (0.812)\tDT 0.000 (0.031)\tloss 4.930 (4.987)\n",
            "Train: [20][60/196]\tBT 0.788 (0.809)\tDT 0.000 (0.026)\tloss 5.076 (4.985)\n",
            "Train: [20][70/196]\tBT 0.793 (0.806)\tDT 0.000 (0.023)\tloss 4.913 (4.975)\n",
            "Train: [20][80/196]\tBT 0.792 (0.805)\tDT 0.000 (0.020)\tloss 5.077 (4.970)\n",
            "Train: [20][90/196]\tBT 0.791 (0.803)\tDT 0.000 (0.018)\tloss 5.108 (4.978)\n",
            "Train: [20][100/196]\tBT 0.789 (0.802)\tDT 0.000 (0.016)\tloss 4.957 (4.976)\n",
            "Train: [20][110/196]\tBT 0.791 (0.801)\tDT 0.000 (0.015)\tloss 4.990 (4.975)\n",
            "Train: [20][120/196]\tBT 0.786 (0.800)\tDT 0.000 (0.013)\tloss 5.024 (4.974)\n",
            "Train: [20][130/196]\tBT 0.796 (0.800)\tDT 0.000 (0.012)\tloss 4.982 (4.975)\n",
            "Train: [20][140/196]\tBT 0.797 (0.799)\tDT 0.000 (0.012)\tloss 4.929 (4.973)\n",
            "Train: [20][150/196]\tBT 0.797 (0.799)\tDT 0.000 (0.011)\tloss 4.875 (4.972)\n",
            "Train: [20][160/196]\tBT 0.795 (0.798)\tDT 0.000 (0.010)\tloss 4.951 (4.967)\n",
            "Train: [20][170/196]\tBT 0.798 (0.798)\tDT 0.000 (0.010)\tloss 5.032 (4.969)\n",
            "Train: [20][180/196]\tBT 0.794 (0.797)\tDT 0.000 (0.009)\tloss 5.088 (4.968)\n",
            "Train: [20][190/196]\tBT 0.791 (0.797)\tDT 0.000 (0.009)\tloss 4.812 (4.965)\n",
            "epoch 20, total time 156.13\n",
            "Train: [21][10/196]\tBT 0.793 (0.880)\tDT 0.000 (0.154)\tloss 4.774 (4.884)\n",
            "Train: [21][20/196]\tBT 0.787 (0.835)\tDT 0.000 (0.077)\tloss 4.857 (4.866)\n",
            "Train: [21][30/196]\tBT 0.793 (0.821)\tDT 0.000 (0.052)\tloss 4.981 (4.891)\n",
            "Train: [21][40/196]\tBT 0.790 (0.813)\tDT 0.000 (0.039)\tloss 4.932 (4.881)\n",
            "Train: [21][50/196]\tBT 0.796 (0.809)\tDT 0.000 (0.031)\tloss 4.882 (4.887)\n",
            "Train: [21][60/196]\tBT 0.792 (0.806)\tDT 0.000 (0.026)\tloss 4.843 (4.887)\n",
            "Train: [21][70/196]\tBT 0.793 (0.804)\tDT 0.000 (0.022)\tloss 5.106 (4.890)\n",
            "Train: [21][80/196]\tBT 0.793 (0.803)\tDT 0.000 (0.020)\tloss 4.902 (4.888)\n",
            "Train: [21][90/196]\tBT 0.795 (0.801)\tDT 0.000 (0.017)\tloss 4.793 (4.886)\n",
            "Train: [21][100/196]\tBT 0.796 (0.800)\tDT 0.000 (0.016)\tloss 4.844 (4.891)\n",
            "Train: [21][110/196]\tBT 0.794 (0.800)\tDT 0.000 (0.014)\tloss 4.709 (4.890)\n",
            "Train: [21][120/196]\tBT 0.790 (0.799)\tDT 0.000 (0.013)\tloss 5.093 (4.892)\n",
            "Train: [21][130/196]\tBT 0.788 (0.798)\tDT 0.000 (0.012)\tloss 4.887 (4.893)\n",
            "Train: [21][140/196]\tBT 0.788 (0.798)\tDT 0.000 (0.011)\tloss 4.865 (4.893)\n",
            "Train: [21][150/196]\tBT 0.788 (0.797)\tDT 0.000 (0.011)\tloss 5.067 (4.894)\n",
            "Train: [21][160/196]\tBT 0.788 (0.797)\tDT 0.000 (0.010)\tloss 4.771 (4.893)\n",
            "Train: [21][170/196]\tBT 0.797 (0.797)\tDT 0.000 (0.009)\tloss 4.966 (4.893)\n",
            "Train: [21][180/196]\tBT 0.797 (0.797)\tDT 0.000 (0.009)\tloss 4.853 (4.893)\n",
            "Train: [21][190/196]\tBT 0.792 (0.796)\tDT 0.000 (0.008)\tloss 4.879 (4.893)\n",
            "epoch 21, total time 155.96\n",
            "Train: [22][10/196]\tBT 0.792 (0.871)\tDT 0.000 (0.130)\tloss 5.084 (4.880)\n",
            "Train: [22][20/196]\tBT 0.791 (0.831)\tDT 0.000 (0.065)\tloss 4.845 (4.846)\n",
            "Train: [22][30/196]\tBT 0.789 (0.817)\tDT 0.000 (0.044)\tloss 4.874 (4.854)\n",
            "Train: [22][40/196]\tBT 0.791 (0.810)\tDT 0.000 (0.033)\tloss 4.968 (4.844)\n",
            "Train: [22][50/196]\tBT 0.787 (0.806)\tDT 0.000 (0.026)\tloss 4.676 (4.839)\n",
            "Train: [22][60/196]\tBT 0.791 (0.803)\tDT 0.000 (0.022)\tloss 4.677 (4.833)\n",
            "Train: [22][70/196]\tBT 0.795 (0.802)\tDT 0.000 (0.019)\tloss 4.891 (4.833)\n",
            "Train: [22][80/196]\tBT 0.800 (0.800)\tDT 0.000 (0.017)\tloss 4.790 (4.835)\n",
            "Train: [22][90/196]\tBT 0.797 (0.799)\tDT 0.000 (0.015)\tloss 4.721 (4.834)\n",
            "Train: [22][100/196]\tBT 0.795 (0.799)\tDT 0.000 (0.013)\tloss 4.894 (4.838)\n",
            "Train: [22][110/196]\tBT 0.791 (0.798)\tDT 0.000 (0.012)\tloss 4.920 (4.837)\n",
            "Train: [22][120/196]\tBT 0.790 (0.798)\tDT 0.000 (0.011)\tloss 4.701 (4.836)\n",
            "Train: [22][130/196]\tBT 0.788 (0.797)\tDT 0.000 (0.010)\tloss 4.773 (4.834)\n",
            "Train: [22][140/196]\tBT 0.797 (0.797)\tDT 0.000 (0.010)\tloss 4.842 (4.833)\n",
            "Train: [22][150/196]\tBT 0.792 (0.796)\tDT 0.000 (0.009)\tloss 4.699 (4.831)\n",
            "Train: [22][160/196]\tBT 0.790 (0.796)\tDT 0.000 (0.009)\tloss 4.702 (4.828)\n",
            "Train: [22][170/196]\tBT 0.792 (0.796)\tDT 0.000 (0.008)\tloss 4.864 (4.830)\n",
            "Train: [22][180/196]\tBT 0.793 (0.796)\tDT 0.000 (0.008)\tloss 4.920 (4.831)\n",
            "Train: [22][190/196]\tBT 0.791 (0.795)\tDT 0.000 (0.007)\tloss 4.704 (4.830)\n",
            "epoch 22, total time 155.78\n",
            "Train: [23][10/196]\tBT 0.785 (0.912)\tDT 0.000 (0.172)\tloss 4.870 (4.780)\n",
            "Train: [23][20/196]\tBT 0.789 (0.851)\tDT 0.000 (0.086)\tloss 5.001 (4.814)\n",
            "Train: [23][30/196]\tBT 0.792 (0.830)\tDT 0.000 (0.058)\tloss 4.809 (4.799)\n",
            "Train: [23][40/196]\tBT 0.790 (0.820)\tDT 0.000 (0.043)\tloss 4.680 (4.791)\n",
            "Train: [23][50/196]\tBT 0.795 (0.814)\tDT 0.000 (0.035)\tloss 4.705 (4.785)\n",
            "Train: [23][60/196]\tBT 0.789 (0.811)\tDT 0.000 (0.029)\tloss 4.757 (4.780)\n",
            "Train: [23][70/196]\tBT 0.789 (0.808)\tDT 0.000 (0.025)\tloss 4.754 (4.776)\n",
            "Train: [23][80/196]\tBT 0.794 (0.806)\tDT 0.000 (0.022)\tloss 4.687 (4.776)\n",
            "Train: [23][90/196]\tBT 0.794 (0.804)\tDT 0.000 (0.020)\tloss 4.754 (4.779)\n",
            "Train: [23][100/196]\tBT 0.789 (0.803)\tDT 0.000 (0.018)\tloss 4.732 (4.782)\n",
            "Train: [23][110/196]\tBT 0.787 (0.802)\tDT 0.000 (0.016)\tloss 4.847 (4.781)\n",
            "Train: [23][120/196]\tBT 0.789 (0.801)\tDT 0.000 (0.015)\tloss 4.733 (4.782)\n",
            "Train: [23][130/196]\tBT 0.792 (0.800)\tDT 0.000 (0.014)\tloss 4.805 (4.785)\n",
            "Train: [23][140/196]\tBT 0.791 (0.800)\tDT 0.000 (0.013)\tloss 4.838 (4.786)\n",
            "Train: [23][150/196]\tBT 0.790 (0.799)\tDT 0.000 (0.012)\tloss 4.788 (4.783)\n",
            "Train: [23][160/196]\tBT 0.787 (0.799)\tDT 0.000 (0.011)\tloss 4.677 (4.783)\n",
            "Train: [23][170/196]\tBT 0.790 (0.798)\tDT 0.000 (0.011)\tloss 4.675 (4.782)\n",
            "Train: [23][180/196]\tBT 0.791 (0.798)\tDT 0.000 (0.010)\tloss 4.664 (4.779)\n",
            "Train: [23][190/196]\tBT 0.801 (0.798)\tDT 0.000 (0.009)\tloss 4.782 (4.780)\n",
            "epoch 23, total time 156.22\n",
            "Train: [24][10/196]\tBT 0.789 (0.899)\tDT 0.000 (0.157)\tloss 4.882 (4.753)\n",
            "Train: [24][20/196]\tBT 0.790 (0.845)\tDT 0.000 (0.079)\tloss 4.928 (4.784)\n",
            "Train: [24][30/196]\tBT 0.791 (0.828)\tDT 0.000 (0.053)\tloss 4.843 (4.770)\n",
            "Train: [24][40/196]\tBT 0.788 (0.819)\tDT 0.000 (0.040)\tloss 4.748 (4.761)\n",
            "Train: [24][50/196]\tBT 0.795 (0.813)\tDT 0.000 (0.032)\tloss 4.754 (4.761)\n",
            "Train: [24][60/196]\tBT 0.800 (0.810)\tDT 0.000 (0.027)\tloss 4.714 (4.755)\n",
            "Train: [24][70/196]\tBT 0.793 (0.807)\tDT 0.000 (0.023)\tloss 4.611 (4.752)\n",
            "Train: [24][80/196]\tBT 0.789 (0.805)\tDT 0.000 (0.020)\tloss 4.724 (4.754)\n",
            "Train: [24][90/196]\tBT 0.794 (0.804)\tDT 0.000 (0.018)\tloss 4.820 (4.752)\n",
            "Train: [24][100/196]\tBT 0.795 (0.803)\tDT 0.000 (0.016)\tloss 4.612 (4.746)\n",
            "Train: [24][110/196]\tBT 0.791 (0.802)\tDT 0.000 (0.015)\tloss 4.685 (4.744)\n",
            "Train: [24][120/196]\tBT 0.792 (0.801)\tDT 0.000 (0.014)\tloss 4.686 (4.734)\n",
            "Train: [24][130/196]\tBT 0.794 (0.800)\tDT 0.000 (0.013)\tloss 4.655 (4.731)\n",
            "Train: [24][140/196]\tBT 0.794 (0.799)\tDT 0.000 (0.012)\tloss 4.805 (4.729)\n",
            "Train: [24][150/196]\tBT 0.790 (0.799)\tDT 0.000 (0.011)\tloss 4.660 (4.729)\n",
            "Train: [24][160/196]\tBT 0.787 (0.798)\tDT 0.000 (0.010)\tloss 4.727 (4.732)\n",
            "Train: [24][170/196]\tBT 0.788 (0.798)\tDT 0.000 (0.010)\tloss 4.976 (4.735)\n",
            "Train: [24][180/196]\tBT 0.789 (0.798)\tDT 0.000 (0.009)\tloss 4.551 (4.733)\n",
            "Train: [24][190/196]\tBT 0.795 (0.797)\tDT 0.000 (0.009)\tloss 4.805 (4.732)\n",
            "epoch 24, total time 156.15\n",
            "Train: [25][10/196]\tBT 0.787 (0.915)\tDT 0.000 (0.172)\tloss 4.738 (4.699)\n",
            "Train: [25][20/196]\tBT 0.784 (0.854)\tDT 0.000 (0.086)\tloss 4.439 (4.708)\n",
            "Train: [25][30/196]\tBT 0.791 (0.833)\tDT 0.000 (0.058)\tloss 4.624 (4.704)\n",
            "Train: [25][40/196]\tBT 0.794 (0.823)\tDT 0.000 (0.043)\tloss 4.645 (4.700)\n",
            "Train: [25][50/196]\tBT 0.786 (0.817)\tDT 0.000 (0.035)\tloss 4.575 (4.693)\n",
            "Train: [25][60/196]\tBT 0.794 (0.813)\tDT 0.000 (0.029)\tloss 4.711 (4.693)\n",
            "Train: [25][70/196]\tBT 0.788 (0.810)\tDT 0.000 (0.025)\tloss 4.586 (4.687)\n",
            "Train: [25][80/196]\tBT 0.788 (0.808)\tDT 0.000 (0.022)\tloss 4.532 (4.688)\n",
            "Train: [25][90/196]\tBT 0.786 (0.806)\tDT 0.000 (0.019)\tloss 4.615 (4.691)\n",
            "Train: [25][100/196]\tBT 0.789 (0.804)\tDT 0.000 (0.018)\tloss 4.694 (4.685)\n",
            "Train: [25][110/196]\tBT 0.792 (0.803)\tDT 0.000 (0.016)\tloss 4.637 (4.685)\n",
            "Train: [25][120/196]\tBT 0.789 (0.802)\tDT 0.000 (0.015)\tloss 4.461 (4.686)\n",
            "Train: [25][130/196]\tBT 0.782 (0.801)\tDT 0.000 (0.014)\tloss 4.611 (4.683)\n",
            "Train: [25][140/196]\tBT 0.788 (0.801)\tDT 0.000 (0.013)\tloss 4.725 (4.684)\n",
            "Train: [25][150/196]\tBT 0.790 (0.800)\tDT 0.000 (0.012)\tloss 4.572 (4.684)\n",
            "Train: [25][160/196]\tBT 0.793 (0.800)\tDT 0.000 (0.011)\tloss 4.687 (4.683)\n",
            "Train: [25][170/196]\tBT 0.793 (0.799)\tDT 0.000 (0.011)\tloss 4.747 (4.683)\n",
            "Train: [25][180/196]\tBT 0.795 (0.799)\tDT 0.000 (0.010)\tloss 4.508 (4.680)\n",
            "Train: [25][190/196]\tBT 0.792 (0.798)\tDT 0.000 (0.009)\tloss 4.553 (4.680)\n",
            "epoch 25, total time 156.36\n",
            "==> Saving...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Entropy Loss Function on CIFAR10"
      ],
      "metadata": {
        "id": "DYJTblvxMDNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import tensorboard_logger as tb_logger\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "try:\n",
        "    import apex\n",
        "    from apex import amp, optimizers\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# Hardcoded configurations\n",
        "class Config:\n",
        "    print_freq = 10\n",
        "    save_freq = 50\n",
        "    batch_size = 256\n",
        "    num_workers = 16\n",
        "    epochs = 15\n",
        "    learning_rate = 0.2\n",
        "    lr_decay_epochs = [350, 400, 450]\n",
        "    lr_decay_rate = 0.1\n",
        "    weight_decay = 1e-4\n",
        "    momentum = 0.9\n",
        "    model = 'resnet50'\n",
        "    dataset = 'cifar10'\n",
        "    cosine = False\n",
        "    syncBN = False\n",
        "    warm = True\n",
        "    trial = '0'\n",
        "    data_folder = './datasets/'\n",
        "    model_path = './save/SupCon/cifar10_models'\n",
        "    tb_path = './save/SupCon/cifar10_tensorboard'\n",
        "\n",
        "opt = Config()\n",
        "\n",
        "opt.model_name = 'SupCE_{}_{}_lr_{}_decay_{}_bsz_{}_trial_{}'.format(\n",
        "    opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,\n",
        "    opt.batch_size, opt.trial)\n",
        "\n",
        "if opt.cosine:\n",
        "    opt.model_name = '{}_cosine'.format(opt.model_name)\n",
        "\n",
        "if opt.warm:\n",
        "    opt.model_name = '{}_warm'.format(opt.model_name)\n",
        "    opt.warmup_from = 0.01\n",
        "    opt.warm_epochs = 10\n",
        "    if opt.cosine:\n",
        "        eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
        "        opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
        "            1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
        "    else:\n",
        "        opt.warmup_to = opt.learning_rate\n",
        "\n",
        "opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n",
        "if not os.path.isdir(opt.tb_folder):\n",
        "    os.makedirs(opt.tb_folder)\n",
        "\n",
        "opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
        "if not os.path.isdir(opt.save_folder):\n",
        "    os.makedirs(opt.save_folder)\n",
        "\n",
        "opt.n_cls = 100\n",
        "\n",
        "def set_loader(opt):\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "    normalize = transforms.Normalize(mean=mean, std=std)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root=opt.data_folder,\n",
        "                                      transform=train_transform,\n",
        "                                      download=True)\n",
        "    val_dataset = datasets.CIFAR10(root=opt.data_folder,\n",
        "                                    train=False,\n",
        "                                    transform=val_transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=opt.batch_size, shuffle=True,\n",
        "        num_workers=opt.num_workers, pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=256, shuffle=False,\n",
        "        num_workers=8, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def set_model(opt):\n",
        "    model = SupCEResNet(name=opt.model, num_classes=opt.n_cls)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    if opt.syncBN:\n",
        "        model = apex.parallel.convert_syncbn_model(model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    return model, criterion\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, opt):\n",
        "    model.train()\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    end = time.time()\n",
        "\n",
        "    for idx, (images, labels) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "        images = images.cuda(non_blocking=True)\n",
        "        labels = labels.cuda(non_blocking=True)\n",
        "        bsz = labels.shape[0]\n",
        "        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        losses.update(loss.item(), bsz)\n",
        "        acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n",
        "        top1.update(acc1[0], bsz)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if (idx + 1) % opt.print_freq == 0:\n",
        "            print('Train: [{0}][{1}/{2}]\\t'\n",
        "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n",
        "                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses, top1=top1))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "def validate(val_loader, model, criterion, opt):\n",
        "    model.eval()\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for idx, (images, labels) in enumerate(val_loader):\n",
        "            images = images.float().cuda()\n",
        "            labels = labels.cuda()\n",
        "            bsz = labels.shape[0]\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            losses.update(loss.item(), bsz)\n",
        "            acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n",
        "            top1.update(acc1[0], bsz)\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if idx % opt.print_freq == 0:\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                       idx, len(val_loader), batch_time=batch_time,\n",
        "                       loss=losses, top1=top1))\n",
        "\n",
        "    print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "def main():\n",
        "    best_acc = 0\n",
        "    # build data loader\n",
        "    train_loader, val_loader = set_loader(opt)\n",
        "    # build model and criterion\n",
        "    model, criterion = set_model(opt)\n",
        "    # build optimizer\n",
        "    optimizer = set_optimizer(opt, model)\n",
        "    # tensorboard\n",
        "    logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n",
        "    # training routine\n",
        "    for epoch in range(1, opt.epochs + 1):\n",
        "        adjust_learning_rate(opt, optimizer, epoch)\n",
        "        time1 = time.time()\n",
        "        loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, opt)\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "        logger.log_value('train_loss', loss, epoch)\n",
        "        logger.log_value('train_acc', train_acc, epoch)\n",
        "        logger.log_value('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
        "        loss, val_acc = validate(val_loader, model, criterion, opt)\n",
        "        logger.log_value('val_loss', loss, epoch)\n",
        "        logger.log_value('val_acc', val_acc, epoch)\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "        if epoch % opt.save_freq == 0:\n",
        "            save_file = os.path.join(\n",
        "                opt.save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            save_model(model, optimizer, opt, epoch, save_file)\n",
        "    save_file = os.path.join(\n",
        "        opt.save_folder, 'last.pth')\n",
        "    save_model(model, optimizer, opt, opt.epochs, save_file)\n",
        "    print('best accuracy: {:.2f}'.format(best_acc))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWUfMkbey_vM",
        "outputId": "397504b1-6d9d-4a1d-fbd8-ca2c1190c4e5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13170340.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/cifar-10-python.tar.gz to ./datasets/\n",
            "Train: [1][10/196]\tBT 0.387 (0.791)\tDT 0.000 (0.070)\tloss 3.701 (3.308)\tAcc@1 9.766 (10.508)\n",
            "Train: [1][20/196]\tBT 0.387 (0.589)\tDT 0.000 (0.047)\tloss 4.496 (3.689)\tAcc@1 9.766 (10.645)\n",
            "Train: [1][30/196]\tBT 0.385 (0.521)\tDT 0.000 (0.040)\tloss 3.243 (3.651)\tAcc@1 10.156 (10.547)\n",
            "Train: [1][40/196]\tBT 0.386 (0.487)\tDT 0.000 (0.036)\tloss 2.709 (3.479)\tAcc@1 8.984 (10.996)\n",
            "Train: [1][50/196]\tBT 0.385 (0.467)\tDT 0.000 (0.034)\tloss 2.490 (3.289)\tAcc@1 14.453 (12.125)\n",
            "Train: [1][60/196]\tBT 0.384 (0.453)\tDT 0.000 (0.032)\tloss 2.518 (3.144)\tAcc@1 15.625 (13.164)\n",
            "Train: [1][70/196]\tBT 0.384 (0.443)\tDT 0.000 (0.031)\tloss 2.498 (3.036)\tAcc@1 18.359 (13.795)\n",
            "Train: [1][80/196]\tBT 0.382 (0.436)\tDT 0.000 (0.030)\tloss 2.042 (2.929)\tAcc@1 23.828 (14.644)\n",
            "Train: [1][90/196]\tBT 0.382 (0.430)\tDT 0.000 (0.030)\tloss 2.022 (2.836)\tAcc@1 20.312 (15.356)\n",
            "Train: [1][100/196]\tBT 0.381 (0.425)\tDT 0.000 (0.029)\tloss 2.472 (2.763)\tAcc@1 21.484 (16.227)\n",
            "Train: [1][110/196]\tBT 0.383 (0.422)\tDT 0.000 (0.029)\tloss 2.211 (2.702)\tAcc@1 21.094 (16.889)\n",
            "Train: [1][120/196]\tBT 0.383 (0.418)\tDT 0.000 (0.028)\tloss 2.274 (2.651)\tAcc@1 18.359 (17.409)\n",
            "Train: [1][130/196]\tBT 0.381 (0.415)\tDT 0.000 (0.028)\tloss 1.970 (2.601)\tAcc@1 20.703 (18.119)\n",
            "Train: [1][140/196]\tBT 0.383 (0.413)\tDT 0.000 (0.028)\tloss 1.801 (2.557)\tAcc@1 35.547 (18.758)\n",
            "Train: [1][150/196]\tBT 0.381 (0.411)\tDT 0.000 (0.027)\tloss 2.017 (2.519)\tAcc@1 31.641 (19.370)\n",
            "Train: [1][160/196]\tBT 0.382 (0.409)\tDT 0.000 (0.027)\tloss 1.860 (2.487)\tAcc@1 33.203 (19.937)\n",
            "Train: [1][170/196]\tBT 0.381 (0.408)\tDT 0.000 (0.027)\tloss 1.817 (2.454)\tAcc@1 30.469 (20.487)\n",
            "Train: [1][180/196]\tBT 0.383 (0.406)\tDT 0.000 (0.027)\tloss 1.888 (2.426)\tAcc@1 28.906 (20.987)\n",
            "Train: [1][190/196]\tBT 0.381 (0.405)\tDT 0.000 (0.027)\tloss 1.799 (2.397)\tAcc@1 33.984 (21.606)\n",
            "epoch 1, total time 80.65\n",
            "Test: [0/40]\tTime 0.437 (0.437)\tLoss 2.0464 (2.0464)\tAcc@1 35.547 (35.547)\n",
            "Test: [10/40]\tTime 0.122 (0.151)\tLoss 1.7486 (1.7601)\tAcc@1 28.125 (36.293)\n",
            "Test: [20/40]\tTime 0.121 (0.137)\tLoss 1.7573 (1.7696)\tAcc@1 33.594 (36.124)\n",
            "Test: [30/40]\tTime 0.121 (0.132)\tLoss 1.7192 (1.7588)\tAcc@1 35.156 (35.572)\n",
            " * Acc@1 35.420\n",
            "Train: [2][10/196]\tBT 0.381 (0.431)\tDT 0.000 (0.068)\tloss 1.970 (1.893)\tAcc@1 29.688 (30.547)\n",
            "Train: [2][20/196]\tBT 0.384 (0.407)\tDT 0.000 (0.046)\tloss 1.810 (1.875)\tAcc@1 36.719 (31.445)\n",
            "Train: [2][30/196]\tBT 0.382 (0.399)\tDT 0.000 (0.039)\tloss 1.844 (1.854)\tAcc@1 30.078 (31.875)\n",
            "Train: [2][40/196]\tBT 0.384 (0.395)\tDT 0.000 (0.035)\tloss 1.816 (1.844)\tAcc@1 32.031 (31.826)\n",
            "Train: [2][50/196]\tBT 0.385 (0.393)\tDT 0.000 (0.033)\tloss 1.811 (1.836)\tAcc@1 35.547 (32.180)\n",
            "Train: [2][60/196]\tBT 0.383 (0.391)\tDT 0.000 (0.032)\tloss 1.618 (1.820)\tAcc@1 41.016 (32.663)\n",
            "Train: [2][70/196]\tBT 0.384 (0.390)\tDT 0.000 (0.031)\tloss 1.752 (1.815)\tAcc@1 33.984 (32.840)\n",
            "Train: [2][80/196]\tBT 0.383 (0.389)\tDT 0.000 (0.030)\tloss 1.808 (1.809)\tAcc@1 34.375 (33.145)\n",
            "Train: [2][90/196]\tBT 0.383 (0.389)\tDT 0.000 (0.029)\tloss 1.930 (1.804)\tAcc@1 32.422 (33.611)\n",
            "Train: [2][100/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 1.786 (1.801)\tAcc@1 33.984 (33.754)\n",
            "Train: [2][110/196]\tBT 0.385 (0.388)\tDT 0.000 (0.028)\tloss 1.770 (1.797)\tAcc@1 35.156 (34.123)\n",
            "Train: [2][120/196]\tBT 0.379 (0.387)\tDT 0.000 (0.028)\tloss 1.677 (1.792)\tAcc@1 43.359 (34.349)\n",
            "Train: [2][130/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 1.812 (1.787)\tAcc@1 35.156 (34.543)\n",
            "Train: [2][140/196]\tBT 0.384 (0.387)\tDT 0.000 (0.028)\tloss 1.739 (1.782)\tAcc@1 36.328 (34.777)\n",
            "Train: [2][150/196]\tBT 0.385 (0.387)\tDT 0.000 (0.027)\tloss 1.659 (1.780)\tAcc@1 41.406 (34.904)\n",
            "Train: [2][160/196]\tBT 0.381 (0.386)\tDT 0.000 (0.027)\tloss 1.736 (1.782)\tAcc@1 32.812 (34.880)\n",
            "Train: [2][170/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 1.770 (1.781)\tAcc@1 34.375 (34.864)\n",
            "Train: [2][180/196]\tBT 0.382 (0.386)\tDT 0.000 (0.027)\tloss 1.704 (1.779)\tAcc@1 36.328 (34.878)\n",
            "Train: [2][190/196]\tBT 0.387 (0.386)\tDT 0.000 (0.027)\tloss 1.681 (1.776)\tAcc@1 38.281 (34.895)\n",
            "epoch 2, total time 75.61\n",
            "Test: [0/40]\tTime 0.394 (0.394)\tLoss 1.5889 (1.5889)\tAcc@1 40.234 (40.234)\n",
            "Test: [10/40]\tTime 0.122 (0.147)\tLoss 1.8534 (1.6589)\tAcc@1 39.844 (40.341)\n",
            "Test: [20/40]\tTime 0.121 (0.135)\tLoss 1.8075 (1.6304)\tAcc@1 40.234 (41.164)\n",
            "Test: [30/40]\tTime 0.122 (0.131)\tLoss 1.5604 (1.6286)\tAcc@1 46.484 (41.419)\n",
            " * Acc@1 41.100\n",
            "Train: [3][10/196]\tBT 0.382 (0.425)\tDT 0.000 (0.066)\tloss 1.679 (1.683)\tAcc@1 38.281 (37.930)\n",
            "Train: [3][20/196]\tBT 0.383 (0.404)\tDT 0.000 (0.045)\tloss 1.632 (1.678)\tAcc@1 39.062 (38.242)\n",
            "Train: [3][30/196]\tBT 0.382 (0.397)\tDT 0.000 (0.038)\tloss 1.715 (1.658)\tAcc@1 38.281 (38.984)\n",
            "Train: [3][40/196]\tBT 0.383 (0.393)\tDT 0.000 (0.035)\tloss 1.675 (1.649)\tAcc@1 35.547 (39.434)\n",
            "Train: [3][50/196]\tBT 0.382 (0.391)\tDT 0.000 (0.033)\tloss 1.599 (1.654)\tAcc@1 44.922 (39.258)\n",
            "Train: [3][60/196]\tBT 0.382 (0.390)\tDT 0.000 (0.031)\tloss 1.533 (1.649)\tAcc@1 42.188 (39.414)\n",
            "Train: [3][70/196]\tBT 0.383 (0.389)\tDT 0.000 (0.030)\tloss 1.469 (1.633)\tAcc@1 45.703 (40.162)\n",
            "Train: [3][80/196]\tBT 0.382 (0.388)\tDT 0.000 (0.030)\tloss 1.472 (1.620)\tAcc@1 47.266 (40.635)\n",
            "Train: [3][90/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 1.626 (1.622)\tAcc@1 41.016 (40.716)\n",
            "Train: [3][100/196]\tBT 0.384 (0.387)\tDT 0.000 (0.029)\tloss 1.568 (1.621)\tAcc@1 45.312 (40.906)\n",
            "Train: [3][110/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 1.638 (1.616)\tAcc@1 35.547 (41.051)\n",
            "Train: [3][120/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 1.530 (1.608)\tAcc@1 46.094 (41.380)\n",
            "Train: [3][130/196]\tBT 0.383 (0.386)\tDT 0.000 (0.028)\tloss 1.519 (1.605)\tAcc@1 46.875 (41.590)\n",
            "Train: [3][140/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 1.529 (1.602)\tAcc@1 40.625 (41.680)\n",
            "Train: [3][150/196]\tBT 0.382 (0.386)\tDT 0.000 (0.027)\tloss 1.589 (1.601)\tAcc@1 41.406 (41.802)\n",
            "Train: [3][160/196]\tBT 0.385 (0.386)\tDT 0.000 (0.027)\tloss 1.464 (1.597)\tAcc@1 48.047 (41.926)\n",
            "Train: [3][170/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 1.450 (1.590)\tAcc@1 47.656 (42.213)\n",
            "Train: [3][180/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 1.440 (1.585)\tAcc@1 48.438 (42.422)\n",
            "Train: [3][190/196]\tBT 0.384 (0.385)\tDT 0.000 (0.027)\tloss 1.496 (1.582)\tAcc@1 48.438 (42.537)\n",
            "epoch 3, total time 75.54\n",
            "Test: [0/40]\tTime 0.400 (0.400)\tLoss 1.8319 (1.8319)\tAcc@1 46.484 (46.484)\n",
            "Test: [10/40]\tTime 0.123 (0.148)\tLoss 1.4358 (1.5566)\tAcc@1 48.047 (47.976)\n",
            "Test: [20/40]\tTime 0.122 (0.136)\tLoss 1.5099 (1.5429)\tAcc@1 47.656 (48.642)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 1.4276 (1.5348)\tAcc@1 46.484 (47.845)\n",
            " * Acc@1 47.780\n",
            "Train: [4][10/196]\tBT 0.381 (0.422)\tDT 0.000 (0.061)\tloss 1.585 (1.514)\tAcc@1 41.016 (45.430)\n",
            "Train: [4][20/196]\tBT 0.382 (0.403)\tDT 0.000 (0.043)\tloss 1.439 (1.495)\tAcc@1 48.047 (45.898)\n",
            "Train: [4][30/196]\tBT 0.382 (0.396)\tDT 0.000 (0.037)\tloss 1.330 (1.469)\tAcc@1 51.953 (46.810)\n",
            "Train: [4][40/196]\tBT 0.384 (0.393)\tDT 0.000 (0.034)\tloss 1.463 (1.469)\tAcc@1 50.781 (47.012)\n",
            "Train: [4][50/196]\tBT 0.383 (0.391)\tDT 0.000 (0.032)\tloss 1.387 (1.460)\tAcc@1 48.828 (47.391)\n",
            "Train: [4][60/196]\tBT 0.387 (0.390)\tDT 0.000 (0.031)\tloss 1.199 (1.449)\tAcc@1 57.812 (47.799)\n",
            "Train: [4][70/196]\tBT 0.384 (0.389)\tDT 0.000 (0.030)\tloss 1.401 (1.437)\tAcc@1 49.609 (48.248)\n",
            "Train: [4][80/196]\tBT 0.383 (0.388)\tDT 0.000 (0.029)\tloss 1.469 (1.429)\tAcc@1 47.266 (48.691)\n",
            "Train: [4][90/196]\tBT 0.383 (0.388)\tDT 0.000 (0.029)\tloss 1.391 (1.428)\tAcc@1 50.781 (48.746)\n",
            "Train: [4][100/196]\tBT 0.382 (0.387)\tDT 0.000 (0.028)\tloss 1.369 (1.428)\tAcc@1 50.000 (48.695)\n",
            "Train: [4][110/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 1.242 (1.419)\tAcc@1 61.328 (49.098)\n",
            "Train: [4][120/196]\tBT 0.386 (0.387)\tDT 0.000 (0.028)\tloss 1.309 (1.411)\tAcc@1 54.297 (49.408)\n",
            "Train: [4][130/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 1.263 (1.405)\tAcc@1 56.250 (49.591)\n",
            "Train: [4][140/196]\tBT 0.379 (0.386)\tDT 0.000 (0.027)\tloss 1.373 (1.397)\tAcc@1 51.562 (49.888)\n",
            "Train: [4][150/196]\tBT 0.385 (0.386)\tDT 0.000 (0.027)\tloss 1.271 (1.389)\tAcc@1 51.172 (50.109)\n",
            "Train: [4][160/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 1.279 (1.386)\tAcc@1 54.688 (50.217)\n",
            "Train: [4][170/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 1.454 (1.381)\tAcc@1 50.391 (50.522)\n",
            "Train: [4][180/196]\tBT 0.381 (0.386)\tDT 0.000 (0.027)\tloss 1.360 (1.375)\tAcc@1 53.906 (50.755)\n",
            "Train: [4][190/196]\tBT 0.384 (0.385)\tDT 0.000 (0.026)\tloss 1.241 (1.369)\tAcc@1 56.641 (50.985)\n",
            "epoch 4, total time 75.54\n",
            "Test: [0/40]\tTime 0.435 (0.435)\tLoss 1.2108 (1.2108)\tAcc@1 55.859 (55.859)\n",
            "Test: [10/40]\tTime 0.122 (0.151)\tLoss 1.1635 (1.3360)\tAcc@1 65.234 (58.061)\n",
            "Test: [20/40]\tTime 0.122 (0.137)\tLoss 1.2201 (1.2473)\tAcc@1 58.594 (58.929)\n",
            "Test: [30/40]\tTime 0.123 (0.132)\tLoss 1.3897 (1.2630)\tAcc@1 57.031 (58.241)\n",
            " * Acc@1 58.210\n",
            "Train: [5][10/196]\tBT 0.382 (0.444)\tDT 0.000 (0.085)\tloss 1.296 (1.319)\tAcc@1 51.562 (52.539)\n",
            "Train: [5][20/196]\tBT 0.384 (0.413)\tDT 0.000 (0.055)\tloss 1.246 (1.300)\tAcc@1 54.297 (53.301)\n",
            "Train: [5][30/196]\tBT 0.383 (0.404)\tDT 0.000 (0.045)\tloss 1.242 (1.285)\tAcc@1 57.422 (53.997)\n",
            "Train: [5][40/196]\tBT 0.384 (0.399)\tDT 0.000 (0.040)\tloss 1.150 (1.278)\tAcc@1 59.375 (54.365)\n",
            "Train: [5][50/196]\tBT 0.382 (0.395)\tDT 0.000 (0.037)\tloss 1.300 (1.270)\tAcc@1 53.516 (54.734)\n",
            "Train: [5][60/196]\tBT 0.382 (0.393)\tDT 0.000 (0.035)\tloss 1.221 (1.259)\tAcc@1 60.156 (55.410)\n",
            "Train: [5][70/196]\tBT 0.378 (0.392)\tDT 0.000 (0.033)\tloss 1.246 (1.256)\tAcc@1 56.250 (55.396)\n",
            "Train: [5][80/196]\tBT 0.384 (0.391)\tDT 0.000 (0.032)\tloss 1.185 (1.248)\tAcc@1 59.766 (55.679)\n",
            "Train: [5][90/196]\tBT 0.382 (0.390)\tDT 0.000 (0.031)\tloss 1.317 (1.244)\tAcc@1 52.344 (55.794)\n",
            "Train: [5][100/196]\tBT 0.384 (0.389)\tDT 0.000 (0.031)\tloss 1.281 (1.242)\tAcc@1 55.469 (55.898)\n",
            "Train: [5][110/196]\tBT 0.383 (0.389)\tDT 0.000 (0.030)\tloss 1.127 (1.237)\tAcc@1 63.281 (56.236)\n",
            "Train: [5][120/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 1.196 (1.236)\tAcc@1 60.156 (56.266)\n",
            "Train: [5][130/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 1.272 (1.230)\tAcc@1 56.250 (56.535)\n",
            "Train: [5][140/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 1.120 (1.224)\tAcc@1 58.984 (56.738)\n",
            "Train: [5][150/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 1.073 (1.221)\tAcc@1 59.766 (56.849)\n",
            "Train: [5][160/196]\tBT 0.391 (0.387)\tDT 0.000 (0.028)\tloss 0.975 (1.217)\tAcc@1 62.109 (56.987)\n",
            "Train: [5][170/196]\tBT 0.384 (0.387)\tDT 0.000 (0.028)\tloss 1.177 (1.215)\tAcc@1 58.984 (57.066)\n",
            "Train: [5][180/196]\tBT 0.382 (0.387)\tDT 0.000 (0.028)\tloss 1.258 (1.212)\tAcc@1 55.859 (57.148)\n",
            "Train: [5][190/196]\tBT 0.382 (0.387)\tDT 0.000 (0.028)\tloss 1.094 (1.207)\tAcc@1 62.500 (57.299)\n",
            "epoch 5, total time 75.77\n",
            "Test: [0/40]\tTime 0.437 (0.437)\tLoss 1.0192 (1.0192)\tAcc@1 63.672 (63.672)\n",
            "Test: [10/40]\tTime 0.122 (0.151)\tLoss 1.0119 (1.0767)\tAcc@1 64.844 (62.891)\n",
            "Test: [20/40]\tTime 0.122 (0.138)\tLoss 1.0970 (1.0594)\tAcc@1 63.672 (63.300)\n",
            "Test: [30/40]\tTime 0.122 (0.133)\tLoss 1.1478 (1.0638)\tAcc@1 58.594 (62.815)\n",
            " * Acc@1 63.000\n",
            "Train: [6][10/196]\tBT 0.384 (0.434)\tDT 0.000 (0.075)\tloss 1.045 (1.096)\tAcc@1 60.156 (60.547)\n",
            "Train: [6][20/196]\tBT 0.382 (0.409)\tDT 0.000 (0.050)\tloss 1.097 (1.088)\tAcc@1 59.375 (60.820)\n",
            "Train: [6][30/196]\tBT 0.383 (0.400)\tDT 0.000 (0.042)\tloss 1.146 (1.100)\tAcc@1 60.547 (60.299)\n",
            "Train: [6][40/196]\tBT 0.381 (0.396)\tDT 0.000 (0.037)\tloss 1.118 (1.101)\tAcc@1 62.109 (60.469)\n",
            "Train: [6][50/196]\tBT 0.383 (0.394)\tDT 0.000 (0.035)\tloss 1.032 (1.096)\tAcc@1 62.500 (61.008)\n",
            "Train: [6][60/196]\tBT 0.383 (0.392)\tDT 0.000 (0.033)\tloss 1.245 (1.105)\tAcc@1 58.984 (60.742)\n",
            "Train: [6][70/196]\tBT 0.383 (0.391)\tDT 0.000 (0.032)\tloss 1.185 (1.108)\tAcc@1 58.984 (60.781)\n",
            "Train: [6][80/196]\tBT 0.377 (0.390)\tDT 0.000 (0.031)\tloss 1.029 (1.108)\tAcc@1 63.672 (60.806)\n",
            "Train: [6][90/196]\tBT 0.383 (0.389)\tDT 0.000 (0.030)\tloss 1.066 (1.102)\tAcc@1 66.016 (61.003)\n",
            "Train: [6][100/196]\tBT 0.384 (0.389)\tDT 0.000 (0.030)\tloss 1.172 (1.099)\tAcc@1 59.766 (61.199)\n",
            "Train: [6][110/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 1.056 (1.095)\tAcc@1 60.547 (61.374)\n",
            "Train: [6][120/196]\tBT 0.383 (0.388)\tDT 0.000 (0.029)\tloss 1.115 (1.096)\tAcc@1 58.984 (61.276)\n",
            "Train: [6][130/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 1.096 (1.090)\tAcc@1 61.328 (61.532)\n",
            "Train: [6][140/196]\tBT 0.382 (0.387)\tDT 0.000 (0.028)\tloss 1.035 (1.085)\tAcc@1 65.625 (61.802)\n",
            "Train: [6][150/196]\tBT 0.385 (0.387)\tDT 0.000 (0.028)\tloss 1.183 (1.083)\tAcc@1 59.766 (61.945)\n",
            "Train: [6][160/196]\tBT 0.384 (0.387)\tDT 0.000 (0.028)\tloss 1.004 (1.080)\tAcc@1 61.328 (62.026)\n",
            "Train: [6][170/196]\tBT 0.382 (0.386)\tDT 0.000 (0.027)\tloss 1.074 (1.079)\tAcc@1 63.672 (62.107)\n",
            "Train: [6][180/196]\tBT 0.382 (0.386)\tDT 0.000 (0.027)\tloss 1.054 (1.076)\tAcc@1 62.500 (62.194)\n",
            "Train: [6][190/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 0.973 (1.074)\tAcc@1 65.625 (62.272)\n",
            "epoch 6, total time 75.67\n",
            "Test: [0/40]\tTime 0.396 (0.396)\tLoss 0.8230 (0.8230)\tAcc@1 69.922 (69.922)\n",
            "Test: [10/40]\tTime 0.122 (0.147)\tLoss 0.9390 (0.9567)\tAcc@1 68.750 (67.223)\n",
            "Test: [20/40]\tTime 0.122 (0.136)\tLoss 1.0533 (0.9441)\tAcc@1 63.672 (67.485)\n",
            "Test: [30/40]\tTime 0.121 (0.131)\tLoss 1.0957 (0.9578)\tAcc@1 64.844 (67.225)\n",
            " * Acc@1 67.410\n",
            "Train: [7][10/196]\tBT 0.382 (0.429)\tDT 0.000 (0.069)\tloss 1.060 (1.081)\tAcc@1 62.500 (62.031)\n",
            "Train: [7][20/196]\tBT 0.385 (0.406)\tDT 0.000 (0.046)\tloss 0.980 (1.052)\tAcc@1 66.016 (63.145)\n",
            "Train: [7][30/196]\tBT 0.382 (0.398)\tDT 0.000 (0.039)\tloss 1.105 (1.047)\tAcc@1 60.156 (63.203)\n",
            "Train: [7][40/196]\tBT 0.383 (0.394)\tDT 0.000 (0.035)\tloss 0.971 (1.034)\tAcc@1 68.359 (63.379)\n",
            "Train: [7][50/196]\tBT 0.384 (0.392)\tDT 0.000 (0.033)\tloss 1.129 (1.035)\tAcc@1 62.500 (63.633)\n",
            "Train: [7][60/196]\tBT 0.382 (0.391)\tDT 0.000 (0.032)\tloss 0.996 (1.030)\tAcc@1 66.406 (63.717)\n",
            "Train: [7][70/196]\tBT 0.383 (0.390)\tDT 0.000 (0.031)\tloss 0.982 (1.026)\tAcc@1 64.062 (63.783)\n",
            "Train: [7][80/196]\tBT 0.385 (0.389)\tDT 0.000 (0.030)\tloss 1.082 (1.022)\tAcc@1 59.375 (63.809)\n",
            "Train: [7][90/196]\tBT 0.383 (0.388)\tDT 0.000 (0.029)\tloss 1.070 (1.019)\tAcc@1 58.594 (63.854)\n",
            "Train: [7][100/196]\tBT 0.382 (0.388)\tDT 0.000 (0.029)\tloss 0.910 (1.011)\tAcc@1 68.750 (64.102)\n",
            "Train: [7][110/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 1.031 (1.009)\tAcc@1 64.844 (64.233)\n",
            "Train: [7][120/196]\tBT 0.385 (0.387)\tDT 0.000 (0.028)\tloss 1.045 (1.009)\tAcc@1 61.328 (64.326)\n",
            "Train: [7][130/196]\tBT 0.384 (0.387)\tDT 0.000 (0.028)\tloss 0.993 (1.006)\tAcc@1 64.453 (64.450)\n",
            "Train: [7][140/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 0.947 (1.002)\tAcc@1 64.453 (64.604)\n",
            "Train: [7][150/196]\tBT 0.386 (0.386)\tDT 0.000 (0.027)\tloss 0.983 (0.999)\tAcc@1 66.797 (64.742)\n",
            "Train: [7][160/196]\tBT 0.385 (0.386)\tDT 0.000 (0.027)\tloss 0.920 (0.996)\tAcc@1 70.703 (64.863)\n",
            "Train: [7][170/196]\tBT 0.384 (0.386)\tDT 0.000 (0.027)\tloss 0.985 (0.994)\tAcc@1 63.281 (64.972)\n",
            "Train: [7][180/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 0.989 (0.990)\tAcc@1 66.406 (65.184)\n",
            "Train: [7][190/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 0.883 (0.985)\tAcc@1 71.484 (65.387)\n",
            "epoch 7, total time 75.63\n",
            "Test: [0/40]\tTime 0.447 (0.447)\tLoss 1.3225 (1.3225)\tAcc@1 59.766 (59.766)\n",
            "Test: [10/40]\tTime 0.121 (0.152)\tLoss 1.2229 (1.3523)\tAcc@1 60.156 (59.055)\n",
            "Test: [20/40]\tTime 0.122 (0.138)\tLoss 1.4041 (1.3173)\tAcc@1 55.859 (59.170)\n",
            "Test: [30/40]\tTime 0.122 (0.133)\tLoss 1.3042 (1.3025)\tAcc@1 60.938 (59.060)\n",
            " * Acc@1 59.100\n",
            "Train: [8][10/196]\tBT 0.382 (0.436)\tDT 0.000 (0.076)\tloss 0.897 (0.945)\tAcc@1 71.484 (66.445)\n",
            "Train: [8][20/196]\tBT 0.385 (0.410)\tDT 0.000 (0.050)\tloss 1.019 (0.969)\tAcc@1 59.766 (65.977)\n",
            "Train: [8][30/196]\tBT 0.384 (0.401)\tDT 0.000 (0.042)\tloss 1.054 (0.960)\tAcc@1 66.797 (66.576)\n",
            "Train: [8][40/196]\tBT 0.383 (0.396)\tDT 0.000 (0.037)\tloss 0.952 (0.948)\tAcc@1 69.531 (66.982)\n",
            "Train: [8][50/196]\tBT 0.381 (0.394)\tDT 0.000 (0.035)\tloss 1.062 (0.943)\tAcc@1 66.016 (67.016)\n",
            "Train: [8][60/196]\tBT 0.384 (0.392)\tDT 0.000 (0.033)\tloss 0.902 (0.935)\tAcc@1 66.406 (67.370)\n",
            "Train: [8][70/196]\tBT 0.385 (0.391)\tDT 0.000 (0.032)\tloss 0.936 (0.927)\tAcc@1 71.094 (67.651)\n",
            "Train: [8][80/196]\tBT 0.382 (0.390)\tDT 0.000 (0.031)\tloss 0.822 (0.922)\tAcc@1 75.000 (67.817)\n",
            "Train: [8][90/196]\tBT 0.382 (0.389)\tDT 0.000 (0.030)\tloss 0.931 (0.917)\tAcc@1 67.188 (68.008)\n",
            "Train: [8][100/196]\tBT 0.383 (0.389)\tDT 0.000 (0.030)\tloss 0.844 (0.917)\tAcc@1 71.875 (68.137)\n",
            "Train: [8][110/196]\tBT 0.385 (0.388)\tDT 0.000 (0.029)\tloss 0.930 (0.918)\tAcc@1 69.141 (68.107)\n",
            "Train: [8][120/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 0.812 (0.916)\tAcc@1 71.484 (68.158)\n",
            "Train: [8][130/196]\tBT 0.384 (0.387)\tDT 0.000 (0.028)\tloss 0.897 (0.916)\tAcc@1 70.312 (68.164)\n",
            "Train: [8][140/196]\tBT 0.382 (0.387)\tDT 0.000 (0.028)\tloss 0.864 (0.911)\tAcc@1 69.922 (68.351)\n",
            "Train: [8][150/196]\tBT 0.387 (0.387)\tDT 0.000 (0.028)\tloss 0.928 (0.910)\tAcc@1 66.406 (68.396)\n",
            "Train: [8][160/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 0.875 (0.912)\tAcc@1 69.141 (68.357)\n",
            "Train: [8][170/196]\tBT 0.384 (0.387)\tDT 0.000 (0.027)\tloss 0.918 (0.909)\tAcc@1 68.359 (68.470)\n",
            "Train: [8][180/196]\tBT 0.384 (0.386)\tDT 0.000 (0.027)\tloss 0.812 (0.907)\tAcc@1 69.141 (68.492)\n",
            "Train: [8][190/196]\tBT 0.382 (0.386)\tDT 0.000 (0.027)\tloss 1.022 (0.909)\tAcc@1 63.672 (68.458)\n",
            "epoch 8, total time 75.68\n",
            "Test: [0/40]\tTime 0.442 (0.442)\tLoss 0.9080 (0.9080)\tAcc@1 71.094 (71.094)\n",
            "Test: [10/40]\tTime 0.122 (0.152)\tLoss 0.9366 (0.9544)\tAcc@1 69.141 (68.714)\n",
            "Test: [20/40]\tTime 0.122 (0.138)\tLoss 1.1270 (0.9417)\tAcc@1 67.188 (69.661)\n",
            "Test: [30/40]\tTime 0.122 (0.133)\tLoss 0.8756 (0.9378)\tAcc@1 70.312 (69.871)\n",
            " * Acc@1 70.000\n",
            "Train: [9][10/196]\tBT 0.382 (0.430)\tDT 0.000 (0.072)\tloss 0.922 (0.900)\tAcc@1 69.922 (69.414)\n",
            "Train: [9][20/196]\tBT 0.383 (0.407)\tDT 0.000 (0.048)\tloss 0.842 (0.889)\tAcc@1 70.703 (69.219)\n",
            "Train: [9][30/196]\tBT 0.384 (0.399)\tDT 0.000 (0.040)\tloss 0.972 (0.889)\tAcc@1 63.672 (68.815)\n",
            "Train: [9][40/196]\tBT 0.383 (0.395)\tDT 0.000 (0.036)\tloss 0.950 (0.889)\tAcc@1 65.625 (68.799)\n",
            "Train: [9][50/196]\tBT 0.384 (0.393)\tDT 0.000 (0.034)\tloss 1.051 (0.887)\tAcc@1 67.969 (69.047)\n",
            "Train: [9][60/196]\tBT 0.383 (0.391)\tDT 0.000 (0.032)\tloss 0.762 (0.880)\tAcc@1 73.828 (69.160)\n",
            "Train: [9][70/196]\tBT 0.382 (0.390)\tDT 0.000 (0.031)\tloss 0.952 (0.874)\tAcc@1 68.359 (69.330)\n",
            "Train: [9][80/196]\tBT 0.382 (0.389)\tDT 0.000 (0.030)\tloss 0.874 (0.872)\tAcc@1 69.531 (69.453)\n",
            "Train: [9][90/196]\tBT 0.382 (0.389)\tDT 0.000 (0.030)\tloss 0.752 (0.868)\tAcc@1 72.656 (69.544)\n",
            "Train: [9][100/196]\tBT 0.385 (0.388)\tDT 0.000 (0.029)\tloss 0.823 (0.865)\tAcc@1 71.484 (69.605)\n",
            "Train: [9][110/196]\tBT 0.382 (0.388)\tDT 0.000 (0.029)\tloss 0.875 (0.867)\tAcc@1 69.922 (69.592)\n",
            "Train: [9][120/196]\tBT 0.382 (0.387)\tDT 0.000 (0.028)\tloss 0.945 (0.868)\tAcc@1 67.578 (69.609)\n",
            "Train: [9][130/196]\tBT 0.382 (0.387)\tDT 0.000 (0.028)\tloss 0.822 (0.868)\tAcc@1 71.094 (69.627)\n",
            "Train: [9][140/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 0.786 (0.867)\tAcc@1 73.438 (69.741)\n",
            "Train: [9][150/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 0.833 (0.865)\tAcc@1 70.703 (69.805)\n",
            "Train: [9][160/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 0.852 (0.862)\tAcc@1 69.531 (69.963)\n",
            "Train: [9][170/196]\tBT 0.384 (0.386)\tDT 0.000 (0.027)\tloss 0.913 (0.862)\tAcc@1 69.141 (69.993)\n",
            "Train: [9][180/196]\tBT 0.384 (0.386)\tDT 0.000 (0.027)\tloss 0.777 (0.859)\tAcc@1 72.266 (70.076)\n",
            "Train: [9][190/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 0.854 (0.856)\tAcc@1 67.969 (70.115)\n",
            "epoch 9, total time 75.63\n",
            "Test: [0/40]\tTime 0.430 (0.430)\tLoss 0.9481 (0.9481)\tAcc@1 70.312 (70.312)\n",
            "Test: [10/40]\tTime 0.121 (0.150)\tLoss 0.8964 (0.9388)\tAcc@1 71.094 (69.638)\n",
            "Test: [20/40]\tTime 0.122 (0.137)\tLoss 0.9660 (0.9392)\tAcc@1 69.141 (69.531)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 0.9803 (0.9430)\tAcc@1 67.188 (69.166)\n",
            " * Acc@1 69.210\n",
            "Train: [10][10/196]\tBT 0.383 (0.439)\tDT 0.000 (0.081)\tloss 0.823 (0.825)\tAcc@1 71.875 (71.172)\n",
            "Train: [10][20/196]\tBT 0.383 (0.411)\tDT 0.000 (0.053)\tloss 0.786 (0.827)\tAcc@1 71.094 (71.113)\n",
            "Train: [10][30/196]\tBT 0.382 (0.402)\tDT 0.000 (0.043)\tloss 0.833 (0.828)\tAcc@1 69.531 (70.938)\n",
            "Train: [10][40/196]\tBT 0.388 (0.398)\tDT 0.000 (0.039)\tloss 0.892 (0.823)\tAcc@1 66.797 (71.035)\n",
            "Train: [10][50/196]\tBT 0.384 (0.395)\tDT 0.000 (0.036)\tloss 0.703 (0.825)\tAcc@1 75.781 (71.125)\n",
            "Train: [10][60/196]\tBT 0.384 (0.393)\tDT 0.000 (0.034)\tloss 0.713 (0.819)\tAcc@1 73.047 (71.296)\n",
            "Train: [10][70/196]\tBT 0.382 (0.391)\tDT 0.000 (0.032)\tloss 0.840 (0.820)\tAcc@1 73.828 (71.390)\n",
            "Train: [10][80/196]\tBT 0.378 (0.390)\tDT 0.000 (0.031)\tloss 0.704 (0.818)\tAcc@1 72.656 (71.499)\n",
            "Train: [10][90/196]\tBT 0.382 (0.390)\tDT 0.000 (0.031)\tloss 0.872 (0.816)\tAcc@1 69.531 (71.606)\n",
            "Train: [10][100/196]\tBT 0.383 (0.389)\tDT 0.000 (0.030)\tloss 0.844 (0.819)\tAcc@1 71.484 (71.547)\n",
            "Train: [10][110/196]\tBT 0.378 (0.389)\tDT 0.000 (0.030)\tloss 0.876 (0.821)\tAcc@1 71.875 (71.484)\n",
            "Train: [10][120/196]\tBT 0.382 (0.388)\tDT 0.000 (0.029)\tloss 0.967 (0.823)\tAcc@1 66.016 (71.449)\n",
            "Train: [10][130/196]\tBT 0.381 (0.388)\tDT 0.000 (0.029)\tloss 0.834 (0.821)\tAcc@1 69.141 (71.487)\n",
            "Train: [10][140/196]\tBT 0.382 (0.387)\tDT 0.000 (0.028)\tloss 0.823 (0.818)\tAcc@1 73.828 (71.604)\n",
            "Train: [10][150/196]\tBT 0.384 (0.387)\tDT 0.000 (0.028)\tloss 0.710 (0.815)\tAcc@1 73.047 (71.693)\n",
            "Train: [10][160/196]\tBT 0.384 (0.387)\tDT 0.000 (0.028)\tloss 0.768 (0.814)\tAcc@1 74.609 (71.719)\n",
            "Train: [10][170/196]\tBT 0.382 (0.387)\tDT 0.000 (0.028)\tloss 0.859 (0.811)\tAcc@1 72.266 (71.815)\n",
            "Train: [10][180/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 0.767 (0.810)\tAcc@1 74.609 (71.888)\n",
            "Train: [10][190/196]\tBT 0.379 (0.386)\tDT 0.000 (0.027)\tloss 0.752 (0.807)\tAcc@1 71.484 (71.972)\n",
            "epoch 10, total time 75.73\n",
            "Test: [0/40]\tTime 0.410 (0.410)\tLoss 0.7400 (0.7400)\tAcc@1 75.391 (75.391)\n",
            "Test: [10/40]\tTime 0.122 (0.149)\tLoss 0.7698 (0.7487)\tAcc@1 73.047 (74.503)\n",
            "Test: [20/40]\tTime 0.121 (0.136)\tLoss 0.8570 (0.7373)\tAcc@1 70.703 (75.205)\n",
            "Test: [30/40]\tTime 0.123 (0.132)\tLoss 0.8157 (0.7352)\tAcc@1 73.438 (75.126)\n",
            " * Acc@1 75.400\n",
            "Train: [11][10/196]\tBT 0.381 (0.429)\tDT 0.000 (0.069)\tloss 0.763 (0.796)\tAcc@1 71.484 (72.148)\n",
            "Train: [11][20/196]\tBT 0.383 (0.406)\tDT 0.000 (0.047)\tloss 0.679 (0.773)\tAcc@1 77.734 (73.262)\n",
            "Train: [11][30/196]\tBT 0.383 (0.399)\tDT 0.000 (0.039)\tloss 0.763 (0.761)\tAcc@1 74.609 (73.607)\n",
            "Train: [11][40/196]\tBT 0.382 (0.395)\tDT 0.000 (0.036)\tloss 0.862 (0.776)\tAcc@1 71.094 (72.891)\n",
            "Train: [11][50/196]\tBT 0.382 (0.392)\tDT 0.000 (0.033)\tloss 0.752 (0.776)\tAcc@1 71.875 (72.859)\n",
            "Train: [11][60/196]\tBT 0.383 (0.391)\tDT 0.000 (0.032)\tloss 0.694 (0.769)\tAcc@1 74.219 (73.027)\n",
            "Train: [11][70/196]\tBT 0.384 (0.390)\tDT 0.000 (0.031)\tloss 0.831 (0.767)\tAcc@1 71.875 (73.225)\n",
            "Train: [11][80/196]\tBT 0.385 (0.389)\tDT 0.000 (0.030)\tloss 0.753 (0.768)\tAcc@1 75.781 (73.184)\n",
            "Train: [11][90/196]\tBT 0.384 (0.389)\tDT 0.000 (0.029)\tloss 0.872 (0.770)\tAcc@1 68.750 (73.125)\n",
            "Train: [11][100/196]\tBT 0.386 (0.388)\tDT 0.000 (0.029)\tloss 0.666 (0.769)\tAcc@1 79.297 (73.211)\n",
            "Train: [11][110/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 0.733 (0.772)\tAcc@1 73.828 (73.068)\n",
            "Train: [11][120/196]\tBT 0.381 (0.387)\tDT 0.000 (0.028)\tloss 0.809 (0.773)\tAcc@1 69.531 (73.089)\n",
            "Train: [11][130/196]\tBT 0.385 (0.387)\tDT 0.000 (0.028)\tloss 0.821 (0.776)\tAcc@1 72.266 (73.029)\n",
            "Train: [11][140/196]\tBT 0.385 (0.387)\tDT 0.000 (0.028)\tloss 0.947 (0.774)\tAcc@1 68.359 (73.125)\n",
            "Train: [11][150/196]\tBT 0.383 (0.386)\tDT 0.000 (0.028)\tloss 0.757 (0.775)\tAcc@1 73.828 (73.130)\n",
            "Train: [11][160/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 0.872 (0.778)\tAcc@1 68.750 (73.040)\n",
            "Train: [11][170/196]\tBT 0.384 (0.386)\tDT 0.000 (0.027)\tloss 0.627 (0.775)\tAcc@1 77.734 (73.143)\n",
            "Train: [11][180/196]\tBT 0.385 (0.386)\tDT 0.000 (0.027)\tloss 0.854 (0.781)\tAcc@1 70.312 (73.043)\n",
            "Train: [11][190/196]\tBT 0.384 (0.386)\tDT 0.000 (0.027)\tloss 0.788 (0.786)\tAcc@1 73.047 (72.862)\n",
            "epoch 11, total time 75.62\n",
            "Test: [0/40]\tTime 0.445 (0.445)\tLoss 0.7771 (0.7771)\tAcc@1 75.781 (75.781)\n",
            "Test: [10/40]\tTime 0.122 (0.152)\tLoss 0.7744 (0.7937)\tAcc@1 74.219 (74.361)\n",
            "Test: [20/40]\tTime 0.121 (0.138)\tLoss 0.7100 (0.7789)\tAcc@1 75.781 (74.721)\n",
            "Test: [30/40]\tTime 0.122 (0.133)\tLoss 0.8171 (0.7776)\tAcc@1 73.047 (74.546)\n",
            " * Acc@1 74.110\n",
            "Train: [12][10/196]\tBT 0.383 (0.423)\tDT 0.000 (0.063)\tloss 0.725 (0.798)\tAcc@1 74.219 (72.305)\n",
            "Train: [12][20/196]\tBT 0.384 (0.403)\tDT 0.000 (0.044)\tloss 0.886 (0.802)\tAcc@1 67.578 (72.090)\n",
            "Train: [12][30/196]\tBT 0.381 (0.396)\tDT 0.000 (0.037)\tloss 0.863 (0.807)\tAcc@1 68.750 (71.836)\n",
            "Train: [12][40/196]\tBT 0.384 (0.393)\tDT 0.000 (0.034)\tloss 0.734 (0.791)\tAcc@1 75.391 (72.354)\n",
            "Train: [12][50/196]\tBT 0.385 (0.391)\tDT 0.000 (0.032)\tloss 0.873 (0.781)\tAcc@1 70.703 (72.727)\n",
            "Train: [12][60/196]\tBT 0.382 (0.390)\tDT 0.000 (0.031)\tloss 0.751 (0.773)\tAcc@1 75.000 (73.034)\n",
            "Train: [12][70/196]\tBT 0.384 (0.389)\tDT 0.000 (0.030)\tloss 0.750 (0.778)\tAcc@1 70.312 (72.846)\n",
            "Train: [12][80/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 0.885 (0.773)\tAcc@1 66.797 (72.930)\n",
            "Train: [12][90/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 0.772 (0.769)\tAcc@1 75.391 (73.051)\n",
            "Train: [12][100/196]\tBT 0.382 (0.387)\tDT 0.000 (0.028)\tloss 0.803 (0.766)\tAcc@1 69.922 (73.152)\n",
            "Train: [12][110/196]\tBT 0.382 (0.387)\tDT 0.000 (0.028)\tloss 0.732 (0.762)\tAcc@1 74.219 (73.274)\n",
            "Train: [12][120/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 0.598 (0.758)\tAcc@1 80.469 (73.548)\n",
            "Train: [12][130/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 0.670 (0.754)\tAcc@1 78.125 (73.669)\n",
            "Train: [12][140/196]\tBT 0.384 (0.386)\tDT 0.000 (0.027)\tloss 0.741 (0.752)\tAcc@1 74.219 (73.770)\n",
            "Train: [12][150/196]\tBT 0.382 (0.386)\tDT 0.000 (0.027)\tloss 0.731 (0.752)\tAcc@1 72.656 (73.703)\n",
            "Train: [12][160/196]\tBT 0.385 (0.386)\tDT 0.000 (0.027)\tloss 0.780 (0.753)\tAcc@1 75.391 (73.762)\n",
            "Train: [12][170/196]\tBT 0.391 (0.386)\tDT 0.000 (0.027)\tloss 0.659 (0.751)\tAcc@1 76.562 (73.782)\n",
            "Train: [12][180/196]\tBT 0.384 (0.385)\tDT 0.000 (0.027)\tloss 0.815 (0.750)\tAcc@1 69.922 (73.830)\n",
            "Train: [12][190/196]\tBT 0.382 (0.385)\tDT 0.000 (0.026)\tloss 0.847 (0.749)\tAcc@1 75.391 (73.921)\n",
            "epoch 12, total time 75.52\n",
            "Test: [0/40]\tTime 0.464 (0.464)\tLoss 0.6918 (0.6918)\tAcc@1 76.562 (76.562)\n",
            "Test: [10/40]\tTime 0.122 (0.154)\tLoss 0.6762 (0.6645)\tAcc@1 78.516 (77.557)\n",
            "Test: [20/40]\tTime 0.122 (0.139)\tLoss 0.6434 (0.6464)\tAcc@1 75.000 (77.939)\n",
            "Test: [30/40]\tTime 0.122 (0.133)\tLoss 0.6482 (0.6407)\tAcc@1 77.344 (77.999)\n",
            " * Acc@1 77.880\n",
            "Train: [13][10/196]\tBT 0.383 (0.431)\tDT 0.000 (0.072)\tloss 0.738 (0.716)\tAcc@1 72.266 (75.156)\n",
            "Train: [13][20/196]\tBT 0.387 (0.407)\tDT 0.000 (0.048)\tloss 0.684 (0.720)\tAcc@1 73.047 (74.785)\n",
            "Train: [13][30/196]\tBT 0.381 (0.399)\tDT 0.000 (0.040)\tloss 0.739 (0.716)\tAcc@1 71.484 (74.844)\n",
            "Train: [13][40/196]\tBT 0.383 (0.395)\tDT 0.000 (0.036)\tloss 0.686 (0.718)\tAcc@1 76.172 (74.727)\n",
            "Train: [13][50/196]\tBT 0.382 (0.392)\tDT 0.000 (0.034)\tloss 0.774 (0.720)\tAcc@1 75.391 (74.773)\n",
            "Train: [13][60/196]\tBT 0.382 (0.391)\tDT 0.000 (0.032)\tloss 0.670 (0.714)\tAcc@1 77.734 (75.130)\n",
            "Train: [13][70/196]\tBT 0.381 (0.390)\tDT 0.000 (0.031)\tloss 0.761 (0.707)\tAcc@1 71.484 (75.335)\n",
            "Train: [13][80/196]\tBT 0.383 (0.389)\tDT 0.000 (0.030)\tloss 0.667 (0.701)\tAcc@1 76.172 (75.498)\n",
            "Train: [13][90/196]\tBT 0.381 (0.388)\tDT 0.000 (0.030)\tloss 0.753 (0.699)\tAcc@1 76.172 (75.621)\n",
            "Train: [13][100/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 0.659 (0.696)\tAcc@1 74.609 (75.789)\n",
            "Train: [13][110/196]\tBT 0.381 (0.387)\tDT 0.000 (0.029)\tloss 0.624 (0.691)\tAcc@1 81.250 (76.023)\n",
            "Train: [13][120/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 0.780 (0.691)\tAcc@1 70.703 (76.084)\n",
            "Train: [13][130/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 0.678 (0.695)\tAcc@1 76.172 (75.983)\n",
            "Train: [13][140/196]\tBT 0.382 (0.386)\tDT 0.000 (0.028)\tloss 0.729 (0.697)\tAcc@1 75.391 (75.926)\n",
            "Train: [13][150/196]\tBT 0.382 (0.386)\tDT 0.000 (0.028)\tloss 0.678 (0.697)\tAcc@1 76.953 (75.893)\n",
            "Train: [13][160/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 0.659 (0.698)\tAcc@1 77.734 (75.916)\n",
            "Train: [13][170/196]\tBT 0.382 (0.386)\tDT 0.000 (0.027)\tloss 0.682 (0.699)\tAcc@1 78.516 (75.938)\n",
            "Train: [13][180/196]\tBT 0.381 (0.386)\tDT 0.000 (0.027)\tloss 0.656 (0.699)\tAcc@1 76.953 (75.927)\n",
            "Train: [13][190/196]\tBT 0.382 (0.385)\tDT 0.000 (0.027)\tloss 0.721 (0.698)\tAcc@1 76.172 (75.989)\n",
            "epoch 13, total time 75.53\n",
            "Test: [0/40]\tTime 0.435 (0.435)\tLoss 0.6362 (0.6362)\tAcc@1 77.344 (77.344)\n",
            "Test: [10/40]\tTime 0.122 (0.151)\tLoss 0.6335 (0.6985)\tAcc@1 81.250 (77.592)\n",
            "Test: [20/40]\tTime 0.122 (0.137)\tLoss 0.6111 (0.6798)\tAcc@1 81.250 (77.586)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 0.6830 (0.6717)\tAcc@1 75.781 (77.596)\n",
            " * Acc@1 77.670\n",
            "Train: [14][10/196]\tBT 0.383 (0.429)\tDT 0.000 (0.067)\tloss 0.881 (0.908)\tAcc@1 69.141 (69.727)\n",
            "Train: [14][20/196]\tBT 0.384 (0.406)\tDT 0.000 (0.046)\tloss 0.926 (0.940)\tAcc@1 67.188 (67.539)\n",
            "Train: [14][30/196]\tBT 0.391 (0.399)\tDT 0.000 (0.039)\tloss 0.943 (0.939)\tAcc@1 64.062 (67.318)\n",
            "Train: [14][40/196]\tBT 0.384 (0.395)\tDT 0.000 (0.035)\tloss 0.737 (0.909)\tAcc@1 72.656 (68.223)\n",
            "Train: [14][50/196]\tBT 0.382 (0.393)\tDT 0.000 (0.033)\tloss 0.757 (0.897)\tAcc@1 72.266 (68.789)\n",
            "Train: [14][60/196]\tBT 0.382 (0.391)\tDT 0.000 (0.032)\tloss 0.886 (0.884)\tAcc@1 69.922 (69.251)\n",
            "Train: [14][70/196]\tBT 0.380 (0.390)\tDT 0.000 (0.031)\tloss 0.613 (0.872)\tAcc@1 79.297 (69.654)\n",
            "Train: [14][80/196]\tBT 0.384 (0.389)\tDT 0.000 (0.030)\tloss 0.895 (0.863)\tAcc@1 67.969 (70.063)\n",
            "Train: [14][90/196]\tBT 0.382 (0.389)\tDT 0.000 (0.029)\tloss 0.787 (0.852)\tAcc@1 73.438 (70.495)\n",
            "Train: [14][100/196]\tBT 0.383 (0.388)\tDT 0.000 (0.029)\tloss 0.824 (0.841)\tAcc@1 73.047 (70.844)\n",
            "Train: [14][110/196]\tBT 0.381 (0.388)\tDT 0.000 (0.028)\tloss 0.818 (0.835)\tAcc@1 72.266 (71.062)\n",
            "Train: [14][120/196]\tBT 0.383 (0.388)\tDT 0.000 (0.028)\tloss 0.741 (0.826)\tAcc@1 73.828 (71.387)\n",
            "Train: [14][130/196]\tBT 0.383 (0.387)\tDT 0.000 (0.028)\tloss 0.618 (0.818)\tAcc@1 76.562 (71.553)\n",
            "Train: [14][140/196]\tBT 0.388 (0.387)\tDT 0.000 (0.027)\tloss 0.743 (0.809)\tAcc@1 76.953 (71.889)\n",
            "Train: [14][150/196]\tBT 0.383 (0.387)\tDT 0.000 (0.027)\tloss 0.759 (0.803)\tAcc@1 75.000 (72.159)\n",
            "Train: [14][160/196]\tBT 0.387 (0.387)\tDT 0.000 (0.027)\tloss 0.607 (0.796)\tAcc@1 77.344 (72.361)\n",
            "Train: [14][170/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 0.774 (0.791)\tAcc@1 71.094 (72.470)\n",
            "Train: [14][180/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 0.727 (0.787)\tAcc@1 77.344 (72.667)\n",
            "Train: [14][190/196]\tBT 0.384 (0.386)\tDT 0.000 (0.027)\tloss 0.768 (0.785)\tAcc@1 75.391 (72.841)\n",
            "epoch 14, total time 75.69\n",
            "Test: [0/40]\tTime 0.433 (0.433)\tLoss 0.5287 (0.5287)\tAcc@1 80.078 (80.078)\n",
            "Test: [10/40]\tTime 0.122 (0.151)\tLoss 0.6887 (0.6579)\tAcc@1 75.000 (76.740)\n",
            "Test: [20/40]\tTime 0.122 (0.137)\tLoss 0.5978 (0.6425)\tAcc@1 80.469 (77.418)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 0.6454 (0.6491)\tAcc@1 77.734 (77.344)\n",
            " * Acc@1 77.310\n",
            "Train: [15][10/196]\tBT 0.381 (0.422)\tDT 0.000 (0.063)\tloss 0.602 (0.705)\tAcc@1 78.516 (74.570)\n",
            "Train: [15][20/196]\tBT 0.383 (0.403)\tDT 0.000 (0.044)\tloss 0.740 (0.700)\tAcc@1 74.609 (75.176)\n",
            "Train: [15][30/196]\tBT 0.384 (0.396)\tDT 0.000 (0.037)\tloss 0.707 (0.703)\tAcc@1 75.781 (75.378)\n",
            "Train: [15][40/196]\tBT 0.382 (0.393)\tDT 0.000 (0.034)\tloss 0.691 (0.696)\tAcc@1 75.391 (75.518)\n",
            "Train: [15][50/196]\tBT 0.382 (0.391)\tDT 0.000 (0.032)\tloss 0.665 (0.692)\tAcc@1 77.344 (75.523)\n",
            "Train: [15][60/196]\tBT 0.380 (0.390)\tDT 0.000 (0.031)\tloss 0.752 (0.693)\tAcc@1 75.000 (75.638)\n",
            "Train: [15][70/196]\tBT 0.383 (0.389)\tDT 0.000 (0.030)\tloss 0.709 (0.694)\tAcc@1 71.094 (75.608)\n",
            "Train: [15][80/196]\tBT 0.384 (0.388)\tDT 0.000 (0.029)\tloss 0.686 (0.694)\tAcc@1 75.781 (75.503)\n",
            "Train: [15][90/196]\tBT 0.381 (0.388)\tDT 0.000 (0.029)\tloss 0.776 (0.689)\tAcc@1 73.438 (75.694)\n",
            "Train: [15][100/196]\tBT 0.384 (0.387)\tDT 0.000 (0.028)\tloss 0.718 (0.686)\tAcc@1 75.000 (75.852)\n",
            "Train: [15][110/196]\tBT 0.384 (0.387)\tDT 0.000 (0.028)\tloss 0.778 (0.686)\tAcc@1 70.312 (75.795)\n",
            "Train: [15][120/196]\tBT 0.384 (0.386)\tDT 0.000 (0.028)\tloss 0.638 (0.686)\tAcc@1 77.344 (75.856)\n",
            "Train: [15][130/196]\tBT 0.382 (0.386)\tDT 0.000 (0.027)\tloss 0.751 (0.686)\tAcc@1 73.828 (75.931)\n",
            "Train: [15][140/196]\tBT 0.383 (0.386)\tDT 0.000 (0.027)\tloss 0.665 (0.684)\tAcc@1 77.344 (76.083)\n",
            "Train: [15][150/196]\tBT 0.384 (0.386)\tDT 0.000 (0.027)\tloss 0.645 (0.682)\tAcc@1 75.391 (76.188)\n",
            "Train: [15][160/196]\tBT 0.382 (0.386)\tDT 0.000 (0.027)\tloss 0.601 (0.679)\tAcc@1 79.297 (76.326)\n",
            "Train: [15][170/196]\tBT 0.382 (0.386)\tDT 0.000 (0.027)\tloss 0.552 (0.677)\tAcc@1 80.078 (76.434)\n",
            "Train: [15][180/196]\tBT 0.383 (0.385)\tDT 0.000 (0.027)\tloss 0.705 (0.677)\tAcc@1 73.438 (76.430)\n",
            "Train: [15][190/196]\tBT 0.382 (0.385)\tDT 0.000 (0.026)\tloss 0.646 (0.677)\tAcc@1 78.125 (76.423)\n",
            "epoch 15, total time 75.51\n",
            "Test: [0/40]\tTime 0.408 (0.408)\tLoss 0.7841 (0.7841)\tAcc@1 75.391 (75.391)\n",
            "Test: [10/40]\tTime 0.122 (0.148)\tLoss 0.7895 (0.7328)\tAcc@1 74.219 (77.237)\n",
            "Test: [20/40]\tTime 0.122 (0.136)\tLoss 0.7771 (0.7251)\tAcc@1 76.953 (77.455)\n",
            "Test: [30/40]\tTime 0.122 (0.132)\tLoss 0.8132 (0.7191)\tAcc@1 76.172 (77.419)\n",
            " * Acc@1 77.470\n",
            "==> Saving...\n",
            "best accuracy: 77.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised Contrastive Learning on CIFAR10"
      ],
      "metadata": {
        "id": "Aff3u6SwMMZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tensorboard_logger as tb_logger\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "\n",
        "try:\n",
        "    import apex\n",
        "    from apex import amp, optimizers\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# Hardcoded configurations\n",
        "class Configuration1:\n",
        "  print_freq = 10\n",
        "  save_freq = 50\n",
        "  batch_size = 256\n",
        "  num_workers = 16\n",
        "  epochs = 15\n",
        "  learning_rate = 0.05\n",
        "  lr_decay_epochs = [700,800,900]\n",
        "  lr_decay_rate = 0.1\n",
        "  weight_decay = 1e-4\n",
        "  momentum = 0.9\n",
        "  model_name = 'resnet50'\n",
        "  dataset = 'cifar10'  # options: ['cifar10', 'cifar100', 'path']\n",
        "  mean = None\n",
        "  std = None\n",
        "  data_folder = './datasets/'\n",
        "  size = 32\n",
        "  method = 'SupCon'  # options: ['SupCon', 'SimCLR']\n",
        "  temp = 0.07\n",
        "  cosine = False\n",
        "  syncBN = False\n",
        "  warm = False\n",
        "  trial = '0'\n",
        "  model_path = './save/SupCon/cifar10_models'\n",
        "  tb_path = './save/SupCon/cifar10_tensorboard'\n",
        "\n",
        "optimize1 = Configuration()"
      ],
      "metadata": {
        "id": "rvwmecss6fBp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Derived configurations\n",
        "#if optimize.dataset == 'path':\n",
        "    #assert optimize.data_folder is not None and optimize.mean is not None and optimize.std is not None\n",
        "\n",
        "#if optimize.data_folder is None:\n",
        "    #data_folder = './datasets/'\n",
        "#optimize.model_path = './save/SupCon/{}_models'.format(optimize.dataset)\n",
        "#optimize.tb_path = './save/SupCon/{}_tensorboard'.format(optimize.dataset)\n",
        "\n",
        "#iterations = optimize.lr_decay_epochs.split(',')\n",
        "#lr_decay_epochs = list(map(int, iterations))\n",
        "\n",
        "model_name = '{}_{}_{}_lr_{}_decay_{}_bsz_{}_temp_{}_trial_{}'.format(\n",
        "    optimize1.method, optimize1.dataset, optimize1.model_name, optimize1.learning_rate,\n",
        "    optimize1.weight_decay, optimize1.batch_size, optimize1.temp, optimize1.trial)\n",
        "\n",
        "if optimize1.cosine:\n",
        "    model_name += '_cosine'\n",
        "\n",
        "if optimize1.batch_size > 256:\n",
        "    optimize1.warm = True\n",
        "if optimize1.warm:\n",
        "    model_name += '_warm'\n",
        "    warmup_from = 0.01\n",
        "    warm_epochs = 10\n",
        "    if optimize1.cosine:\n",
        "        eta_min = optimize1.learning_rate * (optimize1.lr_decay_rate ** 3)\n",
        "        warmup_to = eta_min + (optimize1.learning_rate - eta_min) * (\n",
        "                1 + math.cos(math.pi * warm_epochs / optimize1.epochs)) / 2\n",
        "    else:\n",
        "        warmup_to = optimize1.learning_rate\n",
        "\n",
        "tb_folder = os.path.join(optimize1.tb_path, model_name)\n",
        "if not os.path.isdir(tb_folder):\n",
        "    os.makedirs(tb_folder)\n",
        "\n",
        "save_folder = os.path.join(optimize1.model_path, model_name)\n",
        "if not os.path.isdir(save_folder):\n",
        "    os.makedirs(save_folder)\n",
        "\n",
        "def set_loader():\n",
        "    # construct data loader\n",
        "    if optimize1.dataset == 'cifar10':\n",
        "        mean = (0.4914, 0.4822, 0.4465)\n",
        "        std = (0.2023, 0.1994, 0.2010)\n",
        "    elif optimize1.dataset == 'cifar100':\n",
        "        mean = (0.5071, 0.4867, 0.4408)\n",
        "        std = (0.2675, 0.2565, 0.2761)\n",
        "    elif optimize1.dataset == 'path':\n",
        "        mean = eval(mean)\n",
        "        std = eval(std)\n",
        "    else:\n",
        "        raise ValueError('dataset not supported: {}'.format(optimize1.dataset))\n",
        "    normalize = transforms.Normalize(mean=mean, std=std)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=optimize1.size, scale=(0.2, 1.)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
        "        ], p=0.8),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    if optimize1.dataset == 'cifar10':\n",
        "        train_dataset = datasets.CIFAR10(root=optimize1.data_folder,\n",
        "                                         transform=TwoCropTransform(train_transform),\n",
        "                                         download=True)\n",
        "    elif optimize1.dataset == 'cifar100':\n",
        "        train_dataset = datasets.CIFAR100(root=optimize1.data_folder,\n",
        "                                          transform=TwoCropTransform(train_transform),\n",
        "                                          download=True)\n",
        "    elif optimize1.dataset == 'path':\n",
        "        train_dataset = datasets.ImageFolder(root=optimize1.data_folder,\n",
        "                                             transform=TwoCropTransform(train_transform))\n",
        "    else:\n",
        "        raise ValueError(optimize1.dataset)\n",
        "\n",
        "    train_sampler = None\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=optimize1.batch_size, shuffle=(train_sampler is None),\n",
        "        num_workers=optimize1.num_workers, pin_memory=True, sampler=train_sampler)\n",
        "\n",
        "    return train_loader\n",
        "\n",
        "def set_model():\n",
        "    model = SupConResNet(name='resnet50')\n",
        "    criterion = SupConLoss(temperature=optimize1.temp)\n",
        "\n",
        "    # enable synchronized Batch Normalization\n",
        "    if optimize1.syncBN:\n",
        "        model = apex.parallel.convert_syncbn_model(model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            model.encoder = torch.nn.DataParallel(model.encoder)\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    return model, criterion\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \"\"\"one epoch training\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    end = time.time()\n",
        "    for idx, (images, labels) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        images = torch.cat([images[0], images[1]], dim=0)\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda(non_blocking=True)\n",
        "            labels = labels.cuda(non_blocking=True)\n",
        "        bsz = labels.shape[0]\n",
        "\n",
        "        # warm-up learning rate\n",
        "        warmup_learning_rate(optimize1, epoch, idx, len(train_loader), optimizer)\n",
        "\n",
        "        # compute loss\n",
        "        features = model(images)\n",
        "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
        "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
        "        if optimize1.method == 'SupCon':\n",
        "            loss = criterion(features, labels)\n",
        "        elif optimize1.method == 'SimCLR':\n",
        "            loss = criterion(features)\n",
        "        else:\n",
        "            raise ValueError('contrastive method not supported: {}'.\n",
        "                             format(optimize1.method))\n",
        "\n",
        "        # update metric\n",
        "        losses.update(loss.item(), bsz)\n",
        "\n",
        "        # SGD\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # print info\n",
        "        if (idx + 1) % optimize1.print_freq == 0:\n",
        "            print('Train: [{0}][{1}/{2}]\\t'\n",
        "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'loss {loss.val:.3f} ({loss.avg:.3f})'.format(\n",
        "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "def main():\n",
        "    # build data loader\n",
        "    train_loader = set_loader()\n",
        "\n",
        "    # build model and criterion\n",
        "    model, criterion = set_model()\n",
        "\n",
        "    # build optimizer\n",
        "    optimizer = set_optimizer(optimize1, model)\n",
        "\n",
        "    # tensorboard\n",
        "    logger = tb_logger.Logger(logdir=tb_folder, flush_secs=2)\n",
        "\n",
        "    # training routine\n",
        "    for epoch in range(1, optimize1.epochs + 1):\n",
        "        adjust_learning_rate(optimize1, optimizer, epoch)\n",
        "\n",
        "        # train for one epoch\n",
        "        time1 = time.time()\n",
        "        loss = train(train_loader, model, criterion, optimizer, epoch)\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        # tensorboard logger\n",
        "        logger.log_value('loss', loss, epoch)\n",
        "        logger.log_value('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
        "\n",
        "        if epoch % optimize1.save_freq == 0:\n",
        "            save_file = os.path.join(\n",
        "                save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            save_model(model, optimizer, optimize1, epoch, save_file)\n",
        "\n",
        "    # save the last model\n",
        "    save_file = os.path.join(\n",
        "        save_folder, 'last.pth')\n",
        "    save_model(model, optimizer, optimize1, optimize1.epochs, save_file)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyl5gZFt6ruC",
        "outputId": "e9e2292c-c086-4e71-da03-654c745f2327"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Train: [1][10/196]\tBT 0.789 (0.868)\tDT 0.000 (0.130)\tloss 6.232 (6.235)\n",
            "Train: [1][20/196]\tBT 0.789 (0.828)\tDT 0.000 (0.065)\tloss 6.223 (6.232)\n",
            "Train: [1][30/196]\tBT 0.788 (0.816)\tDT 0.000 (0.044)\tloss 6.230 (6.229)\n",
            "Train: [1][40/196]\tBT 0.799 (0.811)\tDT 0.000 (0.033)\tloss 6.209 (6.224)\n",
            "Train: [1][50/196]\tBT 0.801 (0.808)\tDT 0.000 (0.026)\tloss 6.231 (6.221)\n",
            "Train: [1][60/196]\tBT 0.794 (0.806)\tDT 0.000 (0.022)\tloss 6.189 (6.217)\n",
            "Train: [1][70/196]\tBT 0.789 (0.804)\tDT 0.000 (0.019)\tloss 6.186 (6.215)\n",
            "Train: [1][80/196]\tBT 0.792 (0.803)\tDT 0.000 (0.017)\tloss 6.185 (6.211)\n",
            "Train: [1][90/196]\tBT 0.790 (0.801)\tDT 0.000 (0.015)\tloss 6.216 (6.208)\n",
            "Train: [1][100/196]\tBT 0.791 (0.800)\tDT 0.000 (0.013)\tloss 6.165 (6.205)\n",
            "Train: [1][110/196]\tBT 0.784 (0.799)\tDT 0.000 (0.012)\tloss 6.192 (6.203)\n",
            "Train: [1][120/196]\tBT 0.784 (0.798)\tDT 0.000 (0.011)\tloss 6.084 (6.199)\n",
            "Train: [1][130/196]\tBT 0.788 (0.797)\tDT 0.000 (0.010)\tloss 6.179 (6.198)\n",
            "Train: [1][140/196]\tBT 0.789 (0.797)\tDT 0.000 (0.010)\tloss 6.167 (6.196)\n",
            "Train: [1][150/196]\tBT 0.784 (0.796)\tDT 0.000 (0.009)\tloss 6.184 (6.193)\n",
            "Train: [1][160/196]\tBT 0.792 (0.796)\tDT 0.000 (0.009)\tloss 6.156 (6.191)\n",
            "Train: [1][170/196]\tBT 0.786 (0.796)\tDT 0.000 (0.008)\tloss 6.170 (6.191)\n",
            "Train: [1][180/196]\tBT 0.793 (0.795)\tDT 0.000 (0.008)\tloss 6.134 (6.189)\n",
            "Train: [1][190/196]\tBT 0.788 (0.795)\tDT 0.000 (0.007)\tloss 6.199 (6.188)\n",
            "epoch 1, total time 155.80\n",
            "Train: [2][10/196]\tBT 0.788 (0.910)\tDT 0.000 (0.169)\tloss 6.166 (6.166)\n",
            "Train: [2][20/196]\tBT 0.792 (0.851)\tDT 0.000 (0.085)\tloss 6.166 (6.171)\n",
            "Train: [2][30/196]\tBT 0.793 (0.831)\tDT 0.000 (0.057)\tloss 6.135 (6.167)\n",
            "Train: [2][40/196]\tBT 0.795 (0.821)\tDT 0.000 (0.043)\tloss 6.170 (6.163)\n",
            "Train: [2][50/196]\tBT 0.794 (0.815)\tDT 0.000 (0.034)\tloss 6.168 (6.157)\n",
            "Train: [2][60/196]\tBT 0.784 (0.811)\tDT 0.000 (0.029)\tloss 6.127 (6.153)\n",
            "Train: [2][70/196]\tBT 0.792 (0.808)\tDT 0.000 (0.025)\tloss 6.146 (6.150)\n",
            "Train: [2][80/196]\tBT 0.793 (0.806)\tDT 0.000 (0.022)\tloss 6.117 (6.145)\n",
            "Train: [2][90/196]\tBT 0.794 (0.804)\tDT 0.000 (0.019)\tloss 6.147 (6.144)\n",
            "Train: [2][100/196]\tBT 0.788 (0.803)\tDT 0.000 (0.017)\tloss 6.097 (6.142)\n",
            "Train: [2][110/196]\tBT 0.793 (0.802)\tDT 0.000 (0.016)\tloss 6.119 (6.139)\n",
            "Train: [2][120/196]\tBT 0.796 (0.801)\tDT 0.000 (0.015)\tloss 6.151 (6.136)\n",
            "Train: [2][130/196]\tBT 0.797 (0.800)\tDT 0.000 (0.013)\tloss 6.110 (6.133)\n",
            "Train: [2][140/196]\tBT 0.792 (0.800)\tDT 0.000 (0.012)\tloss 6.085 (6.131)\n",
            "Train: [2][150/196]\tBT 0.795 (0.799)\tDT 0.000 (0.012)\tloss 6.086 (6.129)\n",
            "Train: [2][160/196]\tBT 0.796 (0.799)\tDT 0.000 (0.011)\tloss 6.080 (6.126)\n",
            "Train: [2][170/196]\tBT 0.797 (0.798)\tDT 0.000 (0.010)\tloss 6.049 (6.124)\n",
            "Train: [2][180/196]\tBT 0.795 (0.798)\tDT 0.000 (0.010)\tloss 6.076 (6.121)\n",
            "Train: [2][190/196]\tBT 0.793 (0.798)\tDT 0.000 (0.009)\tloss 6.064 (6.118)\n",
            "epoch 2, total time 156.22\n",
            "Train: [3][10/196]\tBT 0.797 (0.892)\tDT 0.000 (0.149)\tloss 6.043 (6.079)\n",
            "Train: [3][20/196]\tBT 0.793 (0.842)\tDT 0.000 (0.075)\tloss 6.064 (6.074)\n",
            "Train: [3][30/196]\tBT 0.795 (0.825)\tDT 0.000 (0.050)\tloss 6.055 (6.073)\n",
            "Train: [3][40/196]\tBT 0.793 (0.817)\tDT 0.000 (0.038)\tloss 6.022 (6.070)\n",
            "Train: [3][50/196]\tBT 0.787 (0.812)\tDT 0.000 (0.030)\tloss 6.069 (6.067)\n",
            "Train: [3][60/196]\tBT 0.789 (0.809)\tDT 0.000 (0.025)\tloss 6.029 (6.067)\n",
            "Train: [3][70/196]\tBT 0.795 (0.806)\tDT 0.000 (0.022)\tloss 6.055 (6.068)\n",
            "Train: [3][80/196]\tBT 0.793 (0.805)\tDT 0.000 (0.019)\tloss 5.957 (6.065)\n",
            "Train: [3][90/196]\tBT 0.790 (0.803)\tDT 0.000 (0.017)\tloss 6.031 (6.064)\n",
            "Train: [3][100/196]\tBT 0.786 (0.802)\tDT 0.000 (0.015)\tloss 6.095 (6.063)\n",
            "Train: [3][110/196]\tBT 0.791 (0.801)\tDT 0.000 (0.014)\tloss 6.017 (6.060)\n",
            "Train: [3][120/196]\tBT 0.791 (0.800)\tDT 0.000 (0.013)\tloss 6.109 (6.058)\n",
            "Train: [3][130/196]\tBT 0.786 (0.800)\tDT 0.000 (0.012)\tloss 6.006 (6.056)\n",
            "Train: [3][140/196]\tBT 0.789 (0.799)\tDT 0.000 (0.011)\tloss 6.015 (6.055)\n",
            "Train: [3][150/196]\tBT 0.788 (0.799)\tDT 0.000 (0.010)\tloss 6.004 (6.053)\n",
            "Train: [3][160/196]\tBT 0.787 (0.798)\tDT 0.000 (0.010)\tloss 6.005 (6.050)\n",
            "Train: [3][170/196]\tBT 0.791 (0.798)\tDT 0.000 (0.009)\tloss 6.072 (6.047)\n",
            "Train: [3][180/196]\tBT 0.796 (0.798)\tDT 0.000 (0.009)\tloss 6.020 (6.046)\n",
            "Train: [3][190/196]\tBT 0.795 (0.797)\tDT 0.000 (0.008)\tloss 5.959 (6.044)\n",
            "epoch 3, total time 156.14\n",
            "Train: [4][10/196]\tBT 0.789 (0.900)\tDT 0.000 (0.161)\tloss 6.063 (6.028)\n",
            "Train: [4][20/196]\tBT 0.790 (0.845)\tDT 0.000 (0.081)\tloss 6.027 (6.014)\n",
            "Train: [4][30/196]\tBT 0.791 (0.827)\tDT 0.000 (0.054)\tloss 5.979 (5.995)\n",
            "Train: [4][40/196]\tBT 0.794 (0.818)\tDT 0.000 (0.041)\tloss 5.939 (5.998)\n",
            "Train: [4][50/196]\tBT 0.792 (0.813)\tDT 0.000 (0.033)\tloss 5.921 (5.991)\n",
            "Train: [4][60/196]\tBT 0.795 (0.810)\tDT 0.000 (0.027)\tloss 6.005 (5.989)\n",
            "Train: [4][70/196]\tBT 0.787 (0.807)\tDT 0.000 (0.023)\tloss 5.949 (5.988)\n",
            "Train: [4][80/196]\tBT 0.788 (0.805)\tDT 0.000 (0.021)\tloss 6.017 (5.988)\n",
            "Train: [4][90/196]\tBT 0.788 (0.804)\tDT 0.000 (0.018)\tloss 5.983 (5.987)\n",
            "Train: [4][100/196]\tBT 0.789 (0.803)\tDT 0.000 (0.016)\tloss 5.944 (5.983)\n",
            "Train: [4][110/196]\tBT 0.795 (0.802)\tDT 0.000 (0.015)\tloss 6.019 (5.982)\n",
            "Train: [4][120/196]\tBT 0.779 (0.801)\tDT 0.000 (0.014)\tloss 5.947 (5.978)\n",
            "Train: [4][130/196]\tBT 0.792 (0.800)\tDT 0.000 (0.013)\tloss 5.968 (5.977)\n",
            "Train: [4][140/196]\tBT 0.787 (0.799)\tDT 0.000 (0.012)\tloss 5.996 (5.976)\n",
            "Train: [4][150/196]\tBT 0.791 (0.799)\tDT 0.000 (0.011)\tloss 5.921 (5.974)\n",
            "Train: [4][160/196]\tBT 0.789 (0.798)\tDT 0.000 (0.010)\tloss 5.928 (5.971)\n",
            "Train: [4][170/196]\tBT 0.797 (0.798)\tDT 0.000 (0.010)\tloss 5.888 (5.968)\n",
            "Train: [4][180/196]\tBT 0.792 (0.798)\tDT 0.000 (0.009)\tloss 5.896 (5.965)\n",
            "Train: [4][190/196]\tBT 0.796 (0.798)\tDT 0.000 (0.009)\tloss 5.900 (5.963)\n",
            "epoch 4, total time 156.22\n",
            "Train: [5][10/196]\tBT 0.783 (0.935)\tDT 0.000 (0.194)\tloss 6.005 (5.919)\n",
            "Train: [5][20/196]\tBT 0.789 (0.863)\tDT 0.000 (0.097)\tloss 5.928 (5.911)\n",
            "Train: [5][30/196]\tBT 0.797 (0.840)\tDT 0.000 (0.065)\tloss 5.938 (5.917)\n",
            "Train: [5][40/196]\tBT 0.796 (0.828)\tDT 0.000 (0.049)\tloss 5.952 (5.920)\n",
            "Train: [5][50/196]\tBT 0.793 (0.821)\tDT 0.000 (0.039)\tloss 5.840 (5.916)\n",
            "Train: [5][60/196]\tBT 0.796 (0.816)\tDT 0.000 (0.033)\tloss 5.929 (5.910)\n",
            "Train: [5][70/196]\tBT 0.790 (0.813)\tDT 0.000 (0.028)\tloss 5.842 (5.905)\n",
            "Train: [5][80/196]\tBT 0.797 (0.811)\tDT 0.000 (0.025)\tloss 5.794 (5.900)\n",
            "Train: [5][90/196]\tBT 0.791 (0.808)\tDT 0.000 (0.022)\tloss 5.945 (5.900)\n",
            "Train: [5][100/196]\tBT 0.792 (0.807)\tDT 0.000 (0.020)\tloss 5.865 (5.898)\n",
            "Train: [5][110/196]\tBT 0.794 (0.805)\tDT 0.000 (0.018)\tloss 5.906 (5.897)\n",
            "Train: [5][120/196]\tBT 0.790 (0.804)\tDT 0.000 (0.017)\tloss 5.933 (5.894)\n",
            "Train: [5][130/196]\tBT 0.799 (0.803)\tDT 0.000 (0.015)\tloss 5.791 (5.892)\n",
            "Train: [5][140/196]\tBT 0.790 (0.803)\tDT 0.000 (0.014)\tloss 5.921 (5.890)\n",
            "Train: [5][150/196]\tBT 0.790 (0.802)\tDT 0.000 (0.013)\tloss 5.890 (5.886)\n",
            "Train: [5][160/196]\tBT 0.795 (0.801)\tDT 0.001 (0.013)\tloss 5.824 (5.883)\n",
            "Train: [5][170/196]\tBT 0.791 (0.801)\tDT 0.000 (0.012)\tloss 5.804 (5.880)\n",
            "Train: [5][180/196]\tBT 0.797 (0.800)\tDT 0.000 (0.011)\tloss 5.866 (5.875)\n",
            "Train: [5][190/196]\tBT 0.797 (0.800)\tDT 0.000 (0.011)\tloss 5.817 (5.872)\n",
            "epoch 5, total time 156.66\n",
            "Train: [6][10/196]\tBT 0.787 (0.936)\tDT 0.000 (0.197)\tloss 5.820 (5.812)\n",
            "Train: [6][20/196]\tBT 0.790 (0.863)\tDT 0.000 (0.099)\tloss 5.893 (5.821)\n",
            "Train: [6][30/196]\tBT 0.793 (0.839)\tDT 0.000 (0.066)\tloss 5.780 (5.807)\n",
            "Train: [6][40/196]\tBT 0.793 (0.827)\tDT 0.000 (0.050)\tloss 5.858 (5.795)\n",
            "Train: [6][50/196]\tBT 0.793 (0.820)\tDT 0.000 (0.040)\tloss 5.779 (5.788)\n",
            "Train: [6][60/196]\tBT 0.794 (0.816)\tDT 0.000 (0.033)\tloss 5.668 (5.788)\n",
            "Train: [6][70/196]\tBT 0.793 (0.813)\tDT 0.000 (0.028)\tloss 5.762 (5.783)\n",
            "Train: [6][80/196]\tBT 0.786 (0.810)\tDT 0.000 (0.025)\tloss 5.674 (5.781)\n",
            "Train: [6][90/196]\tBT 0.795 (0.808)\tDT 0.000 (0.022)\tloss 5.792 (5.778)\n",
            "Train: [6][100/196]\tBT 0.791 (0.807)\tDT 0.000 (0.020)\tloss 5.767 (5.775)\n",
            "Train: [6][110/196]\tBT 0.795 (0.805)\tDT 0.000 (0.018)\tloss 5.695 (5.771)\n",
            "Train: [6][120/196]\tBT 0.793 (0.804)\tDT 0.000 (0.017)\tloss 5.709 (5.767)\n",
            "Train: [6][130/196]\tBT 0.791 (0.803)\tDT 0.000 (0.016)\tloss 5.763 (5.764)\n",
            "Train: [6][140/196]\tBT 0.790 (0.802)\tDT 0.000 (0.014)\tloss 5.652 (5.760)\n",
            "Train: [6][150/196]\tBT 0.796 (0.802)\tDT 0.000 (0.014)\tloss 5.589 (5.755)\n",
            "Train: [6][160/196]\tBT 0.796 (0.801)\tDT 0.000 (0.013)\tloss 5.685 (5.750)\n",
            "Train: [6][170/196]\tBT 0.791 (0.801)\tDT 0.000 (0.012)\tloss 5.840 (5.747)\n",
            "Train: [6][180/196]\tBT 0.790 (0.800)\tDT 0.000 (0.011)\tloss 5.671 (5.745)\n",
            "Train: [6][190/196]\tBT 0.795 (0.800)\tDT 0.000 (0.011)\tloss 5.697 (5.744)\n",
            "epoch 6, total time 156.63\n",
            "Train: [7][10/196]\tBT 0.789 (0.879)\tDT 0.000 (0.137)\tloss 5.603 (5.672)\n",
            "Train: [7][20/196]\tBT 0.795 (0.836)\tDT 0.000 (0.069)\tloss 5.679 (5.674)\n",
            "Train: [7][30/196]\tBT 0.790 (0.821)\tDT 0.000 (0.046)\tloss 5.589 (5.675)\n",
            "Train: [7][40/196]\tBT 0.788 (0.814)\tDT 0.000 (0.035)\tloss 5.670 (5.663)\n",
            "Train: [7][50/196]\tBT 0.796 (0.810)\tDT 0.000 (0.028)\tloss 5.639 (5.664)\n",
            "Train: [7][60/196]\tBT 0.791 (0.807)\tDT 0.000 (0.023)\tloss 5.604 (5.661)\n",
            "Train: [7][70/196]\tBT 0.795 (0.805)\tDT 0.000 (0.020)\tloss 5.659 (5.659)\n",
            "Train: [7][80/196]\tBT 0.792 (0.804)\tDT 0.000 (0.017)\tloss 5.731 (5.655)\n",
            "Train: [7][90/196]\tBT 0.795 (0.802)\tDT 0.000 (0.016)\tloss 5.654 (5.648)\n",
            "Train: [7][100/196]\tBT 0.789 (0.801)\tDT 0.000 (0.014)\tloss 5.616 (5.648)\n",
            "Train: [7][110/196]\tBT 0.794 (0.801)\tDT 0.000 (0.013)\tloss 5.658 (5.649)\n",
            "Train: [7][120/196]\tBT 0.792 (0.800)\tDT 0.000 (0.012)\tloss 5.670 (5.651)\n",
            "Train: [7][130/196]\tBT 0.794 (0.799)\tDT 0.000 (0.011)\tloss 5.491 (5.649)\n",
            "Train: [7][140/196]\tBT 0.797 (0.799)\tDT 0.000 (0.010)\tloss 5.556 (5.647)\n",
            "Train: [7][150/196]\tBT 0.789 (0.798)\tDT 0.000 (0.010)\tloss 5.700 (5.645)\n",
            "Train: [7][160/196]\tBT 0.790 (0.798)\tDT 0.000 (0.009)\tloss 5.558 (5.642)\n",
            "Train: [7][170/196]\tBT 0.795 (0.798)\tDT 0.000 (0.008)\tloss 5.651 (5.639)\n",
            "Train: [7][180/196]\tBT 0.796 (0.798)\tDT 0.000 (0.008)\tloss 5.667 (5.636)\n",
            "Train: [7][190/196]\tBT 0.794 (0.797)\tDT 0.000 (0.008)\tloss 5.626 (5.635)\n",
            "epoch 7, total time 156.16\n",
            "Train: [8][10/196]\tBT 0.792 (0.877)\tDT 0.000 (0.134)\tloss 5.677 (5.613)\n",
            "Train: [8][20/196]\tBT 0.796 (0.835)\tDT 0.000 (0.067)\tloss 5.613 (5.608)\n",
            "Train: [8][30/196]\tBT 0.791 (0.820)\tDT 0.000 (0.045)\tloss 5.568 (5.589)\n",
            "Train: [8][40/196]\tBT 0.793 (0.814)\tDT 0.000 (0.034)\tloss 5.540 (5.584)\n",
            "Train: [8][50/196]\tBT 0.790 (0.809)\tDT 0.000 (0.027)\tloss 5.556 (5.576)\n",
            "Train: [8][60/196]\tBT 0.797 (0.807)\tDT 0.000 (0.023)\tloss 5.351 (5.570)\n",
            "Train: [8][70/196]\tBT 0.787 (0.805)\tDT 0.000 (0.020)\tloss 5.585 (5.570)\n",
            "Train: [8][80/196]\tBT 0.796 (0.803)\tDT 0.000 (0.017)\tloss 5.439 (5.567)\n",
            "Train: [8][90/196]\tBT 0.793 (0.802)\tDT 0.000 (0.015)\tloss 5.505 (5.567)\n",
            "Train: [8][100/196]\tBT 0.791 (0.801)\tDT 0.000 (0.014)\tloss 5.463 (5.566)\n",
            "Train: [8][110/196]\tBT 0.794 (0.800)\tDT 0.000 (0.013)\tloss 5.543 (5.564)\n",
            "Train: [8][120/196]\tBT 0.792 (0.800)\tDT 0.000 (0.012)\tloss 5.536 (5.562)\n",
            "Train: [8][130/196]\tBT 0.803 (0.799)\tDT 0.000 (0.011)\tloss 5.559 (5.559)\n",
            "Train: [8][140/196]\tBT 0.796 (0.799)\tDT 0.000 (0.010)\tloss 5.551 (5.559)\n",
            "Train: [8][150/196]\tBT 0.796 (0.798)\tDT 0.000 (0.009)\tloss 5.548 (5.554)\n",
            "Train: [8][160/196]\tBT 0.787 (0.798)\tDT 0.000 (0.009)\tloss 5.423 (5.551)\n",
            "Train: [8][170/196]\tBT 0.789 (0.798)\tDT 0.000 (0.008)\tloss 5.499 (5.549)\n",
            "Train: [8][180/196]\tBT 0.790 (0.797)\tDT 0.000 (0.008)\tloss 5.466 (5.545)\n",
            "Train: [8][190/196]\tBT 0.791 (0.797)\tDT 0.000 (0.007)\tloss 5.503 (5.542)\n",
            "epoch 8, total time 156.12\n",
            "Train: [9][10/196]\tBT 0.789 (0.887)\tDT 0.000 (0.144)\tloss 5.598 (5.509)\n",
            "Train: [9][20/196]\tBT 0.795 (0.840)\tDT 0.000 (0.072)\tloss 5.495 (5.500)\n",
            "Train: [9][30/196]\tBT 0.789 (0.824)\tDT 0.000 (0.048)\tloss 5.418 (5.486)\n",
            "Train: [9][40/196]\tBT 0.796 (0.816)\tDT 0.000 (0.036)\tloss 5.618 (5.503)\n",
            "Train: [9][50/196]\tBT 0.797 (0.812)\tDT 0.000 (0.029)\tloss 5.369 (5.499)\n",
            "Train: [9][60/196]\tBT 0.792 (0.809)\tDT 0.000 (0.024)\tloss 5.498 (5.488)\n",
            "Train: [9][70/196]\tBT 0.788 (0.806)\tDT 0.000 (0.021)\tloss 5.432 (5.482)\n",
            "Train: [9][80/196]\tBT 0.800 (0.805)\tDT 0.000 (0.018)\tloss 5.491 (5.479)\n",
            "Train: [9][90/196]\tBT 0.794 (0.803)\tDT 0.000 (0.016)\tloss 5.333 (5.474)\n",
            "Train: [9][100/196]\tBT 0.795 (0.802)\tDT 0.000 (0.015)\tloss 5.302 (5.473)\n",
            "Train: [9][110/196]\tBT 0.796 (0.801)\tDT 0.000 (0.014)\tloss 5.407 (5.473)\n",
            "Train: [9][120/196]\tBT 0.790 (0.800)\tDT 0.000 (0.012)\tloss 5.415 (5.470)\n",
            "Train: [9][130/196]\tBT 0.790 (0.800)\tDT 0.000 (0.011)\tloss 5.600 (5.467)\n",
            "Train: [9][140/196]\tBT 0.790 (0.799)\tDT 0.000 (0.011)\tloss 5.403 (5.464)\n",
            "Train: [9][150/196]\tBT 0.794 (0.799)\tDT 0.000 (0.010)\tloss 5.423 (5.465)\n",
            "Train: [9][160/196]\tBT 0.790 (0.798)\tDT 0.000 (0.009)\tloss 5.475 (5.464)\n",
            "Train: [9][170/196]\tBT 0.789 (0.798)\tDT 0.000 (0.009)\tloss 5.450 (5.460)\n",
            "Train: [9][180/196]\tBT 0.795 (0.798)\tDT 0.000 (0.008)\tloss 5.464 (5.455)\n",
            "Train: [9][190/196]\tBT 0.790 (0.798)\tDT 0.000 (0.008)\tloss 5.262 (5.451)\n",
            "epoch 9, total time 156.20\n",
            "Train: [10][10/196]\tBT 0.793 (0.923)\tDT 0.000 (0.181)\tloss 5.348 (5.475)\n",
            "Train: [10][20/196]\tBT 0.790 (0.858)\tDT 0.000 (0.091)\tloss 5.295 (5.437)\n",
            "Train: [10][30/196]\tBT 0.794 (0.836)\tDT 0.000 (0.061)\tloss 5.272 (5.409)\n",
            "Train: [10][40/196]\tBT 0.801 (0.826)\tDT 0.000 (0.046)\tloss 5.349 (5.390)\n",
            "Train: [10][50/196]\tBT 0.797 (0.819)\tDT 0.000 (0.037)\tloss 5.426 (5.395)\n",
            "Train: [10][60/196]\tBT 0.791 (0.815)\tDT 0.000 (0.031)\tloss 5.400 (5.391)\n",
            "Train: [10][70/196]\tBT 0.793 (0.812)\tDT 0.000 (0.026)\tloss 5.449 (5.391)\n",
            "Train: [10][80/196]\tBT 0.790 (0.809)\tDT 0.000 (0.023)\tloss 5.321 (5.389)\n",
            "Train: [10][90/196]\tBT 0.788 (0.807)\tDT 0.000 (0.021)\tloss 5.406 (5.384)\n",
            "Train: [10][100/196]\tBT 0.790 (0.806)\tDT 0.000 (0.019)\tloss 5.173 (5.383)\n",
            "Train: [10][110/196]\tBT 0.793 (0.805)\tDT 0.000 (0.017)\tloss 5.346 (5.377)\n",
            "Train: [10][120/196]\tBT 0.795 (0.803)\tDT 0.000 (0.016)\tloss 5.269 (5.376)\n",
            "Train: [10][130/196]\tBT 0.801 (0.803)\tDT 0.000 (0.014)\tloss 5.303 (5.372)\n",
            "Train: [10][140/196]\tBT 0.791 (0.802)\tDT 0.000 (0.013)\tloss 5.342 (5.370)\n",
            "Train: [10][150/196]\tBT 0.794 (0.801)\tDT 0.000 (0.013)\tloss 5.341 (5.369)\n",
            "Train: [10][160/196]\tBT 0.790 (0.801)\tDT 0.000 (0.012)\tloss 5.309 (5.366)\n",
            "Train: [10][170/196]\tBT 0.795 (0.800)\tDT 0.000 (0.011)\tloss 5.343 (5.362)\n",
            "Train: [10][180/196]\tBT 0.789 (0.800)\tDT 0.000 (0.010)\tloss 5.317 (5.358)\n",
            "Train: [10][190/196]\tBT 0.796 (0.799)\tDT 0.000 (0.010)\tloss 5.255 (5.351)\n",
            "epoch 10, total time 156.57\n",
            "Train: [11][10/196]\tBT 0.790 (0.910)\tDT 0.000 (0.166)\tloss 5.347 (5.331)\n",
            "Train: [11][20/196]\tBT 0.789 (0.851)\tDT 0.000 (0.083)\tloss 5.299 (5.312)\n",
            "Train: [11][30/196]\tBT 0.795 (0.831)\tDT 0.000 (0.056)\tloss 5.364 (5.306)\n",
            "Train: [11][40/196]\tBT 0.798 (0.822)\tDT 0.000 (0.042)\tloss 5.242 (5.300)\n",
            "Train: [11][50/196]\tBT 0.791 (0.816)\tDT 0.000 (0.033)\tloss 5.138 (5.289)\n",
            "Train: [11][60/196]\tBT 0.798 (0.812)\tDT 0.000 (0.028)\tloss 5.339 (5.290)\n",
            "Train: [11][70/196]\tBT 0.795 (0.810)\tDT 0.000 (0.024)\tloss 5.155 (5.279)\n",
            "Train: [11][80/196]\tBT 0.790 (0.807)\tDT 0.000 (0.021)\tloss 5.376 (5.284)\n",
            "Train: [11][90/196]\tBT 0.795 (0.806)\tDT 0.000 (0.019)\tloss 5.146 (5.279)\n",
            "Train: [11][100/196]\tBT 0.792 (0.804)\tDT 0.000 (0.017)\tloss 5.340 (5.278)\n",
            "Train: [11][110/196]\tBT 0.788 (0.803)\tDT 0.000 (0.015)\tloss 5.260 (5.272)\n",
            "Train: [11][120/196]\tBT 0.787 (0.802)\tDT 0.000 (0.014)\tloss 5.292 (5.272)\n",
            "Train: [11][130/196]\tBT 0.789 (0.802)\tDT 0.000 (0.013)\tloss 5.294 (5.272)\n",
            "Train: [11][140/196]\tBT 0.795 (0.801)\tDT 0.000 (0.012)\tloss 5.263 (5.267)\n",
            "Train: [11][150/196]\tBT 0.795 (0.800)\tDT 0.000 (0.011)\tloss 5.468 (5.264)\n",
            "Train: [11][160/196]\tBT 0.788 (0.800)\tDT 0.000 (0.011)\tloss 5.152 (5.260)\n",
            "Train: [11][170/196]\tBT 0.793 (0.799)\tDT 0.000 (0.010)\tloss 5.196 (5.260)\n",
            "Train: [11][180/196]\tBT 0.794 (0.799)\tDT 0.000 (0.010)\tloss 5.129 (5.260)\n",
            "Train: [11][190/196]\tBT 0.796 (0.799)\tDT 0.000 (0.009)\tloss 5.162 (5.255)\n",
            "epoch 11, total time 156.41\n",
            "Train: [12][10/196]\tBT 0.791 (0.867)\tDT 0.000 (0.129)\tloss 5.276 (5.254)\n",
            "Train: [12][20/196]\tBT 0.789 (0.828)\tDT 0.000 (0.065)\tloss 5.293 (5.262)\n",
            "Train: [12][30/196]\tBT 0.791 (0.816)\tDT 0.000 (0.043)\tloss 5.256 (5.240)\n",
            "Train: [12][40/196]\tBT 0.790 (0.809)\tDT 0.000 (0.033)\tloss 5.217 (5.226)\n",
            "Train: [12][50/196]\tBT 0.790 (0.806)\tDT 0.000 (0.026)\tloss 5.308 (5.227)\n",
            "Train: [12][60/196]\tBT 0.793 (0.804)\tDT 0.000 (0.022)\tloss 5.116 (5.220)\n",
            "Train: [12][70/196]\tBT 0.792 (0.802)\tDT 0.000 (0.019)\tloss 5.115 (5.214)\n",
            "Train: [12][80/196]\tBT 0.789 (0.801)\tDT 0.000 (0.017)\tloss 5.219 (5.209)\n",
            "Train: [12][90/196]\tBT 0.795 (0.800)\tDT 0.000 (0.015)\tloss 5.271 (5.204)\n",
            "Train: [12][100/196]\tBT 0.792 (0.799)\tDT 0.000 (0.013)\tloss 5.171 (5.199)\n",
            "Train: [12][110/196]\tBT 0.792 (0.798)\tDT 0.000 (0.012)\tloss 5.206 (5.191)\n",
            "Train: [12][120/196]\tBT 0.788 (0.798)\tDT 0.000 (0.011)\tloss 5.215 (5.195)\n",
            "Train: [12][130/196]\tBT 0.788 (0.797)\tDT 0.000 (0.010)\tloss 5.034 (5.193)\n",
            "Train: [12][140/196]\tBT 0.796 (0.797)\tDT 0.000 (0.010)\tloss 5.142 (5.186)\n",
            "Train: [12][150/196]\tBT 0.787 (0.797)\tDT 0.000 (0.009)\tloss 5.171 (5.187)\n",
            "Train: [12][160/196]\tBT 0.791 (0.797)\tDT 0.000 (0.009)\tloss 5.297 (5.185)\n",
            "Train: [12][170/196]\tBT 0.795 (0.796)\tDT 0.000 (0.008)\tloss 5.094 (5.182)\n",
            "Train: [12][180/196]\tBT 0.798 (0.796)\tDT 0.000 (0.008)\tloss 5.089 (5.178)\n",
            "Train: [12][190/196]\tBT 0.792 (0.796)\tDT 0.000 (0.007)\tloss 5.201 (5.177)\n",
            "epoch 12, total time 155.87\n",
            "Train: [13][10/196]\tBT 0.789 (0.873)\tDT 0.000 (0.175)\tloss 5.012 (5.181)\n",
            "Train: [13][20/196]\tBT 0.792 (0.833)\tDT 0.000 (0.088)\tloss 5.073 (5.167)\n",
            "Train: [13][30/196]\tBT 0.798 (0.819)\tDT 0.000 (0.059)\tloss 5.081 (5.146)\n",
            "Train: [13][40/196]\tBT 0.793 (0.813)\tDT 0.000 (0.044)\tloss 5.210 (5.131)\n",
            "Train: [13][50/196]\tBT 0.798 (0.809)\tDT 0.000 (0.035)\tloss 5.025 (5.129)\n",
            "Train: [13][60/196]\tBT 0.787 (0.806)\tDT 0.000 (0.030)\tloss 5.091 (5.128)\n",
            "Train: [13][70/196]\tBT 0.794 (0.804)\tDT 0.000 (0.025)\tloss 5.085 (5.127)\n",
            "Train: [13][80/196]\tBT 0.791 (0.803)\tDT 0.000 (0.022)\tloss 5.214 (5.121)\n",
            "Train: [13][90/196]\tBT 0.789 (0.801)\tDT 0.000 (0.020)\tloss 5.131 (5.127)\n",
            "Train: [13][100/196]\tBT 0.800 (0.801)\tDT 0.000 (0.018)\tloss 5.221 (5.127)\n",
            "Train: [13][110/196]\tBT 0.792 (0.800)\tDT 0.000 (0.016)\tloss 4.959 (5.122)\n",
            "Train: [13][120/196]\tBT 0.797 (0.799)\tDT 0.000 (0.015)\tloss 5.077 (5.115)\n",
            "Train: [13][130/196]\tBT 0.790 (0.799)\tDT 0.000 (0.014)\tloss 5.047 (5.112)\n",
            "Train: [13][140/196]\tBT 0.787 (0.798)\tDT 0.000 (0.013)\tloss 5.127 (5.112)\n",
            "Train: [13][150/196]\tBT 0.787 (0.798)\tDT 0.000 (0.012)\tloss 5.163 (5.112)\n",
            "Train: [13][160/196]\tBT 0.789 (0.797)\tDT 0.000 (0.011)\tloss 4.859 (5.107)\n",
            "Train: [13][170/196]\tBT 0.793 (0.797)\tDT 0.000 (0.011)\tloss 5.076 (5.106)\n",
            "Train: [13][180/196]\tBT 0.793 (0.797)\tDT 0.000 (0.010)\tloss 5.090 (5.106)\n",
            "Train: [13][190/196]\tBT 0.794 (0.797)\tDT 0.000 (0.010)\tloss 5.173 (5.102)\n",
            "epoch 13, total time 156.04\n",
            "Train: [14][10/196]\tBT 0.792 (0.870)\tDT 0.000 (0.129)\tloss 5.029 (5.053)\n",
            "Train: [14][20/196]\tBT 0.789 (0.830)\tDT 0.000 (0.065)\tloss 5.056 (5.048)\n",
            "Train: [14][30/196]\tBT 0.792 (0.817)\tDT 0.000 (0.043)\tloss 5.007 (5.054)\n",
            "Train: [14][40/196]\tBT 0.788 (0.810)\tDT 0.000 (0.033)\tloss 4.964 (5.036)\n",
            "Train: [14][50/196]\tBT 0.786 (0.806)\tDT 0.000 (0.026)\tloss 4.891 (5.033)\n",
            "Train: [14][60/196]\tBT 0.794 (0.803)\tDT 0.000 (0.022)\tloss 4.917 (5.023)\n",
            "Train: [14][70/196]\tBT 0.796 (0.802)\tDT 0.000 (0.019)\tloss 4.909 (5.026)\n",
            "Train: [14][80/196]\tBT 0.788 (0.800)\tDT 0.000 (0.017)\tloss 4.974 (5.025)\n",
            "Train: [14][90/196]\tBT 0.787 (0.799)\tDT 0.000 (0.015)\tloss 5.065 (5.021)\n",
            "Train: [14][100/196]\tBT 0.792 (0.799)\tDT 0.000 (0.013)\tloss 4.934 (5.023)\n",
            "Train: [14][110/196]\tBT 0.789 (0.798)\tDT 0.000 (0.012)\tloss 4.976 (5.021)\n",
            "Train: [14][120/196]\tBT 0.794 (0.798)\tDT 0.000 (0.011)\tloss 5.137 (5.020)\n",
            "Train: [14][130/196]\tBT 0.789 (0.797)\tDT 0.000 (0.010)\tloss 4.840 (5.018)\n",
            "Train: [14][140/196]\tBT 0.788 (0.797)\tDT 0.000 (0.010)\tloss 5.051 (5.012)\n",
            "Train: [14][150/196]\tBT 0.794 (0.797)\tDT 0.000 (0.009)\tloss 4.949 (5.010)\n",
            "Train: [14][160/196]\tBT 0.793 (0.796)\tDT 0.000 (0.008)\tloss 4.967 (5.008)\n",
            "Train: [14][170/196]\tBT 0.786 (0.796)\tDT 0.000 (0.008)\tloss 5.122 (5.008)\n",
            "Train: [14][180/196]\tBT 0.794 (0.796)\tDT 0.000 (0.008)\tloss 5.004 (5.006)\n",
            "Train: [14][190/196]\tBT 0.794 (0.796)\tDT 0.000 (0.007)\tloss 4.998 (5.003)\n",
            "epoch 14, total time 155.84\n",
            "Train: [15][10/196]\tBT 0.791 (0.921)\tDT 0.000 (0.180)\tloss 5.059 (4.993)\n",
            "Train: [15][20/196]\tBT 0.790 (0.856)\tDT 0.000 (0.090)\tloss 4.770 (4.965)\n",
            "Train: [15][30/196]\tBT 0.793 (0.835)\tDT 0.000 (0.060)\tloss 5.082 (4.964)\n",
            "Train: [15][40/196]\tBT 0.791 (0.824)\tDT 0.000 (0.045)\tloss 5.016 (4.972)\n",
            "Train: [15][50/196]\tBT 0.793 (0.818)\tDT 0.000 (0.036)\tloss 4.820 (4.971)\n",
            "Train: [15][60/196]\tBT 0.793 (0.814)\tDT 0.000 (0.030)\tloss 4.971 (4.963)\n",
            "Train: [15][70/196]\tBT 0.792 (0.811)\tDT 0.000 (0.026)\tloss 4.881 (4.957)\n",
            "Train: [15][80/196]\tBT 0.791 (0.808)\tDT 0.000 (0.023)\tloss 4.976 (4.952)\n",
            "Train: [15][90/196]\tBT 0.795 (0.807)\tDT 0.000 (0.020)\tloss 4.905 (4.952)\n",
            "Train: [15][100/196]\tBT 0.797 (0.805)\tDT 0.000 (0.018)\tloss 4.874 (4.945)\n",
            "Train: [15][110/196]\tBT 0.792 (0.804)\tDT 0.000 (0.017)\tloss 4.967 (4.944)\n",
            "Train: [15][120/196]\tBT 0.788 (0.803)\tDT 0.000 (0.015)\tloss 4.962 (4.945)\n",
            "Train: [15][130/196]\tBT 0.791 (0.802)\tDT 0.000 (0.014)\tloss 4.847 (4.937)\n",
            "Train: [15][140/196]\tBT 0.797 (0.802)\tDT 0.000 (0.013)\tloss 4.807 (4.934)\n",
            "Train: [15][150/196]\tBT 0.791 (0.801)\tDT 0.000 (0.012)\tloss 5.072 (4.936)\n",
            "Train: [15][160/196]\tBT 0.796 (0.800)\tDT 0.000 (0.012)\tloss 4.912 (4.937)\n",
            "Train: [15][170/196]\tBT 0.790 (0.800)\tDT 0.000 (0.011)\tloss 4.814 (4.935)\n",
            "Train: [15][180/196]\tBT 0.800 (0.800)\tDT 0.000 (0.010)\tloss 4.724 (4.935)\n",
            "Train: [15][190/196]\tBT 0.795 (0.799)\tDT 0.000 (0.010)\tloss 4.864 (4.932)\n",
            "epoch 15, total time 156.52\n",
            "Train: [16][10/196]\tBT 0.796 (0.874)\tDT 0.000 (0.131)\tloss 5.072 (4.948)\n",
            "Train: [16][20/196]\tBT 0.792 (0.833)\tDT 0.000 (0.066)\tloss 4.872 (4.952)\n",
            "Train: [16][30/196]\tBT 0.788 (0.819)\tDT 0.000 (0.044)\tloss 4.693 (4.927)\n",
            "Train: [16][40/196]\tBT 0.794 (0.813)\tDT 0.000 (0.033)\tloss 4.915 (4.908)\n",
            "Train: [16][50/196]\tBT 0.794 (0.809)\tDT 0.000 (0.027)\tloss 4.820 (4.899)\n",
            "Train: [16][60/196]\tBT 0.797 (0.806)\tDT 0.000 (0.022)\tloss 4.914 (4.889)\n",
            "Train: [16][70/196]\tBT 0.793 (0.804)\tDT 0.000 (0.019)\tloss 4.783 (4.886)\n",
            "Train: [16][80/196]\tBT 0.794 (0.803)\tDT 0.000 (0.017)\tloss 4.801 (4.887)\n",
            "Train: [16][90/196]\tBT 0.795 (0.802)\tDT 0.000 (0.015)\tloss 4.864 (4.883)\n",
            "Train: [16][100/196]\tBT 0.792 (0.801)\tDT 0.000 (0.014)\tloss 4.898 (4.879)\n",
            "Train: [16][110/196]\tBT 0.792 (0.800)\tDT 0.000 (0.012)\tloss 4.801 (4.876)\n",
            "Train: [16][120/196]\tBT 0.792 (0.799)\tDT 0.000 (0.011)\tloss 4.797 (4.871)\n",
            "Train: [16][130/196]\tBT 0.796 (0.799)\tDT 0.000 (0.011)\tloss 4.831 (4.871)\n",
            "Train: [16][140/196]\tBT 0.790 (0.798)\tDT 0.000 (0.010)\tloss 4.869 (4.869)\n",
            "Train: [16][150/196]\tBT 0.792 (0.798)\tDT 0.000 (0.009)\tloss 4.850 (4.868)\n",
            "Train: [16][160/196]\tBT 0.790 (0.797)\tDT 0.000 (0.009)\tloss 4.696 (4.865)\n",
            "Train: [16][170/196]\tBT 0.789 (0.797)\tDT 0.000 (0.008)\tloss 4.721 (4.867)\n",
            "Train: [16][180/196]\tBT 0.796 (0.797)\tDT 0.000 (0.008)\tloss 4.902 (4.864)\n",
            "Train: [16][190/196]\tBT 0.789 (0.797)\tDT 0.000 (0.007)\tloss 4.801 (4.863)\n",
            "epoch 16, total time 156.03\n",
            "Train: [17][10/196]\tBT 0.789 (0.883)\tDT 0.000 (0.141)\tloss 4.771 (4.774)\n",
            "Train: [17][20/196]\tBT 0.792 (0.837)\tDT 0.000 (0.071)\tloss 4.725 (4.802)\n",
            "Train: [17][30/196]\tBT 0.791 (0.821)\tDT 0.000 (0.047)\tloss 4.933 (4.822)\n",
            "Train: [17][40/196]\tBT 0.792 (0.814)\tDT 0.000 (0.036)\tloss 4.777 (4.821)\n",
            "Train: [17][50/196]\tBT 0.795 (0.810)\tDT 0.000 (0.029)\tloss 4.836 (4.817)\n",
            "Train: [17][60/196]\tBT 0.798 (0.807)\tDT 0.000 (0.024)\tloss 4.931 (4.819)\n",
            "Train: [17][70/196]\tBT 0.788 (0.805)\tDT 0.000 (0.021)\tloss 4.676 (4.810)\n",
            "Train: [17][80/196]\tBT 0.787 (0.803)\tDT 0.000 (0.018)\tloss 4.897 (4.819)\n",
            "Train: [17][90/196]\tBT 0.789 (0.802)\tDT 0.000 (0.016)\tloss 4.764 (4.818)\n",
            "Train: [17][100/196]\tBT 0.789 (0.801)\tDT 0.000 (0.015)\tloss 4.942 (4.820)\n",
            "Train: [17][110/196]\tBT 0.790 (0.800)\tDT 0.000 (0.013)\tloss 4.632 (4.813)\n",
            "Train: [17][120/196]\tBT 0.791 (0.800)\tDT 0.000 (0.012)\tloss 4.855 (4.814)\n",
            "Train: [17][130/196]\tBT 0.799 (0.799)\tDT 0.000 (0.011)\tloss 4.629 (4.810)\n",
            "Train: [17][140/196]\tBT 0.796 (0.799)\tDT 0.000 (0.011)\tloss 4.718 (4.812)\n",
            "Train: [17][150/196]\tBT 0.787 (0.798)\tDT 0.000 (0.010)\tloss 4.784 (4.809)\n",
            "Train: [17][160/196]\tBT 0.789 (0.798)\tDT 0.000 (0.009)\tloss 4.716 (4.808)\n",
            "Train: [17][170/196]\tBT 0.796 (0.797)\tDT 0.000 (0.009)\tloss 4.809 (4.806)\n",
            "Train: [17][180/196]\tBT 0.790 (0.797)\tDT 0.000 (0.008)\tloss 4.812 (4.807)\n",
            "Train: [17][190/196]\tBT 0.793 (0.797)\tDT 0.000 (0.008)\tloss 4.937 (4.808)\n",
            "epoch 17, total time 156.09\n",
            "Train: [18][10/196]\tBT 0.793 (0.883)\tDT 0.000 (0.142)\tloss 4.695 (4.753)\n",
            "Train: [18][20/196]\tBT 0.794 (0.836)\tDT 0.000 (0.071)\tloss 4.945 (4.772)\n",
            "Train: [18][30/196]\tBT 0.787 (0.821)\tDT 0.000 (0.048)\tloss 4.777 (4.765)\n",
            "Train: [18][40/196]\tBT 0.789 (0.813)\tDT 0.000 (0.036)\tloss 4.748 (4.760)\n",
            "Train: [18][50/196]\tBT 0.791 (0.809)\tDT 0.000 (0.029)\tloss 4.922 (4.764)\n",
            "Train: [18][60/196]\tBT 0.792 (0.806)\tDT 0.000 (0.024)\tloss 4.730 (4.770)\n",
            "Train: [18][70/196]\tBT 0.794 (0.804)\tDT 0.000 (0.021)\tloss 4.811 (4.771)\n",
            "Train: [18][80/196]\tBT 0.789 (0.803)\tDT 0.000 (0.018)\tloss 4.694 (4.769)\n",
            "Train: [18][90/196]\tBT 0.793 (0.802)\tDT 0.000 (0.016)\tloss 4.642 (4.770)\n",
            "Train: [18][100/196]\tBT 0.797 (0.801)\tDT 0.000 (0.015)\tloss 4.749 (4.765)\n",
            "Train: [18][110/196]\tBT 0.804 (0.800)\tDT 0.000 (0.013)\tloss 4.853 (4.764)\n",
            "Train: [18][120/196]\tBT 0.789 (0.799)\tDT 0.000 (0.012)\tloss 4.704 (4.764)\n",
            "Train: [18][130/196]\tBT 0.797 (0.799)\tDT 0.000 (0.011)\tloss 4.787 (4.765)\n",
            "Train: [18][140/196]\tBT 0.791 (0.798)\tDT 0.000 (0.011)\tloss 4.759 (4.763)\n",
            "Train: [18][150/196]\tBT 0.787 (0.798)\tDT 0.000 (0.010)\tloss 4.710 (4.761)\n",
            "Train: [18][160/196]\tBT 0.792 (0.798)\tDT 0.000 (0.009)\tloss 4.898 (4.763)\n",
            "Train: [18][170/196]\tBT 0.793 (0.797)\tDT 0.000 (0.009)\tloss 4.651 (4.764)\n",
            "Train: [18][180/196]\tBT 0.797 (0.797)\tDT 0.000 (0.008)\tloss 4.646 (4.761)\n",
            "Train: [18][190/196]\tBT 0.788 (0.797)\tDT 0.000 (0.008)\tloss 4.790 (4.762)\n",
            "epoch 18, total time 156.08\n",
            "Train: [19][10/196]\tBT 0.789 (0.901)\tDT 0.000 (0.160)\tloss 4.735 (4.780)\n",
            "Train: [19][20/196]\tBT 0.795 (0.846)\tDT 0.000 (0.080)\tloss 4.638 (4.756)\n",
            "Train: [19][30/196]\tBT 0.790 (0.828)\tDT 0.000 (0.054)\tloss 4.710 (4.756)\n",
            "Train: [19][40/196]\tBT 0.798 (0.819)\tDT 0.000 (0.040)\tloss 4.621 (4.733)\n",
            "Train: [19][50/196]\tBT 0.793 (0.814)\tDT 0.000 (0.032)\tloss 4.665 (4.730)\n",
            "Train: [19][60/196]\tBT 0.794 (0.810)\tDT 0.000 (0.027)\tloss 4.617 (4.723)\n",
            "Train: [19][70/196]\tBT 0.794 (0.808)\tDT 0.000 (0.023)\tloss 4.539 (4.725)\n",
            "Train: [19][80/196]\tBT 0.791 (0.806)\tDT 0.000 (0.020)\tloss 4.626 (4.722)\n",
            "Train: [19][90/196]\tBT 0.795 (0.804)\tDT 0.000 (0.018)\tloss 4.626 (4.719)\n",
            "Train: [19][100/196]\tBT 0.797 (0.803)\tDT 0.000 (0.016)\tloss 4.690 (4.718)\n",
            "Train: [19][110/196]\tBT 0.793 (0.802)\tDT 0.000 (0.015)\tloss 4.891 (4.721)\n",
            "Train: [19][120/196]\tBT 0.788 (0.801)\tDT 0.000 (0.014)\tloss 4.596 (4.714)\n",
            "Train: [19][130/196]\tBT 0.792 (0.801)\tDT 0.000 (0.013)\tloss 4.716 (4.713)\n",
            "Train: [19][140/196]\tBT 0.795 (0.800)\tDT 0.000 (0.012)\tloss 4.726 (4.709)\n",
            "Train: [19][150/196]\tBT 0.791 (0.799)\tDT 0.000 (0.011)\tloss 4.714 (4.710)\n",
            "Train: [19][160/196]\tBT 0.790 (0.799)\tDT 0.000 (0.010)\tloss 4.694 (4.709)\n",
            "Train: [19][170/196]\tBT 0.795 (0.799)\tDT 0.000 (0.010)\tloss 4.437 (4.707)\n",
            "Train: [19][180/196]\tBT 0.798 (0.798)\tDT 0.000 (0.009)\tloss 4.858 (4.708)\n",
            "Train: [19][190/196]\tBT 0.795 (0.798)\tDT 0.000 (0.009)\tloss 4.595 (4.705)\n",
            "epoch 19, total time 156.29\n",
            "Train: [20][10/196]\tBT 0.792 (0.901)\tDT 0.000 (0.159)\tloss 4.620 (4.636)\n",
            "Train: [20][20/196]\tBT 0.793 (0.847)\tDT 0.000 (0.080)\tloss 4.712 (4.659)\n",
            "Train: [20][30/196]\tBT 0.792 (0.829)\tDT 0.000 (0.053)\tloss 4.587 (4.652)\n",
            "Train: [20][40/196]\tBT 0.791 (0.820)\tDT 0.000 (0.040)\tloss 4.755 (4.654)\n",
            "Train: [20][50/196]\tBT 0.796 (0.814)\tDT 0.000 (0.032)\tloss 4.768 (4.650)\n",
            "Train: [20][60/196]\tBT 0.787 (0.811)\tDT 0.000 (0.027)\tloss 4.643 (4.645)\n",
            "Train: [20][70/196]\tBT 0.794 (0.808)\tDT 0.000 (0.023)\tloss 4.624 (4.652)\n",
            "Train: [20][80/196]\tBT 0.787 (0.806)\tDT 0.000 (0.020)\tloss 4.658 (4.657)\n",
            "Train: [20][90/196]\tBT 0.787 (0.805)\tDT 0.000 (0.018)\tloss 4.638 (4.657)\n",
            "Train: [20][100/196]\tBT 0.792 (0.803)\tDT 0.000 (0.016)\tloss 4.634 (4.655)\n",
            "Train: [20][110/196]\tBT 0.788 (0.802)\tDT 0.000 (0.015)\tloss 4.767 (4.661)\n",
            "Train: [20][120/196]\tBT 0.794 (0.801)\tDT 0.000 (0.014)\tloss 4.574 (4.659)\n",
            "Train: [20][130/196]\tBT 0.794 (0.801)\tDT 0.000 (0.013)\tloss 4.608 (4.661)\n",
            "Train: [20][140/196]\tBT 0.793 (0.800)\tDT 0.000 (0.012)\tloss 4.705 (4.658)\n",
            "Train: [20][150/196]\tBT 0.792 (0.800)\tDT 0.000 (0.011)\tloss 4.569 (4.658)\n",
            "Train: [20][160/196]\tBT 0.795 (0.799)\tDT 0.000 (0.010)\tloss 4.552 (4.655)\n",
            "Train: [20][170/196]\tBT 0.791 (0.799)\tDT 0.000 (0.010)\tloss 4.601 (4.655)\n",
            "Train: [20][180/196]\tBT 0.792 (0.798)\tDT 0.000 (0.009)\tloss 4.651 (4.655)\n",
            "Train: [20][190/196]\tBT 0.796 (0.798)\tDT 0.000 (0.009)\tloss 4.469 (4.652)\n",
            "epoch 20, total time 156.32\n",
            "Train: [21][10/196]\tBT 0.792 (0.913)\tDT 0.000 (0.172)\tloss 4.847 (4.615)\n",
            "Train: [21][20/196]\tBT 0.787 (0.851)\tDT 0.000 (0.086)\tloss 4.841 (4.622)\n",
            "Train: [21][30/196]\tBT 0.788 (0.831)\tDT 0.000 (0.058)\tloss 4.663 (4.619)\n",
            "Train: [21][40/196]\tBT 0.791 (0.821)\tDT 0.000 (0.043)\tloss 4.581 (4.604)\n",
            "Train: [21][50/196]\tBT 0.791 (0.816)\tDT 0.000 (0.035)\tloss 4.627 (4.604)\n",
            "Train: [21][60/196]\tBT 0.790 (0.812)\tDT 0.000 (0.029)\tloss 4.589 (4.606)\n",
            "Train: [21][70/196]\tBT 0.791 (0.809)\tDT 0.000 (0.025)\tloss 4.557 (4.601)\n",
            "Train: [21][80/196]\tBT 0.792 (0.807)\tDT 0.000 (0.022)\tloss 4.468 (4.598)\n",
            "Train: [21][90/196]\tBT 0.789 (0.806)\tDT 0.000 (0.019)\tloss 4.636 (4.601)\n",
            "Train: [21][100/196]\tBT 0.791 (0.804)\tDT 0.000 (0.018)\tloss 4.375 (4.595)\n",
            "Train: [21][110/196]\tBT 0.792 (0.803)\tDT 0.000 (0.016)\tloss 4.672 (4.595)\n",
            "Train: [21][120/196]\tBT 0.790 (0.802)\tDT 0.000 (0.015)\tloss 4.769 (4.599)\n",
            "Train: [21][130/196]\tBT 0.794 (0.801)\tDT 0.000 (0.014)\tloss 4.641 (4.598)\n",
            "Train: [21][140/196]\tBT 0.792 (0.801)\tDT 0.000 (0.013)\tloss 4.613 (4.596)\n",
            "Train: [21][150/196]\tBT 0.796 (0.800)\tDT 0.000 (0.012)\tloss 4.568 (4.598)\n",
            "Train: [21][160/196]\tBT 0.792 (0.800)\tDT 0.000 (0.011)\tloss 4.492 (4.595)\n",
            "Train: [21][170/196]\tBT 0.790 (0.799)\tDT 0.000 (0.011)\tloss 4.754 (4.598)\n",
            "Train: [21][180/196]\tBT 0.794 (0.799)\tDT 0.000 (0.010)\tloss 4.696 (4.599)\n",
            "Train: [21][190/196]\tBT 0.792 (0.798)\tDT 0.000 (0.009)\tloss 4.781 (4.601)\n",
            "epoch 21, total time 156.38\n",
            "Train: [22][10/196]\tBT 0.789 (0.875)\tDT 0.000 (0.130)\tloss 4.696 (4.560)\n",
            "Train: [22][20/196]\tBT 0.782 (0.834)\tDT 0.000 (0.065)\tloss 4.470 (4.537)\n",
            "Train: [22][30/196]\tBT 0.788 (0.820)\tDT 0.000 (0.044)\tloss 4.662 (4.548)\n",
            "Train: [22][40/196]\tBT 0.790 (0.813)\tDT 0.000 (0.033)\tloss 4.608 (4.567)\n",
            "Train: [22][50/196]\tBT 0.792 (0.809)\tDT 0.000 (0.026)\tloss 4.647 (4.569)\n",
            "Train: [22][60/196]\tBT 0.785 (0.806)\tDT 0.000 (0.022)\tloss 4.420 (4.558)\n",
            "Train: [22][70/196]\tBT 0.798 (0.804)\tDT 0.000 (0.019)\tloss 4.500 (4.550)\n",
            "Train: [22][80/196]\tBT 0.795 (0.803)\tDT 0.000 (0.017)\tloss 4.594 (4.549)\n",
            "Train: [22][90/196]\tBT 0.788 (0.801)\tDT 0.000 (0.015)\tloss 4.493 (4.551)\n",
            "Train: [22][100/196]\tBT 0.795 (0.801)\tDT 0.000 (0.013)\tloss 4.666 (4.556)\n",
            "Train: [22][110/196]\tBT 0.793 (0.800)\tDT 0.000 (0.012)\tloss 4.433 (4.557)\n",
            "Train: [22][120/196]\tBT 0.794 (0.799)\tDT 0.000 (0.011)\tloss 4.501 (4.558)\n",
            "Train: [22][130/196]\tBT 0.793 (0.799)\tDT 0.000 (0.010)\tloss 4.679 (4.555)\n",
            "Train: [22][140/196]\tBT 0.789 (0.798)\tDT 0.000 (0.010)\tloss 4.493 (4.554)\n",
            "Train: [22][150/196]\tBT 0.794 (0.798)\tDT 0.000 (0.009)\tloss 4.568 (4.551)\n",
            "Train: [22][160/196]\tBT 0.792 (0.797)\tDT 0.000 (0.009)\tloss 4.454 (4.550)\n",
            "Train: [22][170/196]\tBT 0.791 (0.797)\tDT 0.000 (0.008)\tloss 4.408 (4.546)\n",
            "Train: [22][180/196]\tBT 0.794 (0.797)\tDT 0.000 (0.008)\tloss 4.618 (4.547)\n",
            "Train: [22][190/196]\tBT 0.789 (0.797)\tDT 0.000 (0.007)\tloss 4.628 (4.550)\n",
            "epoch 22, total time 156.05\n",
            "Train: [23][10/196]\tBT 0.797 (0.880)\tDT 0.000 (0.147)\tloss 4.776 (4.616)\n",
            "Train: [23][20/196]\tBT 0.793 (0.836)\tDT 0.000 (0.074)\tloss 4.625 (4.547)\n",
            "Train: [23][30/196]\tBT 0.792 (0.821)\tDT 0.000 (0.049)\tloss 4.413 (4.538)\n",
            "Train: [23][40/196]\tBT 0.787 (0.814)\tDT 0.000 (0.037)\tloss 4.341 (4.526)\n",
            "Train: [23][50/196]\tBT 0.795 (0.810)\tDT 0.000 (0.030)\tloss 4.477 (4.523)\n",
            "Train: [23][60/196]\tBT 0.789 (0.807)\tDT 0.000 (0.025)\tloss 4.619 (4.527)\n",
            "Train: [23][70/196]\tBT 0.794 (0.805)\tDT 0.000 (0.021)\tloss 4.667 (4.521)\n",
            "Train: [23][80/196]\tBT 0.793 (0.803)\tDT 0.000 (0.019)\tloss 4.557 (4.523)\n",
            "Train: [23][90/196]\tBT 0.790 (0.802)\tDT 0.000 (0.017)\tloss 4.433 (4.519)\n",
            "Train: [23][100/196]\tBT 0.789 (0.801)\tDT 0.000 (0.015)\tloss 4.620 (4.518)\n",
            "Train: [23][110/196]\tBT 0.786 (0.800)\tDT 0.000 (0.014)\tloss 4.489 (4.517)\n",
            "Train: [23][120/196]\tBT 0.789 (0.800)\tDT 0.000 (0.013)\tloss 4.452 (4.519)\n",
            "Train: [23][130/196]\tBT 0.791 (0.799)\tDT 0.000 (0.012)\tloss 4.542 (4.519)\n",
            "Train: [23][140/196]\tBT 0.789 (0.798)\tDT 0.000 (0.011)\tloss 4.648 (4.518)\n",
            "Train: [23][150/196]\tBT 0.797 (0.798)\tDT 0.000 (0.010)\tloss 4.650 (4.521)\n",
            "Train: [23][160/196]\tBT 0.793 (0.798)\tDT 0.000 (0.010)\tloss 4.342 (4.519)\n",
            "Train: [23][170/196]\tBT 0.791 (0.797)\tDT 0.000 (0.009)\tloss 4.482 (4.517)\n",
            "Train: [23][180/196]\tBT 0.799 (0.797)\tDT 0.000 (0.009)\tloss 4.535 (4.514)\n",
            "Train: [23][190/196]\tBT 0.788 (0.797)\tDT 0.000 (0.008)\tloss 4.496 (4.512)\n",
            "epoch 23, total time 156.08\n",
            "Train: [24][10/196]\tBT 0.789 (0.877)\tDT 0.000 (0.135)\tloss 4.451 (4.533)\n",
            "Train: [24][20/196]\tBT 0.789 (0.833)\tDT 0.000 (0.068)\tloss 4.350 (4.488)\n",
            "Train: [24][30/196]\tBT 0.793 (0.819)\tDT 0.000 (0.045)\tloss 4.471 (4.464)\n",
            "Train: [24][40/196]\tBT 0.788 (0.812)\tDT 0.000 (0.034)\tloss 4.659 (4.465)\n",
            "Train: [24][50/196]\tBT 0.786 (0.808)\tDT 0.000 (0.027)\tloss 4.390 (4.459)\n",
            "Train: [24][60/196]\tBT 0.794 (0.806)\tDT 0.000 (0.023)\tloss 4.365 (4.453)\n",
            "Train: [24][70/196]\tBT 0.796 (0.804)\tDT 0.000 (0.020)\tloss 4.487 (4.455)\n",
            "Train: [24][80/196]\tBT 0.792 (0.802)\tDT 0.000 (0.017)\tloss 4.431 (4.451)\n",
            "Train: [24][90/196]\tBT 0.792 (0.801)\tDT 0.000 (0.015)\tloss 4.435 (4.449)\n",
            "Train: [24][100/196]\tBT 0.793 (0.800)\tDT 0.000 (0.014)\tloss 4.404 (4.451)\n",
            "Train: [24][110/196]\tBT 0.790 (0.799)\tDT 0.000 (0.013)\tloss 4.361 (4.448)\n",
            "Train: [24][120/196]\tBT 0.797 (0.799)\tDT 0.000 (0.012)\tloss 4.346 (4.452)\n",
            "Train: [24][130/196]\tBT 0.793 (0.798)\tDT 0.000 (0.011)\tloss 4.593 (4.453)\n",
            "Train: [24][140/196]\tBT 0.792 (0.798)\tDT 0.000 (0.010)\tloss 4.336 (4.453)\n",
            "Train: [24][150/196]\tBT 0.792 (0.798)\tDT 0.000 (0.009)\tloss 4.501 (4.452)\n",
            "Train: [24][160/196]\tBT 0.787 (0.797)\tDT 0.000 (0.009)\tloss 4.269 (4.452)\n",
            "Train: [24][170/196]\tBT 0.795 (0.797)\tDT 0.000 (0.008)\tloss 4.427 (4.455)\n",
            "Train: [24][180/196]\tBT 0.792 (0.797)\tDT 0.000 (0.008)\tloss 4.431 (4.458)\n",
            "Train: [24][190/196]\tBT 0.798 (0.796)\tDT 0.000 (0.008)\tloss 4.620 (4.463)\n",
            "epoch 24, total time 156.00\n",
            "Train: [25][10/196]\tBT 0.795 (0.905)\tDT 0.000 (0.160)\tloss 4.402 (4.476)\n",
            "Train: [25][20/196]\tBT 0.795 (0.848)\tDT 0.000 (0.080)\tloss 4.382 (4.444)\n",
            "Train: [25][30/196]\tBT 0.798 (0.830)\tDT 0.000 (0.054)\tloss 4.598 (4.450)\n",
            "Train: [25][40/196]\tBT 0.789 (0.820)\tDT 0.000 (0.040)\tloss 4.266 (4.420)\n",
            "Train: [25][50/196]\tBT 0.798 (0.815)\tDT 0.000 (0.032)\tloss 4.718 (4.420)\n",
            "Train: [25][60/196]\tBT 0.791 (0.811)\tDT 0.000 (0.027)\tloss 4.461 (4.423)\n",
            "Train: [25][70/196]\tBT 0.792 (0.808)\tDT 0.000 (0.023)\tloss 4.479 (4.419)\n",
            "Train: [25][80/196]\tBT 0.789 (0.806)\tDT 0.000 (0.020)\tloss 4.382 (4.429)\n",
            "Train: [25][90/196]\tBT 0.788 (0.805)\tDT 0.000 (0.018)\tloss 4.373 (4.432)\n",
            "Train: [25][100/196]\tBT 0.788 (0.804)\tDT 0.000 (0.016)\tloss 4.382 (4.425)\n",
            "Train: [25][110/196]\tBT 0.794 (0.803)\tDT 0.000 (0.015)\tloss 4.371 (4.424)\n",
            "Train: [25][120/196]\tBT 0.792 (0.802)\tDT 0.000 (0.014)\tloss 4.481 (4.424)\n",
            "Train: [25][130/196]\tBT 0.797 (0.801)\tDT 0.000 (0.013)\tloss 4.689 (4.423)\n",
            "Train: [25][140/196]\tBT 0.789 (0.800)\tDT 0.000 (0.012)\tloss 4.589 (4.424)\n",
            "Train: [25][150/196]\tBT 0.788 (0.800)\tDT 0.000 (0.011)\tloss 4.316 (4.426)\n",
            "Train: [25][160/196]\tBT 0.790 (0.799)\tDT 0.000 (0.010)\tloss 4.353 (4.424)\n",
            "Train: [25][170/196]\tBT 0.795 (0.799)\tDT 0.000 (0.010)\tloss 4.329 (4.423)\n",
            "Train: [25][180/196]\tBT 0.797 (0.799)\tDT 0.000 (0.009)\tloss 4.544 (4.422)\n",
            "Train: [25][190/196]\tBT 0.796 (0.798)\tDT 0.000 (0.009)\tloss 4.406 (4.422)\n",
            "epoch 25, total time 156.35\n",
            "==> Saving...\n"
          ]
        }
      ]
    }
  ]
}